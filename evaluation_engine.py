# evaluation_engine.py - è¯„ä¼°å¼•æ“

import json
import time
from typing import List, Dict, Optional, Tuple
from pathlib import Path
import numpy as np
from retrieval_engine import RetrievalEngine
from qa_generator import QAGenerator
import config

class EvaluationEngine:
    """è¯„ä¼°å¼•æ“ - è¯„ä¼°æ–°ç³»ç»Ÿçš„æ€§èƒ½"""
    
    def __init__(self):
        self.retrieval_engine = RetrievalEngine()
        self.qa_generator = QAGenerator()
        self.evaluation_dir = Path(config.QA_EVALUATION_DIR)
        self.evaluation_dir.mkdir(exist_ok=True)
    
    def calculate_precision_at_k(self, retrieved_items: List[Dict], ground_truth: Dict, k: int = 5) -> float:
        """è®¡ç®—Precision@K"""
        if not retrieved_items or k <= 0:
            return 0.0
        
        # å–å‰kä¸ªç»“æœ
        top_k_items = retrieved_items[:k]
        
        # æ£€æŸ¥ç›¸å…³æ€§ï¼ˆåŸºäºä¸‰å…ƒç»„åŒ¹é…ï¼‰
        relevant_count = 0
        ground_truth_triple = ground_truth.get('triple', ())
        
        for item in top_k_items:
            item_triple = item.get('triple', ())
            # å¦‚æœä¸‰å…ƒç»„çš„ä»»ä½•éƒ¨åˆ†åŒ¹é…ï¼Œè®¤ä¸ºæ˜¯ç›¸å…³çš„
            if any(gt_part in item_triple for gt_part in ground_truth_triple if gt_part):
                relevant_count += 1
        
        return relevant_count / k
    
    def calculate_recall_at_k(self, retrieved_items: List[Dict], ground_truth: Dict, k: int = 5) -> float:
        """è®¡ç®—Recall@K"""
        if not retrieved_items or k <= 0:
            return 0.0
        
        # å‡è®¾åªæœ‰ä¸€ä¸ªç›¸å…³æ–‡æ¡£ï¼ˆground truthï¼‰
        top_k_items = retrieved_items[:k]
        ground_truth_triple = ground_truth.get('triple', ())
        
        # æ£€æŸ¥æ˜¯å¦æ‰¾åˆ°äº†ç›¸å…³æ–‡æ¡£
        for item in top_k_items:
            item_triple = item.get('triple', ())
            if any(gt_part in item_triple for gt_part in ground_truth_triple if gt_part):
                return 1.0
        
        return 0.0
    
    def calculate_ndcg_at_k(self, retrieved_items: List[Dict], ground_truth: Dict, k: int = 5) -> float:
        """è®¡ç®—nDCG@K"""
        if not retrieved_items or k <= 0:
            return 0.0
        
        top_k_items = retrieved_items[:k]
        ground_truth_triple = ground_truth.get('triple', ())
        
        # è®¡ç®—ç›¸å…³æ€§åˆ†æ•°ï¼ˆ0æˆ–1ï¼‰
        relevance_scores = []
        for item in top_k_items:
            item_triple = item.get('triple', ())
            if any(gt_part in item_triple for gt_part in ground_truth_triple if gt_part):
                relevance_scores.append(1.0)
            else:
                relevance_scores.append(0.0)
        
        # è®¡ç®—DCG
        dcg = 0.0
        for i, score in enumerate(relevance_scores):
            dcg += score / np.log2(i + 2)  # i+2 because log2(1) = 0
        
        # è®¡ç®—IDCGï¼ˆç†æƒ³æƒ…å†µä¸‹çš„DCGï¼‰
        ideal_scores = sorted(relevance_scores, reverse=True)
        idcg = 0.0
        for i, score in enumerate(ideal_scores):
            idcg += score / np.log2(i + 2)
        
        return dcg / idcg if idcg > 0 else 0.0
    
    def evaluate_single_qa(self, qa_pair: Dict, k_values: List[int] = [1, 3, 5]) -> Dict:
        """è¯„ä¼°å•ä¸ªQAå¯¹"""
        question = qa_pair['question']
        
        # ä½¿ç”¨æ£€ç´¢å¼•æ“è·å–ç»“æœ
        result = self.retrieval_engine.retrieve_and_rewrite(question, n_results=max(k_values))
        
        # è®¡ç®—å„ç§æŒ‡æ ‡
        metrics = {
            'question': question,
            'answer': qa_pair['answer'],
            'question_type': qa_pair.get('question_type', 'general'),
            'retrieved_count': len(result['retrieved_items']),
            'cotkr_knowledge': result['cotkr_knowledge'],
            'final_answer': result['final_answer']
        }
        
        # ä¸ºæ¯ä¸ªkå€¼è®¡ç®—æŒ‡æ ‡
        for k in k_values:
            metrics[f'precision_at_{k}'] = self.calculate_precision_at_k(
                result['retrieved_items'], qa_pair, k
            )
            metrics[f'recall_at_{k}'] = self.calculate_recall_at_k(
                result['retrieved_items'], qa_pair, k
            )
            metrics[f'ndcg_at_{k}'] = self.calculate_ndcg_at_k(
                result['retrieved_items'], qa_pair, k
            )
        
        return metrics
    
    def evaluate_qa_dataset(self, qa_dataset: List[Dict], k_values: List[int] = [1, 3, 5]) -> Dict:
        """è¯„ä¼°æ•´ä¸ªQAæ•°æ®é›†"""
        print(f"ğŸ”„ å¼€å§‹è¯„ä¼° {len(qa_dataset)} ä¸ªQAå¯¹")
        
        all_metrics = []
        start_time = time.time()
        
        for i, qa_pair in enumerate(qa_dataset):
            if i % 10 == 0:
                print(f"  è¿›åº¦: {i}/{len(qa_dataset)}")
            
            metrics = self.evaluate_single_qa(qa_pair, k_values)
            all_metrics.append(metrics)
        
        # è®¡ç®—å¹³å‡æŒ‡æ ‡
        avg_metrics = self._calculate_average_metrics(all_metrics, k_values)
        
        # æŒ‰é—®é¢˜ç±»å‹åˆ†ç»„ç»Ÿè®¡
        type_metrics = self._calculate_metrics_by_type(all_metrics, k_values)
        
        evaluation_time = time.time() - start_time
        
        evaluation_result = {
            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
            'total_questions': len(qa_dataset),
            'evaluation_time_seconds': evaluation_time,
            'average_metrics': avg_metrics,
            'metrics_by_type': type_metrics,
            'detailed_results': all_metrics
        }
        
        print(f"âœ… è¯„ä¼°å®Œæˆï¼Œè€—æ—¶ {evaluation_time:.2f} ç§’")
        return evaluation_result
    
    def _calculate_average_metrics(self, all_metrics: List[Dict], k_values: List[int]) -> Dict:
        """è®¡ç®—å¹³å‡æŒ‡æ ‡"""
        avg_metrics = {}
        
        for k in k_values:
            precision_key = f'precision_at_{k}'
            recall_key = f'recall_at_{k}'
            ndcg_key = f'ndcg_at_{k}'
            
            precisions = [m[precision_key] for m in all_metrics if precision_key in m]
            recalls = [m[recall_key] for m in all_metrics if recall_key in m]
            ndcgs = [m[ndcg_key] for m in all_metrics if ndcg_key in m]
            
            avg_metrics[precision_key] = np.mean(precisions) if precisions else 0.0
            avg_metrics[recall_key] = np.mean(recalls) if recalls else 0.0
            avg_metrics[ndcg_key] = np.mean(ndcgs) if ndcgs else 0.0
        
        return avg_metrics
    
    def _calculate_metrics_by_type(self, all_metrics: List[Dict], k_values: List[int]) -> Dict:
        """æŒ‰é—®é¢˜ç±»å‹è®¡ç®—æŒ‡æ ‡"""
        type_metrics = {}
        
        # æŒ‰ç±»å‹åˆ†ç»„
        metrics_by_type = {}
        for metric in all_metrics:
            q_type = metric.get('question_type', 'general')
            if q_type not in metrics_by_type:
                metrics_by_type[q_type] = []
            metrics_by_type[q_type].append(metric)
        
        # ä¸ºæ¯ç§ç±»å‹è®¡ç®—å¹³å‡æŒ‡æ ‡
        for q_type, type_metrics_list in metrics_by_type.items():
            type_metrics[q_type] = {
                'count': len(type_metrics_list),
                'metrics': self._calculate_average_metrics(type_metrics_list, k_values)
            }
        
        return type_metrics
    
    def save_evaluation_results(self, results: Dict, filename: str = None) -> str:
        """ä¿å­˜è¯„ä¼°ç»“æœ"""
        if filename is None:
            timestamp = time.strftime('%Y%m%d_%H%M%S')
            filename = f"evaluation_results_{timestamp}.json"
        
        filepath = self.evaluation_dir / filename
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ è¯„ä¼°ç»“æœå·²ä¿å­˜åˆ°: {filepath}")
        return str(filepath)
    
    def load_evaluation_results(self, filename: str) -> Dict:
        """åŠ è½½è¯„ä¼°ç»“æœ"""
        filepath = self.evaluation_dir / filename
        
        if not filepath.exists():
            print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {filepath}")
            return {}
        
        with open(filepath, 'r', encoding='utf-8') as f:
            results = json.load(f)
        
        print(f"ğŸ“– å·²åŠ è½½è¯„ä¼°ç»“æœ: {filename}")
        return results
    
    def print_evaluation_summary(self, results: Dict):
        """æ‰“å°è¯„ä¼°ç»“æœæ‘˜è¦"""
        print("\n" + "="*60)
        print("ğŸ“Š æ–°ç³»ç»Ÿè¯„ä¼°ç»“æœæ‘˜è¦")
        print("="*60)
        
        avg_metrics = results.get('average_metrics', {})
        
        print(f"æ€»é—®é¢˜æ•°: {results.get('total_questions', 0)}")
        print(f"è¯„ä¼°æ—¶é—´: {results.get('evaluation_time_seconds', 0):.2f} ç§’")
        
        print("\nå¹³å‡æŒ‡æ ‡:")
        for metric_name, value in avg_metrics.items():
            print(f"  {metric_name}: {value:.4f}")
        
        print("\næŒ‰é—®é¢˜ç±»å‹ç»Ÿè®¡:")
        type_metrics = results.get('metrics_by_type', {})
        for q_type, type_data in type_metrics.items():
            print(f"\n  {q_type} ({type_data['count']} ä¸ªé—®é¢˜):")
            for metric_name, value in type_data['metrics'].items():
                print(f"    {metric_name}: {value:.4f}")

# æµ‹è¯•å‡½æ•°
def test_evaluation_engine():
    """æµ‹è¯•è¯„ä¼°å¼•æ“"""
    evaluator = EvaluationEngine()
    
    # ç”Ÿæˆå°è§„æ¨¡æµ‹è¯•æ•°æ®é›†
    print("ğŸ”„ ç”Ÿæˆæµ‹è¯•QAæ•°æ®é›†")
    qa_dataset = evaluator.qa_generator.generate_qa_dataset(max_entries=5)
    
    if qa_dataset:
        # è¯„ä¼°æ•°æ®é›†
        results = evaluator.evaluate_qa_dataset(qa_dataset, k_values=[1, 3, 5])
        
        # æ‰“å°æ‘˜è¦
        evaluator.print_evaluation_summary(results)
        
        # ä¿å­˜ç»“æœ
        evaluator.save_evaluation_results(results, "test_evaluation.json")

if __name__ == '__main__':
    test_evaluation_engine()