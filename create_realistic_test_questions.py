#!/usr/bin/env python3
# create_realistic_test_questions.py - åŸºäºå®é™…æ•°æ®åº“å†…å®¹åˆ›å»ºæµ‹è¯•é—®é¢˜

from enhanced_embedding_system import EnhancedVectorDatabaseManager
import json
import random

def analyze_database_content():
    """åˆ†ææ•°æ®åº“å†…å®¹ï¼Œç”Ÿæˆåˆé€‚çš„æµ‹è¯•é—®é¢˜"""
    print("ğŸ” åˆ†ææ•°æ®åº“å†…å®¹ç”Ÿæˆæµ‹è¯•é—®é¢˜")
    print("=" * 50)
    
    db_manager = EnhancedVectorDatabaseManager()
    db_manager.initialize_collection()
    
    # è·å–æ•°æ®åº“ä¸­çš„æ‰€æœ‰æ•°æ®
    all_data = db_manager.collection.get()
    
    if not all_data or not all_data['metadatas']:
        print("âŒ æ•°æ®åº“ä¸ºç©º")
        return []
    
    # åˆ†æä¸åŒç±»å‹çš„å…³ç³»
    relations = {}
    for metadata in all_data['metadatas']:
        rel = metadata.get('rel', '')
        sub = metadata.get('sub', '')
        obj = metadata.get('obj', '')
        sub_type = metadata.get('sub_type', '')
        obj_type = metadata.get('obj_type', '')
        
        if rel not in relations:
            relations[rel] = []
        
        relations[rel].append({
            'sub': sub,
            'obj': obj,
            'sub_type': sub_type,
            'obj_type': obj_type
        })
    
    print(f"ğŸ“Š å‘ç° {len(relations)} ç§å…³ç³»ç±»å‹:")
    for rel, items in relations.items():
        print(f"  - {rel}: {len(items)} æ¡è®°å½•")
    
    # ç”ŸæˆåŸºäºå®é™…æ•°æ®çš„æµ‹è¯•é—®é¢˜
    test_questions = []
    
    # 1. åŸºäº 'leader' å…³ç³»çš„é—®é¢˜
    if 'leader' in relations:
        leader_items = relations['leader'][:3]  # å–å‰3ä¸ª
        for item in leader_items:
            test_questions.append({
                'question': f"Who is the leader of {item['sub'].replace('_', ' ')}?",
                'expected_answer': item['obj'].replace('_', ' '),
                'question_type': 'factual',
                'relation': 'leader',
                'subject': item['sub']
            })
    
    # 2. åŸºäº 'capital' å…³ç³»çš„é—®é¢˜
    if 'capital' in relations:
        capital_items = relations['capital'][:3]
        for item in capital_items:
            test_questions.append({
                'question': f"What is the capital of {item['sub'].replace('_', ' ')}?",
                'expected_answer': item['obj'].replace('_', ' '),
                'question_type': 'factual',
                'relation': 'capital',
                'subject': item['sub']
            })
    
    # 3. åŸºäº 'location' å…³ç³»çš„é—®é¢˜
    if 'location' in relations:
        location_items = relations['location'][:3]
        for item in location_items:
            test_questions.append({
                'question': f"Where is {item['sub'].replace('_', ' ')} located?",
                'expected_answer': item['obj'].replace('_', ' '),
                'question_type': 'location',
                'relation': 'location',
                'subject': item['sub']
            })
    
    # 4. åŸºäº 'country' å…³ç³»çš„é—®é¢˜
    if 'country' in relations:
        country_items = relations['country'][:2]
        for item in country_items:
            test_questions.append({
                'question': f"Which country is {item['sub'].replace('_', ' ')} in?",
                'expected_answer': item['obj'].replace('_', ' '),
                'question_type': 'location',
                'relation': 'country',
                'subject': item['sub']
            })
    
    print(f"\nğŸ“ ç”Ÿæˆäº† {len(test_questions)} ä¸ªåŸºäºå®é™…æ•°æ®çš„æµ‹è¯•é—®é¢˜:")
    for i, q in enumerate(test_questions, 1):
        print(f"  {i}. {q['question']}")
        print(f"     æœŸæœ›ç­”æ¡ˆ: {q['expected_answer']}")
        print(f"     å…³ç³»ç±»å‹: {q['relation']}")
        print()
    
    return test_questions

def save_realistic_test_questions():
    """ä¿å­˜ç°å®çš„æµ‹è¯•é—®é¢˜"""
    questions = analyze_database_content()
    
    if questions:
        # ä¿å­˜ä¸ºJSONæ–‡ä»¶
        with open('realistic_test_questions.json', 'w', encoding='utf-8') as f:
            json.dump(questions, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ å·²ä¿å­˜ {len(questions)} ä¸ªæµ‹è¯•é—®é¢˜åˆ° realistic_test_questions.json")
        
        # åˆ›å»ºæ›´æ–°çš„æµ‹è¯•è„šæœ¬
        create_updated_test_script(questions)
    
    return questions

def create_updated_test_script(questions):
    """åˆ›å»ºä½¿ç”¨ç°å®é—®é¢˜çš„æµ‹è¯•è„šæœ¬"""
    
    script_content = f'''#!/usr/bin/env python3
# test_realistic_rag_vs_llm_evaluation.py - ä½¿ç”¨ç°å®é—®é¢˜çš„RAG vs LLMè¯„ä¼°æµ‹è¯•

import json
from pathlib import Path
from rag_vs_llm_evaluation import RAGvsLLMEvaluator

def test_realistic_evaluation():
    """ä½¿ç”¨åŸºäºå®é™…æ•°æ®åº“å†…å®¹çš„é—®é¢˜è¿›è¡Œæµ‹è¯•"""
    print("ğŸ§ª æµ‹è¯•RAG vs LLMè¯„ä¼°ç³»ç»Ÿ (ä½¿ç”¨ç°å®é—®é¢˜)")
    print("=" * 60)
    
    # åˆ›å»ºè¯„ä¼°å™¨
    evaluator = RAGvsLLMEvaluator()
    
    # ä½¿ç”¨åŸºäºå®é™…æ•°æ®åº“å†…å®¹çš„æµ‹è¯•é—®é¢˜
    test_questions = {questions}
    
    print(f"ğŸ“ æµ‹è¯• {{len(test_questions)}} ä¸ªåŸºäºå®é™…æ•°æ®çš„é—®é¢˜")
    
    # æµ‹è¯•å•ä¸ªé—®é¢˜è¯„ä¼°
    print("\\nğŸ” æµ‹è¯•å•ä¸ªé—®é¢˜è¯„ä¼°:")
    for i, qa_item in enumerate(test_questions[:3], 1):  # åªæµ‹è¯•å‰3ä¸ª
        print(f"\\né—®é¢˜ {{i}}: {{qa_item['question']}}")
        print(f"  å…³ç³»ç±»å‹: {{qa_item['relation']}}")
        print(f"  ä¸»ä½“: {{qa_item['subject']}}")
        
        result = evaluator.evaluate_single_question(qa_item)
        
        print(f"  æœŸæœ›ç­”æ¡ˆ: {{result['expected_answer']}}")
        print(f"  RAGç­”æ¡ˆ: {{result['rag_answer']}}")
        print(f"  LLMç­”æ¡ˆ: {{result['llm_answer']}}")
        
        # æ˜¾ç¤ºæ£€ç´¢æŒ‡æ ‡
        if 'rag_retrieval_metrics' in result:
            metrics = result['rag_retrieval_metrics']
            print(f"  æ£€ç´¢æŒ‡æ ‡:")
            print(f"    Precision@1: {{metrics.get('precision@1', 0):.3f}}")
            print(f"    Recall@1: {{metrics.get('recall@1', 0):.3f}}")
            print(f"    nDCG@1: {{metrics.get('ndcg@1', 0):.3f}}")
        
        # æ˜¾ç¤ºç­”æ¡ˆè´¨é‡åˆ†æ•°
        print(f"  RAGç»¼åˆåˆ†æ•°: {{result['rag_scores']['composite_score']:.3f}}")
        print(f"  LLMç»¼åˆåˆ†æ•°: {{result['llm_scores']['composite_score']:.3f}}")
    
    # æµ‹è¯•å®Œæ•´è¯„ä¼°æµç¨‹
    print(f"\\nğŸš€ è¿è¡Œå®Œæ•´è¯„ä¼°æµç¨‹...")
    
    results = evaluator.evaluate_dataset(test_questions)
    
    # ä¿å­˜ç»“æœ
    evaluator.save_evaluation_results(results, output_dir="realistic_evaluation")
    
    # æ‰“å°æŠ¥å‘Š
    evaluator.print_summary_report(results)
    
    print(f"\\nâœ… ç°å®é—®é¢˜è¯„ä¼°æµ‹è¯•å®Œæˆï¼")

if __name__ == '__main__':
    test_realistic_evaluation()
'''
    
    with open('test_realistic_rag_vs_llm_evaluation.py', 'w', encoding='utf-8') as f:
        f.write(script_content)
    
    print(f"ğŸ“„ å·²åˆ›å»ºæ›´æ–°çš„æµ‹è¯•è„šæœ¬: test_realistic_rag_vs_llm_evaluation.py")

if __name__ == '__main__':
    save_realistic_test_questions()