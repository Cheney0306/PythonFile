# text_based_qa_generator.py - åŸºäºæ–‡æœ¬çš„QAç”Ÿæˆå™¨

import json
import os
import xml.etree.ElementTree as ET
from typing import List, Dict, Optional
from pathlib import Path
from tqdm import tqdm
from datetime import datetime
import config
from retrieval_engine import RetrievalEngine

# å¯¼å…¥promptæ¨¡æ¿æ„å»ºå‡½æ•°
import sys
sys.path.append('..')
try:
    from prompt_templates import build_prompt
except ImportError:
    print("âš  è­¦å‘Š: æ— æ³•å¯¼å…¥prompt_templatesï¼Œå°†ä½¿ç”¨ç®€åŒ–ç‰ˆæœ¬")
    def build_prompt(text, triple, schema, prompt_type):
        return f"Generate a {prompt_type} question based on: {text}"

class TextBasedQAGenerator:
    """åŸºäºæ–‡æœ¬çš„QAç”Ÿæˆå™¨ - éå†trainæ•°æ®é›†æ–‡æœ¬ï¼Œé€šè¿‡RAGç³»ç»Ÿç”ŸæˆQAå¯¹"""
    
    def __init__(self):
        self.openai_api_key = config.OPENAI_API_KEY
        self.output_dir = config.QA_OUTPUT_DIR
        self.train_dataset_path = config.DATASET_PATHS[0]  # ä½¿ç”¨trainæ•°æ®é›†
        self.retrieval_engine = RetrievalEngine()
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        Path(self.output_dir).mkdir(exist_ok=True)
        
        # åˆå§‹åŒ–æ—¥å¿—æ–‡ä»¶è·¯å¾„
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.log_file = Path(self.output_dir) / f"qa_generation_log_{timestamp}.txt"
        
        # åˆå§‹åŒ–æ—¥å¿—æ–‡ä»¶
        self._init_log_file()
        
        # æ£€æŸ¥OpenAI APIå¯†é’¥
        if not self.openai_api_key or self.openai_api_key == "your-openai-api-key-here":
            print("âš  è­¦å‘Š: OpenAI APIå¯†é’¥æœªè®¾ç½®ï¼Œå°†æ— æ³•ç”ŸæˆQAå¯¹")
    
    def _init_log_file(self):
        """åˆå§‹åŒ–æ—¥å¿—æ–‡ä»¶"""
        with open(self.log_file, 'w', encoding='utf-8') as f:
            f.write("=" * 80 + "\n")
            f.write("QAç”Ÿæˆè¯¦ç»†æ—¥å¿—\n")
            f.write(f"å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write("=" * 80 + "\n\n")
        print(f"ğŸ“ æ—¥å¿—æ–‡ä»¶å·²åˆ›å»º: {self.log_file}")
    
    def _log_qa_generation(self, entry_id: str, text: str, rag_result: Dict, 
                          prompt: str, qa_pair: Dict, prompt_type: str = ""):
        """è®°å½•QAç”Ÿæˆè¿‡ç¨‹åˆ°æ—¥å¿—æ–‡ä»¶"""
        try:
            with open(self.log_file, 'a', encoding='utf-8') as f:
                f.write("ğŸ”¸" + "=" * 78 + "ğŸ”¸\n")
                f.write(f"ğŸ“‹ æ¡ç›®ID: {entry_id}\n")
                f.write(f"ğŸ• æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                if prompt_type:
                    f.write(f"ğŸ· é—®é¢˜ç±»å‹: {prompt_type}\n")
                f.write("-" * 80 + "\n")
                
                f.write("ğŸ“ è¾“å…¥æ–‡æœ¬:\n")
                f.write(f"{text}\n")
                f.write("-" * 80 + "\n")
                
                f.write("ğŸ” RAGæ£€ç´¢ç»“æœ:\n")
                f.write(f"æ£€ç´¢çŸ¥è¯†: {rag_result.get('cotkr_knowledge', 'N/A')[:300]}...\n")
                f.write(f"æœ€ç»ˆç­”æ¡ˆ: {rag_result.get('final_answer', 'N/A')}\n")
                f.write(f"é—®é¢˜ç±»å‹: {rag_result.get('retrieval_stats', {}).get('question_type', 'N/A')}\n")
                f.write("-" * 80 + "\n")
                
                f.write("ğŸ’¬ æ„å»ºçš„Prompt:\n")
                f.write(f"{prompt}\n")
                f.write("-" * 80 + "\n")
                
                f.write("ğŸ¤– LLMç”Ÿæˆçš„QAå¯¹:\n")
                if qa_pair:
                    f.write(f"é—®é¢˜: {qa_pair.get('question', 'N/A')}\n")
                    f.write(f"ç­”æ¡ˆ: {qa_pair.get('answer', 'N/A')}\n")
                    f.write(f"ç”Ÿæˆæ–¹æ³•: {qa_pair.get('generation_method', 'N/A')}\n")
                else:
                    f.write("âŒ QAç”Ÿæˆå¤±è´¥\n")
                
                f.write("ğŸ”¸" + "=" * 78 + "ğŸ”¸\n\n")
                
        except Exception as e:
            print(f"âš  æ—¥å¿—å†™å…¥å¤±è´¥: {e}")
    
    def extract_texts_from_xml_files(self, target_dir: str = None) -> List[Dict]:
        """
        ä»trainæ•°æ®é›†çš„XMLæ–‡ä»¶ä¸­æå–æ‰€æœ‰<text>æ ‡ç­¾çš„æ–‡æœ¬å†…å®¹
        """
        if target_dir is None:
            target_dir = self.train_dataset_path
        
        texts = []
        
        if not os.path.exists(target_dir):
            print(f"âŒ ç›®å½•ä¸å­˜åœ¨: {target_dir}")
            return texts
        
        # æŸ¥æ‰¾æ‰€æœ‰XMLæ–‡ä»¶
        xml_files = list(Path(target_dir).rglob('*.xml'))
        print(f"ğŸ“ åœ¨trainæ•°æ®é›† {target_dir} ä¸­æ‰¾åˆ° {len(xml_files)} ä¸ªXMLæ–‡ä»¶")
        
        for xml_file in tqdm(xml_files, desc="æå–æ–‡æœ¬"):
            try:
                tree = ET.parse(xml_file)
                root = tree.getroot()
                
                # æŸ¥æ‰¾æ‰€æœ‰entryèŠ‚ç‚¹
                for entry_node in root.findall('.//entry'):
                    entry_id = entry_node.get('id', 'unknown')
                    
                    # æå–textæ ‡ç­¾å†…å®¹
                    text_node = entry_node.find('text')
                    if text_node is not None and text_node.text:
                        text_content = text_node.text.strip()
                        if text_content and len(text_content) > 20:  # è¿‡æ»¤å¤ªçŸ­çš„æ–‡æœ¬
                            
                            # åŒæ—¶æå–ä¸‰å…ƒç»„å’Œschemaä¿¡æ¯ï¼ˆç”¨äºæ„å»ºpromptï¼‰
                            triple_node = entry_node.find('triples/triple')
                            schema_node = entry_node.find('schemas/schema')
                            
                            triple = None
                            schema = None
                            
                            if triple_node is not None:
                                try:
                                    triple = (
                                        triple_node.find('sub').text,
                                        triple_node.find('rel').text,
                                        triple_node.find('obj').text
                                    )
                                except:
                                    pass
                            
                            if schema_node is not None:
                                try:
                                    schema = (
                                        schema_node.find('sub').text,
                                        schema_node.find('rel').text,
                                        schema_node.find('obj').text
                                    )
                                except:
                                    pass
                            
                            texts.append({
                                'id': entry_id,
                                'text': text_content,
                                'triple': triple,
                                'schema': schema,
                                'source_file': str(xml_file)
                            })
                            
            except ET.ParseError as e:
                print(f"âš  è§£æXMLæ–‡ä»¶å¤±è´¥ {xml_file}: {e}")
            except Exception as e:
                print(f"âš  å¤„ç†æ–‡ä»¶æ—¶å‡ºé”™ {xml_file}: {e}")
        
        print(f"âœ… æˆåŠŸæå– {len(texts)} ä¸ªæ–‡æœ¬ç‰‡æ®µ")
        return texts
    
    def generate_qa_from_text_via_rag(self, text_item: Dict, output_filename: str = None) -> List[Dict]:
        """
        é€šè¿‡RAGç³»ç»Ÿå¤„ç†æ–‡æœ¬ï¼Œç„¶åç”ŸæˆQAå¯¹
        
        æµç¨‹ï¼š
        1. å¯¹æ¯ç§é—®é¢˜ç±»å‹ï¼Œå°†æ–‡æœ¬å’Œç±»å‹ä½œä¸ºæŸ¥è¯¢è¾“å…¥RAGç³»ç»Ÿ
        2. è·å–RAGç³»ç»Ÿçš„æ£€ç´¢å’Œé‡å†™ç»“æœï¼ˆé’ˆå¯¹ç‰¹å®šé—®é¢˜ç±»å‹ï¼‰
        3. ä½¿ç”¨prompt_templatesæ„å»ºprompt
        4. å‘é€ç»™LLMç”ŸæˆQAå¯¹
        """
        qa_pairs = []
        
        text = text_item['text']
        entry_id = text_item['id']
        triple = text_item.get('triple')
        schema = text_item.get('schema')
        
        try:
            print(f"ğŸ” å¤„ç†æ–‡æœ¬ (ID: {entry_id}): {text[:50]}...")
            
            # å¦‚æœæœ‰ä¸‰å…ƒç»„å’Œschemaä¿¡æ¯ï¼Œç”Ÿæˆå››ç§ç±»å‹çš„QAå¯¹
            if triple and schema and self.openai_api_key and self.openai_api_key != "your-openai-api-key-here":
                for prompt_type in ['sub', 'obj', 'rel', 'type']:
                    # ä¸ºæ¯ç§é—®é¢˜ç±»å‹å•ç‹¬è°ƒç”¨RAGç³»ç»Ÿ
                    rag_result = self.retrieval_engine.retrieve_and_rewrite(text, prompt_type=prompt_type)
                    
                    qa_pair = self._generate_qa_with_prompt_template(
                        text, triple, schema, prompt_type, rag_result, entry_id, output_filename
                    )
                    if qa_pair:
                        qa_pairs.append(qa_pair)
            else:
                # å¦‚æœæ²¡æœ‰ä¸‰å…ƒç»„ä¿¡æ¯æˆ–APIå¯†é’¥ï¼Œä½¿ç”¨ç®€åŒ–æ–¹æ³•
                rag_result = self.retrieval_engine.retrieve_and_rewrite(text)
                qa_pair = self._generate_simple_qa_from_rag(text, rag_result, entry_id, output_filename)
                if qa_pair:
                    qa_pairs.append(qa_pair)
                
        except Exception as e:
            print(f"âš  å¤„ç†æ–‡æœ¬æ—¶å‡ºé”™ (ID: {entry_id}): {e}")
        
        return qa_pairs
    
    def _generate_qa_with_prompt_template(self, text: str, triple: tuple, schema: tuple, 
                                         prompt_type: str, rag_result: Dict, entry_id: str, 
                                         output_filename: str = None) -> Optional[Dict]:
        """
        ä½¿ç”¨prompt_templatesæ„å»ºpromptï¼Œç„¶åå‘é€ç»™LLMç”ŸæˆQAå¯¹
        """
        qa_pair = None
        prompt = ""
        
        try:
            # 1. ä½¿ç”¨prompt_templatesæ„å»ºprompt
            prompt = build_prompt(text, triple, schema, prompt_type)
            
            # 2. æ·»åŠ RAGç³»ç»Ÿçš„ç»“æœä½œä¸ºä¸Šä¸‹æ–‡
            rag_context = f"""
**RAG System Results:**
- Retrieved Knowledge: {rag_result['cotkr_knowledge'][:300]}...
- Final Answer: {rag_result['final_answer']}
- Question Type: {rag_result['retrieval_stats']['question_type']}

**Your Task:**
{prompt}

**Output Format:**
Please provide only:
Question: [your question]
Answer: [your answer]
"""
            
            # 3. å‘é€ç»™LLM
            from openai import OpenAI
            client = OpenAI(api_key=self.openai_api_key)
            
            response = client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[
                    {
                        "role": "system", 
                        "content": "You are an expert at generating high-quality question-answer pairs from knowledge graphs and text. Follow the instructions precisely and provide concise, accurate responses."
                    },
                    {
                        "role": "user", 
                        "content": rag_context
                    }
                ],
                temperature=0.3,
                max_tokens=200
            )
            
            # 4. è§£æå“åº”
            content = response.choices[0].message.content.strip()
            question, answer = self._parse_qa_response(content)
            
            if question and answer:
                qa_pair = {
                    'question': question,
                    'answer': answer,
                    'question_type': prompt_type,
                    'source_text': text,
                    'triple': triple,
                    'schema': schema,
                    'entry_id': entry_id,
                    'generation_method': 'llm_with_prompt_template',
                    'rag_context': rag_result['cotkr_knowledge'][:200],
                    'rag_answer': rag_result['final_answer'],
                    'timestamp': datetime.now().isoformat()
                }
                
                # å¢é‡ä¿å­˜QAå¯¹åˆ°æ–‡ä»¶
                if output_filename:
                    self._append_qa_to_file(qa_pair, output_filename)
                    print(f"âœ… QAå¯¹å·²ä¿å­˜ (ç±»å‹: {prompt_type})")
            
        except Exception as e:
            print(f"âš  LLMç”Ÿæˆå¤±è´¥ (ç±»å‹: {prompt_type}): {e}")
        
        # è®°å½•åˆ°æ—¥å¿—æ–‡ä»¶
        self._log_qa_generation(entry_id, text, rag_result, rag_context, qa_pair, prompt_type)
        
        return qa_pair
    
    def _parse_qa_response(self, response: str) -> tuple:
        """
        è§£æLLMå“åº”ï¼Œæå–é—®é¢˜å’Œç­”æ¡ˆ
        """
        question = ""
        answer = ""
        
        lines = response.split('\n')
        for line in lines:
            line = line.strip()
            if line.startswith('Question:'):
                question = line.replace('Question:', '').strip()
            elif line.startswith('Answer:'):
                answer = line.replace('Answer:', '').strip()
        
        return question, answer
    
    def _generate_simple_qa_from_rag(self, text: str, rag_result: Dict, entry_id: str, 
                                    output_filename: str = None) -> Optional[Dict]:
        """
        å½“æ²¡æœ‰ä¸‰å…ƒç»„ä¿¡æ¯æ—¶ï¼ŒåŸºäºRAGç»“æœç”Ÿæˆç®€å•çš„QAå¯¹
        """
        qa_pair = None
        prompt = ""
        
        try:
            # æ„å»ºç®€å•çš„prompt
            prompt = f"""
Based on the following text and RAG system analysis, generate one high-quality question-answer pair.

Original Text: "{text}"

RAG Analysis:
- Retrieved Knowledge: {rag_result['cotkr_knowledge'][:200]}...
- System Answer: {rag_result['final_answer']}

Generate a natural question about the text that can be answered using the RAG analysis.

Format:
Question: [your question]
Answer: [your answer]
"""
            
            from openai import OpenAI
            client = OpenAI(api_key=self.openai_api_key)
            
            response = client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[
                    {
                        "role": "system", 
                        "content": "Generate a single, high-quality question-answer pair based on the provided text and analysis."
                    },
                    {
                        "role": "user", 
                        "content": prompt
                    }
                ],
                temperature=0.3,
                max_tokens=150
            )
            
            content = response.choices[0].message.content.strip()
            question, answer = self._parse_qa_response(content)
            
            if question and answer:
                qa_pair = {
                    'question': question,
                    'answer': answer,
                    'question_type': 'general',
                    'source_text': text,
                    'entry_id': entry_id,
                    'generation_method': 'llm_simple',
                    'rag_context': rag_result['cotkr_knowledge'][:200],
                    'rag_answer': rag_result['final_answer'],
                    'timestamp': datetime.now().isoformat()
                }
                
                # å¢é‡ä¿å­˜QAå¯¹åˆ°æ–‡ä»¶
                if output_filename:
                    self._append_qa_to_file(qa_pair, output_filename)
                    print(f"âœ… ç®€å•QAå¯¹å·²ä¿å­˜")
                
        except Exception as e:
            print(f"âš  ç®€å•QAç”Ÿæˆå¤±è´¥: {e}")
        
        # è®°å½•åˆ°æ—¥å¿—æ–‡ä»¶
        self._log_qa_generation(entry_id, text, rag_result, prompt, qa_pair, "general")
        
        return qa_pair
    

    
    def generate_qa_dataset_from_texts(self, max_texts: int = None, output_filename: str = None) -> List[Dict]:
        """
        ä»trainæ•°æ®é›†çš„XMLæ–‡ä»¶æ–‡æœ¬ç”ŸæˆQAæ•°æ®é›†ï¼ˆæ”¯æŒå¢é‡å†™å…¥ï¼‰
        
        æµç¨‹ï¼š
        1. éå†trainæ•°æ®é›†çš„XMLæ–‡ä»¶
        2. æå–<text>æ ‡ç­¾å†…å®¹
        3. å°†æ–‡æœ¬é€šè¿‡RAGç³»ç»Ÿå¤„ç†
        4. ä½¿ç”¨prompt_templatesæ„å»ºprompt
        5. å‘é€ç»™LLMç”ŸæˆQAå¯¹
        6. æ¯ç”Ÿæˆä¸€ä¸ªQAå¯¹å°±ç«‹å³å†™å…¥æ–‡ä»¶
        """
        print("ğŸš€ åŸºäºtrainæ•°æ®é›†æ–‡æœ¬çš„QAç”Ÿæˆï¼ˆå¢é‡å†™å…¥æ¨¡å¼ï¼‰")
        print("ğŸ¯ æµç¨‹: trainæ–‡æœ¬ â†’ RAGç³»ç»Ÿ â†’ promptæ¨¡æ¿ â†’ LLM â†’ QAå¯¹ â†’ ç«‹å³ä¿å­˜")
        print("=" * 70)
        
        # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
        if not output_filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            text_count = max_texts if max_texts else 'all'
            output_filename = f"train_text_qa_dataset_{text_count}_{timestamp}.json"
        
        print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶: {output_filename}")
        print(f"ğŸ“ æ—¥å¿—æ–‡ä»¶: {self.log_file}")
        
        # 1. ä»trainæ•°æ®é›†æå–æ–‡æœ¬
        print("ğŸ“š ä»trainæ•°æ®é›†æå–å…¨éƒ¨æ–‡æœ¬...")
        texts = self.extract_texts_from_xml_files()
        
        if not texts:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°æ–‡æœ¬å†…å®¹")
            return []
        
        # 2. å¤„ç†æ•°é‡æ§åˆ¶
        if max_texts and len(texts) > max_texts:
            texts = texts[:max_texts]
            print(f"ğŸ“Š é™åˆ¶å¤„ç†æ•°é‡ä¸º {max_texts} ä¸ªæ–‡æœ¬ï¼ˆæµ‹è¯•æ¨¡å¼ï¼‰")
        else:
            print(f"ğŸ“Š å¤„ç†å…¨éƒ¨ {len(texts)} ä¸ªæ–‡æœ¬")
        
        # 3. æ£€æŸ¥RAGç³»ç»ŸçŠ¶æ€
        print("ğŸ”§ æ£€æŸ¥RAGç³»ç»ŸçŠ¶æ€...")
        try:
            system_status = self.retrieval_engine.get_system_status()
            db_count = system_status['database_status']['total_documents']
            print(f"   - å‘é‡æ•°æ®åº“æ–‡æ¡£æ•°: {db_count}")
            if db_count == 0:
                print("âš  è­¦å‘Š: å‘é‡æ•°æ®åº“ä¸ºç©ºï¼ŒRAGç³»ç»Ÿå¯èƒ½æ— æ³•æ­£å¸¸å·¥ä½œ")
        except Exception as e:
            print(f"âš  æ— æ³•è·å–ç³»ç»ŸçŠ¶æ€: {e}")
        
        # 4. ä¸ºæ¯ä¸ªæ–‡æœ¬ç”ŸæˆQAå¯¹ï¼ˆå¢é‡å†™å…¥ï¼‰
        all_qa_pairs = []
        total_saved = 0
        
        print(f"ğŸ”„ å¼€å§‹å¤„ç† {len(texts)} ä¸ªæ–‡æœ¬...")
        print("   æ¯ä¸ªæ–‡æœ¬å°†é€šè¿‡: æ–‡æœ¬ â†’ RAGæ£€ç´¢ â†’ promptæ„å»º â†’ LLMç”Ÿæˆ â†’ ç«‹å³ä¿å­˜")
        
        for i, text_item in enumerate(tqdm(texts, desc="ç”ŸæˆQAå¯¹"), 1):
            try:
                qa_pairs = self.generate_qa_from_text_via_rag(text_item, output_filename)
                all_qa_pairs.extend(qa_pairs)
                total_saved += len(qa_pairs)
                
                # æ¯å¤„ç†10ä¸ªæ–‡æœ¬æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦
                if i % 10 == 0:
                    print(f"ğŸ“Š å·²å¤„ç† {i}/{len(texts)} ä¸ªæ–‡æœ¬ï¼Œç´¯è®¡ç”Ÿæˆ {total_saved} ä¸ªQAå¯¹")
                    
            except Exception as e:
                print(f"âš  å¤„ç†æ–‡æœ¬ {text_item.get('id', 'unknown')} æ—¶å‡ºé”™: {e}")
                continue
        
        # 5. å®Œæˆæ–‡ä»¶å†™å…¥
        self._finalize_qa_file(output_filename)
        
        print(f"âœ… ç”Ÿæˆå®Œæˆï¼Œå…± {len(all_qa_pairs)} ä¸ªQAå¯¹")
        print(f"ğŸ’¾ æ‰€æœ‰QAå¯¹å·²ä¿å­˜åˆ°: {Path(self.output_dir) / output_filename}")
        print(f"ğŸ“ è¯¦ç»†æ—¥å¿—å·²ä¿å­˜åˆ°: {self.log_file}")
        
        # 6. ç»Ÿè®¡ä¿¡æ¯
        method_counts = {}
        type_counts = {}
        
        for qa in all_qa_pairs:
            method = qa.get('generation_method', 'unknown')
            method_counts[method] = method_counts.get(method, 0) + 1
            
            q_type = qa.get('question_type', 'general')
            type_counts[q_type] = type_counts.get(q_type, 0) + 1
        
        print(f"\nğŸ“Š ç”Ÿæˆæ–¹æ³•ç»Ÿè®¡:")
        for method, count in method_counts.items():
            print(f"   {method}: {count} ä¸ª")
        
        print(f"\nğŸ“Š é—®é¢˜ç±»å‹ç»Ÿè®¡:")
        for q_type, count in type_counts.items():
            print(f"   {q_type}: {count} ä¸ª")
        
        return all_qa_pairs
    
    def save_qa_dataset(self, qa_dataset: List[Dict], filename: str = "text_based_qa_dataset.json") -> str:
        """ä¿å­˜QAæ•°æ®é›†åˆ°æ–‡ä»¶"""
        filepath = Path(self.output_dir) / filename
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(qa_dataset, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ QAæ•°æ®é›†å·²ä¿å­˜åˆ°: {filepath}")
        return str(filepath)
    
    def _append_qa_to_file(self, qa_pair: Dict, filename: str):
        """å¢é‡è¿½åŠ QAå¯¹åˆ°æ–‡ä»¶"""
        filepath = Path(self.output_dir) / filename
        
        # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œåˆ›å»ºæ–°æ–‡ä»¶å¹¶å†™å…¥æ•°ç»„å¼€å§‹ç¬¦å·
        if not filepath.exists():
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write("[\n")
                json.dump(qa_pair, f, ensure_ascii=False, indent=2)
        else:
            # è¯»å–ç°æœ‰æ–‡ä»¶å†…å®¹
            with open(filepath, 'r', encoding='utf-8') as f:
                content = f.read().strip()
            
            # ç§»é™¤æœ€åçš„ ] ç¬¦å·ï¼Œæ·»åŠ é€—å·å’Œæ–°çš„QAå¯¹
            if content.endswith(']'):
                content = content[:-1].rstrip()
                if not content.endswith('['):
                    content += ","
            
            # å†™å›æ–‡ä»¶
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(content)
                if not content.endswith('['):
                    f.write("\n")
                f.write("  ")
                json.dump(qa_pair, f, ensure_ascii=False, indent=2)
                f.write("\n]")
    
    def _finalize_qa_file(self, filename: str):
        """å®ŒæˆQAæ–‡ä»¶çš„å†™å…¥ï¼ˆç¡®ä¿JSONæ ¼å¼æ­£ç¡®ï¼‰"""
        filepath = Path(self.output_dir) / filename
        
        if filepath.exists():
            with open(filepath, 'r', encoding='utf-8') as f:
                content = f.read().strip()
            
            # ç¡®ä¿æ–‡ä»¶ä»¥ ] ç»“å°¾
            if not content.endswith(']'):
                with open(filepath, 'a', encoding='utf-8') as f:
                    f.write("\n]")

# æµ‹è¯•å‡½æ•°
def test_text_based_qa_generator():
    """æµ‹è¯•åŸºäºæ–‡æœ¬çš„QAç”Ÿæˆå™¨"""
    print("ğŸ§ª æµ‹è¯•æ”¹é€ åçš„åŸºäºæ–‡æœ¬çš„QAç”Ÿæˆå™¨")
    print("ğŸ¯ æµ‹è¯•æµç¨‹: trainæ–‡æœ¬ â†’ RAGç³»ç»Ÿ â†’ promptæ¨¡æ¿ â†’ LLM")
    print("=" * 60)
    
    generator = TextBasedQAGenerator()
    
    # æµ‹è¯•æ–‡æœ¬æå–
    print("1ï¸âƒ£ æµ‹è¯•ä»trainæ•°æ®é›†æå–æ–‡æœ¬...")
    texts = generator.extract_texts_from_xml_files()
    
    if texts:
        print(f"âœ… æˆåŠŸæå– {len(texts)} ä¸ªæ–‡æœ¬")
        
        # æ˜¾ç¤ºç¤ºä¾‹
        sample = texts[0]
        print(f"ğŸ“‹ ç¤ºä¾‹æ–‡æœ¬:")
        print(f"   ID: {sample['id']}")
        print(f"   æ–‡æœ¬: {sample['text'][:100]}...")
        print(f"   ä¸‰å…ƒç»„: {sample.get('triple', 'N/A')}")
        print(f"   Schema: {sample.get('schema', 'N/A')}")
        
        # æµ‹è¯•QAç”Ÿæˆ
        print("\n2ï¸âƒ£ æµ‹è¯•QAç”Ÿæˆæµç¨‹...")
        qa_pairs = generator.generate_qa_from_text_via_rag(sample)
        
        if qa_pairs:
            print(f"âœ… æˆåŠŸç”Ÿæˆ {len(qa_pairs)} ä¸ªQAå¯¹")
            for i, qa in enumerate(qa_pairs, 1):
                print(f"\n{i}. ç±»å‹: {qa.get('question_type', 'N/A')}")
                print(f"   é—®é¢˜: {qa['question']}")
                print(f"   ç­”æ¡ˆ: {qa['answer']}")
                print(f"   æ–¹æ³•: {qa.get('generation_method', 'unknown')}")
                if 'rag_answer' in qa:
                    print(f"   RAGç­”æ¡ˆ: {qa['rag_answer']}")
        else:
            print("âŒ QAç”Ÿæˆå¤±è´¥")
            
        # æµ‹è¯•å°è§„æ¨¡æ•°æ®é›†ç”Ÿæˆ
        print("\n3ï¸âƒ£ æµ‹è¯•å°è§„æ¨¡æ•°æ®é›†ç”Ÿæˆ...")
        qa_dataset = generator.generate_qa_dataset_from_texts(max_texts=2)
        
        if qa_dataset:
            print(f"âœ… æ•°æ®é›†ç”ŸæˆæˆåŠŸï¼Œå…± {len(qa_dataset)} ä¸ªQAå¯¹")
        else:
            print("âŒ æ•°æ®é›†ç”Ÿæˆå¤±è´¥")
            
    else:
        print("âŒ æ–‡æœ¬æå–å¤±è´¥")

if __name__ == '__main__':
    test_text_based_qa_generator()