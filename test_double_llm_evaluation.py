#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
æµ‹è¯•åŒLLMè°ƒç”¨çš„RAG vs LLMè¯„ä¼°
éªŒè¯ç°åœ¨çš„è¯„ä¼°ç¡®å®æ˜¯ï¼šRAG(ä½¿ç”¨LLM) vs çº¯LLM
"""

from rag_vs_llm_evaluation import RAGvsLLMEvaluator
import config

def test_double_llm_evaluation():
    """æµ‹è¯•åŒLLMè°ƒç”¨çš„è¯„ä¼°æµç¨‹"""
    print("ğŸ”„ æµ‹è¯•åŒLLMè°ƒç”¨çš„RAG vs LLMè¯„ä¼°")
    print("=" * 60)
    
    # æ£€æŸ¥APIå¯†é’¥
    if not config.OPENAI_API_KEY or config.OPENAI_API_KEY == "your-openai-api-key-here":
        print("âŒ OpenAI APIå¯†é’¥æœªé…ç½®ï¼Œæ— æ³•è¿›è¡Œæµ‹è¯•")
        return
    
    print("âœ… OpenAI APIå¯†é’¥å·²é…ç½®")
    
    # åˆå§‹åŒ–è¯„ä¼°å™¨
    evaluator = RAGvsLLMEvaluator()
    
    # æµ‹è¯•é—®é¢˜
    test_question = "Who is the leader of Belgium?"
    expected_answer = "Philippe of Belgium"
    
    print(f"\nğŸ“ æµ‹è¯•é—®é¢˜: {test_question}")
    print(f"ğŸ¯ æœŸæœ›ç­”æ¡ˆ: {expected_answer}")
    print("-" * 60)
    
    try:
        # æ‰§è¡Œå•ä¸ªé—®é¢˜çš„è¯„ä¼°
        qa_item = {
            'question': test_question,
            'expected_answer': expected_answer
        }
        result = evaluator.evaluate_single_question(qa_item)
        
        print("ğŸ“Š è¯„ä¼°ç»“æœ:")
        print(f"   RAGç­”æ¡ˆ: {result['rag_answer']}")
        print(f"   çº¯LLMç­”æ¡ˆ: {result['llm_answer']}")
        
        print(f"\nğŸ“ˆ RAGç³»ç»Ÿè¯„åˆ†:")
        for metric, score in result['rag_scores'].items():
            print(f"   {metric}: {score:.4f}")
        
        print(f"\nğŸ“ˆ çº¯LLMç³»ç»Ÿè¯„åˆ†:")
        for metric, score in result['llm_scores'].items():
            print(f"   {metric}: {score:.4f}")
        
        # åˆ†æLLMä½¿ç”¨æƒ…å†µ
        print(f"\nğŸ” LLMä½¿ç”¨åˆ†æ:")
        print(f"   RAGç³»ç»Ÿ: ä½¿ç”¨LLMè¿›è¡Œç­”æ¡ˆç”Ÿæˆ (åŸºäºCoTKRæ¨ç†é“¾)")
        print(f"   çº¯LLMç³»ç»Ÿ: ç›´æ¥ä½¿ç”¨LLMå›ç­”é—®é¢˜")
        print(f"   æ€»LLMè°ƒç”¨æ¬¡æ•°: 2æ¬¡ (æ¯ä¸ªé—®é¢˜)")
        
        # æ£€ç´¢æŒ‡æ ‡
        if 'rag_retrieval_metrics' in result:
            print(f"\nğŸ“‹ RAGæ£€ç´¢æŒ‡æ ‡:")
            for metric, score in result['rag_retrieval_metrics'].items():
                print(f"   {metric}: {score:.4f}")
        
    except Exception as e:
        print(f"âŒ è¯„ä¼°å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

def analyze_evaluation_architecture():
    """åˆ†æè¯„ä¼°æ¶æ„çš„å˜åŒ–"""
    print("\nğŸ—ï¸ è¯„ä¼°æ¶æ„åˆ†æ")
    print("=" * 60)
    
    print("ğŸ“ˆ ä¿®æ”¹å‰çš„è¯„ä¼°:")
    print("   RAGç³»ç»Ÿ: æ£€ç´¢ â†’ CoTKRé‡å†™ â†’ è§„åˆ™å¼æå– â†’ ç­”æ¡ˆ")
    print("   çº¯LLMç³»ç»Ÿ: é—®é¢˜ â†’ LLM â†’ ç­”æ¡ˆ")
    print("   å¯¹æ¯”: è§„åˆ™å¼RAG vs çº¯LLM")
    
    print("\nğŸ“ˆ ä¿®æ”¹åçš„è¯„ä¼°:")
    print("   RAGç³»ç»Ÿ: æ£€ç´¢ â†’ CoTKRé‡å†™ â†’ LLMç”Ÿæˆ â†’ ç­”æ¡ˆ")
    print("   çº¯LLMç³»ç»Ÿ: é—®é¢˜ â†’ LLM â†’ ç­”æ¡ˆ")
    print("   å¯¹æ¯”: å¢å¼ºRAG(å¸¦LLM) vs çº¯LLM")
    
    print("\nğŸ¯ å…³é”®å·®å¼‚:")
    print("   1. RAGç³»ç»Ÿç°åœ¨ä¹Ÿä½¿ç”¨LLMï¼Œä½†åŸºäºç»“æ„åŒ–çš„CoTKRæ¨ç†é“¾")
    print("   2. çº¯LLMç³»ç»Ÿç›´æ¥å›ç­”ï¼Œæ²¡æœ‰å¤–éƒ¨çŸ¥è¯†æ”¯æŒ")
    print("   3. è¯„ä¼°å˜æˆäº†ï¼š'æœ‰çŸ¥è¯†æ”¯æŒçš„LLM' vs 'æ— çŸ¥è¯†æ”¯æŒçš„LLM'")
    
    print("\nğŸ’¡ è¯„ä¼°æ„ä¹‰:")
    print("   - æµ‹è¯•å¤–éƒ¨çŸ¥è¯†æ£€ç´¢å’Œç»“æ„åŒ–æ¨ç†çš„ä»·å€¼")
    print("   - å¯¹æ¯”æœ‰æ— RAGæ”¯æŒçš„LLMæ€§èƒ½å·®å¼‚")
    print("   - éªŒè¯CoTKRé‡å†™æ˜¯å¦æå‡äº†LLMçš„æ¨ç†èƒ½åŠ›")

def estimate_api_costs():
    """ä¼°ç®—APIæˆæœ¬"""
    print("\nğŸ’° APIæˆæœ¬ä¼°ç®—")
    print("=" * 60)
    
    print("ğŸ”¢ æ¯ä¸ªé—®é¢˜çš„LLMè°ƒç”¨:")
    print("   RAGç³»ç»Ÿ: 1æ¬¡LLMè°ƒç”¨ (ç­”æ¡ˆç”Ÿæˆ)")
    print("   çº¯LLMç³»ç»Ÿ: 1æ¬¡LLMè°ƒç”¨ (ç›´æ¥å›ç­”)")
    print("   æ€»è®¡: 2æ¬¡LLMè°ƒç”¨/é—®é¢˜")
    
    print("\nğŸ“Š æˆæœ¬ä¼°ç®— (åŸºäºgpt-3.5-turbo):")
    print("   è¾“å…¥token: ~200-300 tokens/è°ƒç”¨")
    print("   è¾“å‡ºtoken: ~50-100 tokens/è°ƒç”¨")
    print("   æˆæœ¬: ~$0.002-0.004/é—®é¢˜")
    
    print("\nâš ï¸ æ³¨æ„äº‹é¡¹:")
    print("   - å¤§è§„æ¨¡è¯„ä¼°æ—¶æˆæœ¬ä¼šå¿«é€Ÿç´¯ç§¯")
    print("   - å»ºè®®å…ˆç”¨å°æ ·æœ¬æµ‹è¯•")
    print("   - å¯ä»¥è®¾ç½®APIè°ƒç”¨é™åˆ¶")

def main():
    """ä¸»å‡½æ•°"""
    try:
        test_double_llm_evaluation()
        analyze_evaluation_architecture()
        estimate_api_costs()
        
        print("\n" + "=" * 60)
        print("âœ… åˆ†æå®Œæˆ")
        print("\nğŸ‰ æ€»ç»“:")
        print("   âœ… ç°åœ¨çš„RAG vs LLMè¯„ä¼°ç¡®å®æ˜¯åŒLLMè°ƒç”¨")
        print("   âœ… RAGç³»ç»Ÿä½¿ç”¨LLMè¿›è¡Œæœ€ç»ˆç­”æ¡ˆç”Ÿæˆ")
        print("   âœ… è¯„ä¼°å¯¹æ¯”çš„æ˜¯'å¢å¼ºRAG' vs 'çº¯LLM'")
        print("   âœ… è¿™æ ·çš„å¯¹æ¯”æ›´èƒ½ä½“ç°RAGç³»ç»Ÿçš„ä»·å€¼")
        
    except Exception as e:
        print(f"âŒ åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()