#!/usr/bin/env python3
# test_enhanced_rag_vs_llm_evaluation.py - æµ‹è¯•å¢å¼ºçš„RAG vs LLMè¯„ä¼°ç³»ç»Ÿ

import json
import random
from pathlib import Path
from rag_vs_llm_evaluation import RAGvsLLMEvaluator

def load_qa_dataset_questions(dataset_path: str = None, num_questions: int = 5):
    """ä»QAæ•°æ®é›†ä¸­éšæœºåŠ è½½æµ‹è¯•é—®é¢˜"""
    
    # å°è¯•å¤šä¸ªå¯èƒ½çš„QAæ•°æ®é›†ä½ç½®
    possible_paths = [
        "qa_datasets",  # å½“å‰ç›®å½•ä¸‹çš„qa_datasets
        "../qa_datasets",  # ä¸Šçº§ç›®å½•çš„qa_datasets
        ".",  # å½“å‰ç›®å½•ï¼ˆæŸ¥æ‰¾æ‰€æœ‰jsonæ–‡ä»¶ï¼‰
    ]
    
    if dataset_path:
        possible_paths.insert(0, dataset_path)
    
    qa_files = []
    used_path = None
    
    for path in possible_paths:
        print(f"ğŸ“ å°è¯•ä» {path} åŠ è½½QAæ•°æ®é›†...")
        try:
            path_obj = Path(path)
            if path_obj.exists():
                files = list(path_obj.glob("*.json"))
                # è¿‡æ»¤å‡ºçœ‹èµ·æ¥åƒQAæ•°æ®é›†çš„æ–‡ä»¶
                qa_files = [f for f in files if any(keyword in f.name.lower() 
                           for keyword in ['qa', 'question', 'dataset', 'enhanced'])]
                if qa_files:
                    used_path = path
                    print(f"âœ… åœ¨ {path} ä¸­æ‰¾åˆ° {len(qa_files)} ä¸ªQAæ•°æ®é›†æ–‡ä»¶")
                    break
                else:
                    print(f"âš  åœ¨ {path} ä¸­æœªæ‰¾åˆ°QAæ•°æ®é›†æ–‡ä»¶")
            else:
                print(f"âš  è·¯å¾„ {path} ä¸å­˜åœ¨")
        except Exception as e:
            print(f"âš  æ£€æŸ¥è·¯å¾„ {path} æ—¶å‡ºé”™: {e}")
    
    if not qa_files:
        print(f"âŒ åœ¨æ‰€æœ‰å¯èƒ½çš„ä½ç½®éƒ½æœªæ‰¾åˆ°QAæ•°æ®é›†æ–‡ä»¶")
        return []
    
    all_questions = []
    
    print(f"ğŸ“„ æ‰¾åˆ°çš„QAæ•°æ®é›†æ–‡ä»¶:")
    for qa_file in qa_files:
        print(f"   - {qa_file.name}")
    
    for qa_file in qa_files:
        try:
            with open(qa_file, 'r', encoding='utf-8') as f:
                qa_data = json.load(f)
            
            file_questions = 0
            for qa_item in qa_data:
                if 'question' in qa_item and 'answer' in qa_item:
                    all_questions.append({
                        'question': qa_item['question'],
                        'expected_answer': qa_item['answer'],
                        'question_type': qa_item.get('question_type', 'unknown'),
                        'source_text': qa_item.get('source_text', ''),
                        'triple': qa_item.get('triple'),
                        'schema': qa_item.get('schema'),
                        'source_file': qa_file.name
                    })
                    file_questions += 1
            
            print(f"   âœ… {qa_file.name}: {file_questions} ä¸ªé—®é¢˜")
                    
        except Exception as e:
            print(f"âš  åŠ è½½æ–‡ä»¶ {qa_file} æ—¶å‡ºé”™: {e}")
    
    print(f"ğŸ“Š æ€»è®¡åŠ è½½: {len(all_questions)} ä¸ªé—®é¢˜")
    
    # éšæœºé€‰æ‹©æŒ‡å®šæ•°é‡çš„é—®é¢˜
    if len(all_questions) > num_questions:
        selected_questions = random.sample(all_questions, num_questions)
        print(f"ğŸ² éšæœºé€‰æ‹©äº† {num_questions} ä¸ªé—®é¢˜è¿›è¡Œæµ‹è¯•")
    else:
        selected_questions = all_questions
        print(f"âœ… ä½¿ç”¨å…¨éƒ¨ {len(selected_questions)} ä¸ªé—®é¢˜")
    
    return selected_questions

def test_enhanced_evaluation():
    """æµ‹è¯•å¢å¼ºçš„è¯„ä¼°åŠŸèƒ½"""
    print("ğŸ§ª æµ‹è¯•å¢å¼ºçš„RAG vs LLMè¯„ä¼°ç³»ç»Ÿ")
    print("=" * 50)
    
    # åˆ›å»ºè¯„ä¼°å™¨
    evaluator = RAGvsLLMEvaluator()
    
    # ä»QAæ•°æ®é›†ä¸­åŠ è½½æµ‹è¯•é—®é¢˜
    test_questions = load_qa_dataset_questions(num_questions=5)
    
    if not test_questions:
        print("âŒ æ— æ³•åŠ è½½æµ‹è¯•é—®é¢˜ï¼Œä½¿ç”¨é»˜è®¤é—®é¢˜")
        # å¤‡ç”¨çš„ç¡¬ç¼–ç é—®é¢˜
        test_questions = [
            {
                'question': 'Who is the leader of Belgium?',
                'expected_answer': 'Alexander De Croo',
                'question_type': 'factual'
            },
            {
                'question': 'What is the capital of Netherlands?',
                'expected_answer': 'Amsterdam',
                'question_type': 'factual'
            }
        ]
    
    print(f"ğŸ“ æµ‹è¯• {len(test_questions)} ä¸ªé—®é¢˜")
    
    # æµ‹è¯•å•ä¸ªé—®é¢˜è¯„ä¼°
    print("\nğŸ” æµ‹è¯•å•ä¸ªé—®é¢˜è¯„ä¼°:")
    for i, qa_item in enumerate(test_questions, 1):
        print(f"\né—®é¢˜ {i}: {qa_item['question']}")
        
        result = evaluator.evaluate_single_question(qa_item)
        
        print(f"  æœŸæœ›ç­”æ¡ˆ: {result['expected_answer']}")
        print(f"  RAGç­”æ¡ˆ: {result['rag_answer']}")
        print(f"  LLMç­”æ¡ˆ: {result['llm_answer']}")
        
        # æ˜¾ç¤ºæ£€ç´¢æŒ‡æ ‡
        if 'rag_retrieval_metrics' in result:
            metrics = result['rag_retrieval_metrics']
            print(f"  æ£€ç´¢æŒ‡æ ‡:")
            print(f"    Precision@1: {metrics.get('precision@1', 0):.3f}")
            print(f"    Recall@1: {metrics.get('recall@1', 0):.3f}")
            print(f"    nDCG@1: {metrics.get('ndcg@1', 0):.3f}")
        
        # æ˜¾ç¤ºç­”æ¡ˆè´¨é‡åˆ†æ•°
        print(f"  RAGç»¼åˆåˆ†æ•°: {result['rag_scores']['composite_score']:.3f}")
        print(f"  LLMç»¼åˆåˆ†æ•°: {result['llm_scores']['composite_score']:.3f}")
    
    # æµ‹è¯•å®Œæ•´è¯„ä¼°æµç¨‹
    print(f"\nğŸš€ è¿è¡Œå®Œæ•´è¯„ä¼°æµç¨‹...")
    
    results = evaluator.evaluate_dataset(test_questions)
    
    # ä¿å­˜ç»“æœ
    evaluator.save_evaluation_results(results, output_dir="test_evaluation")
    
    # æ‰“å°æŠ¥å‘Š
    evaluator.print_summary_report(results)
    
    # éªŒè¯ç”Ÿæˆçš„æ–‡ä»¶
    print(f"\nğŸ“ éªŒè¯ç”Ÿæˆçš„æ–‡ä»¶:")
    test_dir = Path("test_evaluation")
    if test_dir.exists():
        files = list(test_dir.glob("*"))
        for file in files:
            print(f"  âœ… {file.name} ({file.stat().st_size} bytes)")
            
            # æ£€æŸ¥ç®€åŒ–é—®ç­”è®°å½•æ–‡ä»¶
            if file.name.startswith("simple_qa_records_") and file.suffix == ".jsonl":
                print(f"    ğŸ“„ æ£€æŸ¥ç®€åŒ–é—®ç­”è®°å½•:")
                with open(file, 'r', encoding='utf-8') as f:
                    lines = f.readlines()
                    print(f"      - æ€»è¡Œæ•°: {len(lines)}")
                    if lines:
                        # è§£æç¬¬ä¸€è¡Œ
                        first_record = json.loads(lines[0])
                        print(f"      - ç¬¬ä¸€æ¡è®°å½•å­—æ®µ: {list(first_record.keys())}")
                        print(f"      - ç¤ºä¾‹é—®é¢˜: {first_record.get('question', 'N/A')[:50]}...")
    
    print(f"\nâœ… å¢å¼ºè¯„ä¼°ç³»ç»Ÿæµ‹è¯•å®Œæˆ!")

def test_retrieval_metrics():
    """æµ‹è¯•æ£€ç´¢æŒ‡æ ‡è®¡ç®—"""
    print("\nğŸ¯ æµ‹è¯•æ£€ç´¢æŒ‡æ ‡è®¡ç®—")
    print("-" * 30)
    
    evaluator = RAGvsLLMEvaluator()
    
    # æ¨¡æ‹Ÿæ£€ç´¢ç»“æœ
    mock_retrieved_items = [
        {
            'text': 'Alexander De Croo is the Prime Minister of Belgium',
            'distance': 0.1,
            'triple': ('Alexander De Croo', 'is Prime Minister of', 'Belgium')
        },
        {
            'text': 'Belgium is a country in Europe',
            'distance': 0.3,
            'triple': ('Belgium', 'is located in', 'Europe')
        },
        {
            'text': 'Brussels is the capital of Belgium',
            'distance': 0.5,
            'triple': ('Brussels', 'is capital of', 'Belgium')
        }
    ]
    
    expected_answer = "Alexander De Croo"
    
    # è®¡ç®—æ£€ç´¢æŒ‡æ ‡
    metrics = evaluator.calculate_retrieval_metrics(
        mock_retrieved_items, expected_answer, k_values=[1, 2, 3]
    )
    
    print("æ£€ç´¢æŒ‡æ ‡ç»“æœ:")
    for metric, score in metrics.items():
        print(f"  {metric}: {score:.4f}")

if __name__ == '__main__':
    # æµ‹è¯•æ£€ç´¢æŒ‡æ ‡è®¡ç®—
    test_retrieval_metrics()
    
    # æµ‹è¯•å®Œæ•´è¯„ä¼°ç³»ç»Ÿ
    test_enhanced_evaluation()