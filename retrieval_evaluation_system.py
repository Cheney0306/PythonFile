# retrieval_evaluation_system.py - æ£€ç´¢è¯„ä¼°ç³»ç»Ÿ

import json
import random
import numpy as np
from typing import List, Dict, Tuple, Optional
from pathlib import Path
from tqdm import tqdm
import math
from collections import defaultdict

# å¯¼å…¥æˆ‘ä»¬çš„ç³»ç»Ÿ
from retrieval_engine import RetrievalEngine  # åŸå§‹ç³»ç»Ÿ
from enhanced_retrieval_engine import EnhancedRetrievalEngine  # å¢å¼ºç³»ç»Ÿ
import config  # å¯¼å…¥é…ç½®

class RetrievalEvaluator:
    """æ£€ç´¢ç³»ç»Ÿè¯„ä¼°å™¨ - è®¡ç®—Precision@K, Recall@K, nDCG@Kç­‰æŒ‡æ ‡"""
    
    def __init__(self):
        self.original_engine = RetrievalEngine()
        self.enhanced_engine = EnhancedRetrievalEngine()
        
    def load_qa_dataset(self, dataset_path: str = "qa_datasets", limit: int = 100, scan_all: bool = False) -> List[Dict]:
        """
        ä»QAæ•°æ®é›†ä¸­åŠ è½½é—®é¢˜ç”¨äºè¯„ä¼°
        
        Args:
            dataset_path: QAæ•°æ®é›†è·¯å¾„
            limit: é™åˆ¶åŠ è½½çš„é—®é¢˜æ•°é‡ (scan_all=Trueæ—¶å¿½ç•¥)
            scan_all: æ˜¯å¦æ‰«æå…¨éƒ¨æ•°æ®ï¼Œå¿½ç•¥limité™åˆ¶
        """
        qa_files = list(Path(dataset_path).glob("*.json"))
        
        if not qa_files:
            print(f"âŒ åœ¨ {dataset_path} ä¸­æœªæ‰¾åˆ°QAæ•°æ®é›†æ–‡ä»¶")
            return []
        
        all_questions = []
        
        print(f"ğŸ“ æ‰¾åˆ° {len(qa_files)} ä¸ªQAæ•°æ®é›†æ–‡ä»¶:")
        for qa_file in qa_files:
            print(f"   - {qa_file.name}")
        
        for qa_file in qa_files:
            try:
                with open(qa_file, 'r', encoding='utf-8') as f:
                    qa_data = json.load(f)
                
                file_questions = 0
                for qa_item in qa_data:
                    if 'question' in qa_item and 'answer' in qa_item:
                        all_questions.append({
                            'question': qa_item['question'],
                            'expected_answer': qa_item['answer'],
                            'question_type': qa_item.get('question_type', 'unknown'),
                            'source_text': qa_item.get('source_text', ''),
                            'triple': qa_item.get('triple'),
                            'schema': qa_item.get('schema'),
                            'source_file': qa_file.name
                        })
                        file_questions += 1
                
                print(f"   âœ… {qa_file.name}: {file_questions} ä¸ªé—®é¢˜")
                        
            except Exception as e:
                print(f"âš  åŠ è½½æ–‡ä»¶ {qa_file} æ—¶å‡ºé”™: {e}")
        
        print(f"ğŸ“Š æ€»è®¡åŠ è½½: {len(all_questions)} ä¸ªé—®é¢˜")
        
        # æ ¹æ®scan_allå‚æ•°å†³å®šæ˜¯å¦é™åˆ¶æ•°é‡
        if scan_all:
            print(f"ğŸ” æ‰«æå…¨éƒ¨æ¨¡å¼: ä½¿ç”¨å…¨éƒ¨ {len(all_questions)} ä¸ªé—®é¢˜")
            final_questions = all_questions
        else:
            # éšæœºé‡‡æ ·å¹¶é™åˆ¶æ•°é‡
            if len(all_questions) > limit:
                final_questions = random.sample(all_questions, limit)
                print(f"ğŸ² éšæœºé‡‡æ ·: ä» {len(all_questions)} ä¸ªé—®é¢˜ä¸­é€‰æ‹© {len(final_questions)} ä¸ª")
            else:
                final_questions = all_questions
                print(f"âœ… ä½¿ç”¨å…¨éƒ¨ {len(final_questions)} ä¸ªé—®é¢˜")
        
        print(f"âœ… æœ€ç»ˆè¯„ä¼°é—®é¢˜æ•°: {len(final_questions)}")
        return final_questions
    
    def create_ground_truth_relevance(self, question_data: Dict, retrieved_items: List[Dict]) -> List[int]:
        """
        åˆ›å»ºç›¸å…³æ€§æ ‡æ³¨ (0: ä¸ç›¸å…³, 1: ç›¸å…³, 2: é«˜åº¦ç›¸å…³)
        
        åŸºäºé—®é¢˜çš„é¢„æœŸç­”æ¡ˆå’Œä¸‰å…ƒç»„ä¿¡æ¯åˆ¤æ–­æ£€ç´¢ç»“æœçš„ç›¸å…³æ€§
        """
        relevance_scores = []
        expected_answer = question_data['expected_answer'].lower()
        question_lower = question_data['question'].lower()
        expected_triple = question_data.get('triple')
        
        for item in retrieved_items:
            score = 0
            triple = item['triple']
            sub, rel, obj = triple
            
            # æ¸…ç†å®ä½“åç§°
            sub_clean = sub.replace('_', ' ').lower()
            obj_clean = obj.replace('_', ' ').lower()
            rel_clean = rel.replace('_', ' ').lower()
            
            # 1. æ£€æŸ¥æ˜¯å¦åŒ…å«é¢„æœŸç­”æ¡ˆ
            if expected_answer in sub_clean or expected_answer in obj_clean:
                score += 2
            
            # 2. æ£€æŸ¥ä¸‰å…ƒç»„åŒ¹é…
            if expected_triple:
                expected_sub, expected_rel, expected_obj = expected_triple
                if (sub == expected_sub and rel == expected_rel and obj == expected_obj):
                    score = 2  # å®Œå…¨åŒ¹é…
                elif (sub == expected_sub or obj == expected_obj) and rel == expected_rel:
                    score = max(score, 1)  # éƒ¨åˆ†åŒ¹é…
            
            # 3. æ£€æŸ¥å®ä½“åœ¨é—®é¢˜ä¸­çš„å‡ºç°
            question_entities = self._extract_entities_from_question(question_lower)
            if any(entity in sub_clean or entity in obj_clean for entity in question_entities):
                score = max(score, 1)
            
            # 4. æ£€æŸ¥å…³ç³»ç›¸å…³æ€§
            if self._is_relation_relevant(question_lower, rel_clean):
                score = max(score, 1)
            
            relevance_scores.append(min(score, 2))  # é™åˆ¶åœ¨0-2èŒƒå›´
        
        return relevance_scores
    
    def _extract_entities_from_question(self, question: str) -> List[str]:
        """ä»é—®é¢˜ä¸­æå–å¯èƒ½çš„å®ä½“åç§°"""
        # ç®€å•çš„å®ä½“æå–é€»è¾‘
        entities = []
        
        # å¸¸è§çš„å®ä½“æ¨¡å¼
        words = question.split()
        for i, word in enumerate(words):
            # å¤§å†™å¼€å¤´çš„è¯å¯èƒ½æ˜¯å®ä½“
            if word[0].isupper() and len(word) > 2:
                entities.append(word.lower())
            
            # è¿ç»­çš„å¤§å†™è¯ç»„åˆ
            if i < len(words) - 1 and word[0].isupper() and words[i+1][0].isupper():
                entities.append(f"{word} {words[i+1]}".lower())
        
        # ç‰¹å®šå®ä½“
        known_entities = ['belgium', 'amsterdam', 'airport', 'schiphol', 'netherlands']
        for entity in known_entities:
            if entity in question:
                entities.append(entity)
        
        return list(set(entities))
    
    def _is_relation_relevant(self, question: str, relation: str) -> bool:
        """åˆ¤æ–­å…³ç³»æ˜¯å¦ä¸é—®é¢˜ç›¸å…³"""
        relation_keywords = {
            'leader': ['leader', 'president', 'king', 'queen', 'head', 'who leads'],
            'location': ['location', 'located', 'where', 'place'],
            'capital': ['capital'],
            'type': ['type', 'kind', 'what type'],
            'runway': ['runway', 'strip'],
        }
        
        for rel_type, keywords in relation_keywords.items():
            if rel_type in relation:
                return any(keyword in question for keyword in keywords)
        
        return False
    
    def calculate_precision_at_k(self, relevance_scores: List[int], k: int) -> float:
        """è®¡ç®—Precision@K"""
        if k > len(relevance_scores):
            k = len(relevance_scores)
        
        if k == 0:
            return 0.0
        
        relevant_count = sum(1 for score in relevance_scores[:k] if score > 0)
        return relevant_count / k
    
    def calculate_recall_at_k(self, relevance_scores: List[int], k: int) -> float:
        """è®¡ç®—Recall@K"""
        if k > len(relevance_scores):
            k = len(relevance_scores)
        
        total_relevant = sum(1 for score in relevance_scores if score > 0)
        if total_relevant == 0:
            return 0.0
        
        relevant_at_k = sum(1 for score in relevance_scores[:k] if score > 0)
        return relevant_at_k / total_relevant
    
    def calculate_ndcg_at_k(self, relevance_scores: List[int], k: int) -> float:
        """è®¡ç®—nDCG@K (Normalized Discounted Cumulative Gain)"""
        if k > len(relevance_scores):
            k = len(relevance_scores)
        
        if k == 0:
            return 0.0
        
        # è®¡ç®—DCG@K
        dcg = 0.0
        for i in range(k):
            if i < len(relevance_scores):
                dcg += relevance_scores[i] / math.log2(i + 2)
        
        # è®¡ç®—IDCG@K (ç†æƒ³æƒ…å†µä¸‹çš„DCG)
        ideal_scores = sorted(relevance_scores, reverse=True)
        idcg = 0.0
        for i in range(k):
            if i < len(ideal_scores):
                idcg += ideal_scores[i] / math.log2(i + 2)
        
        if idcg == 0:
            return 0.0
        
        return dcg / idcg
    
    def evaluate_single_question(self, question_data: Dict, k_values: List[int] = [1, 3, 5, 10]) -> Dict:
        """è¯„ä¼°å•ä¸ªé—®é¢˜çš„æ£€ç´¢æ•ˆæœ"""
        question = question_data['question']
        
        # è·å–ä¸¤ä¸ªç³»ç»Ÿçš„æ£€ç´¢ç»“æœ
        original_result = self.original_engine.retrieve_and_rewrite(question, n_results=max(k_values))
        enhanced_result = self.enhanced_engine.retrieve_and_rewrite(question, n_results=max(k_values), use_reranking=True)
        
        # åˆ›å»ºç›¸å…³æ€§æ ‡æ³¨
        original_relevance = self.create_ground_truth_relevance(question_data, original_result['retrieved_items'])
        enhanced_relevance = self.create_ground_truth_relevance(question_data, enhanced_result['retrieved_items'])
        
        # è®¡ç®—å„ç§æŒ‡æ ‡
        metrics = {
            'question': question,
            'expected_answer': question_data['expected_answer'],
            'question_type': question_data['question_type'],
            'original_system': {
                'final_answer': original_result['final_answer'],
                'rewritten_query': original_result.get('rewritten_query', question),
                'relevance_scores': original_relevance,
                'metrics': {}
            },
            'enhanced_system': {
                'final_answer': enhanced_result['final_answer'],
                'rewritten_query': enhanced_result.get('rewritten_query', question),
                'relevance_scores': enhanced_relevance,
                'metrics': {}
            }
        }
        
        # ä¸ºæ¯ä¸ªKå€¼è®¡ç®—æŒ‡æ ‡
        for k in k_values:
            # åŸå§‹ç³»ç»Ÿ
            metrics['original_system']['metrics'][f'precision@{k}'] = self.calculate_precision_at_k(original_relevance, k)
            metrics['original_system']['metrics'][f'recall@{k}'] = self.calculate_recall_at_k(original_relevance, k)
            metrics['original_system']['metrics'][f'ndcg@{k}'] = self.calculate_ndcg_at_k(original_relevance, k)
            
            # å¢å¼ºç³»ç»Ÿ
            metrics['enhanced_system']['metrics'][f'precision@{k}'] = self.calculate_precision_at_k(enhanced_relevance, k)
            metrics['enhanced_system']['metrics'][f'recall@{k}'] = self.calculate_recall_at_k(enhanced_relevance, k)
            metrics['enhanced_system']['metrics'][f'ndcg@{k}'] = self.calculate_ndcg_at_k(enhanced_relevance, k)
        
        return metrics
    
    def evaluate_dataset(self, questions: List[Dict], k_values: List[int] = [1, 3, 5, 10]) -> Dict:
        """è¯„ä¼°æ•´ä¸ªæ•°æ®é›†"""
        print(f"ğŸ”„ å¼€å§‹è¯„ä¼° {len(questions)} ä¸ªé—®é¢˜...")
        
        all_results = []
        
        for question_data in tqdm(questions, desc="è¯„ä¼°è¿›åº¦"):
            try:
                result = self.evaluate_single_question(question_data, k_values)
                all_results.append(result)
            except Exception as e:
                print(f"âš  è¯„ä¼°é—®é¢˜æ—¶å‡ºé”™: {question_data['question'][:50]}... - {e}")
                continue
        
        # è®¡ç®—å¹³å‡æŒ‡æ ‡
        summary = self._calculate_summary_metrics(all_results, k_values)
        
        return {
            'summary': summary,
            'detailed_results': all_results,
            'total_questions': len(all_results)
        }
    
    def _calculate_summary_metrics(self, results: List[Dict], k_values: List[int]) -> Dict:
        """è®¡ç®—æ±‡æ€»æŒ‡æ ‡"""
        summary = {
            'original_system': defaultdict(list),
            'enhanced_system': defaultdict(list)
        }
        
        # æ”¶é›†æ‰€æœ‰æŒ‡æ ‡
        for result in results:
            for system in ['original_system', 'enhanced_system']:
                for metric_name, metric_value in result[system]['metrics'].items():
                    summary[system][metric_name].append(metric_value)
        
        # è®¡ç®—å¹³å‡å€¼å’Œæ ‡å‡†å·®
        final_summary = {}
        for system in ['original_system', 'enhanced_system']:
            final_summary[system] = {}
            for metric_name, values in summary[system].items():
                if values:
                    final_summary[system][metric_name] = {
                        'mean': np.mean(values),
                        'std': np.std(values),
                        'count': len(values)
                    }
        
        return final_summary
    
    def save_evaluation_results(self, results: Dict, output_dir: str = None):
        """
        ä¿å­˜è¯„ä¼°ç»“æœåˆ°æœ¬åœ°ç£ç›˜
        
        Args:
            results: è¯„ä¼°ç»“æœå­—å…¸
            output_dir: è¾“å‡ºç›®å½•ï¼Œé»˜è®¤ä½¿ç”¨configä¸­çš„é…ç½®
        """
        import config
        from datetime import datetime
        
        # ç¡®å®šè¾“å‡ºç›®å½•
        if output_dir is None:
            output_dir = config.EVALUATION_OUTPUT_DIR
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True)
        
        # ç”Ÿæˆæ—¶é—´æˆ³
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # 1. ä¿å­˜å®Œæ•´çš„è¯„ä¼°ç»“æœ
        full_results_file = output_path / f"full_evaluation_results_{timestamp}.json"
        with open(full_results_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2, default=str)
        
        # 2. ä¿å­˜æ±‡æ€»æŒ‡æ ‡
        summary_file = output_path / f"evaluation_summary_{timestamp}.json"
        summary_data = {
            'timestamp': timestamp,
            'total_questions': results['total_questions'],
            'summary_metrics': results['summary'],
            'evaluation_config': {
                'sample_size': config.EVALUATION_SAMPLE_SIZE,
                'k_values': config.EVALUATION_K_VALUES,
                'embedding_model': config.EMBEDDING_MODEL
            }
        }
        
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary_data, f, ensure_ascii=False, indent=2, default=str)
        
        # 3. ä¿å­˜é—®é¢˜å’Œç­”æ¡ˆå¯¹æ¯”
        self._save_qa_comparison(results, output_path, timestamp)
        
        # 4. ä¿å­˜CSVæ ¼å¼çš„æŒ‡æ ‡å¯¹æ¯”
        self._save_metrics_csv(results['summary'], output_path, timestamp)
        
        # 5. ä¿å­˜è¯¦ç»†ç»“æœï¼ˆå¦‚æœé…ç½®å…è®¸ï¼‰
        if config.SAVE_DETAILED_RESULTS:
            detailed_file = output_path / f"detailed_results_{timestamp}.json"
            detailed_data = {
                'timestamp': timestamp,
                'detailed_results': results['detailed_results']
            }
            with open(detailed_file, 'w', encoding='utf-8') as f:
                json.dump(detailed_data, f, ensure_ascii=False, indent=2, default=str)
        
        # 6. ç”ŸæˆMarkdownæŠ¥å‘Š
        self._generate_markdown_report(results, output_path, timestamp)
        
        # 7. ä¿å­˜å¯è§†åŒ–å›¾è¡¨ï¼ˆå¦‚æœé…ç½®å…è®¸ï¼‰
        if config.SAVE_SUMMARY_CHARTS:
            self._save_visualization_charts(results['summary'], output_path, timestamp)
        
        print(f"ğŸ’¾ è¯„ä¼°ç»“æœå·²ä¿å­˜åˆ°ç›®å½•: {output_path}")
        print(f"ğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:")
        print(f"   - å®Œæ•´ç»“æœ: {full_results_file.name}")
        print(f"   - æ±‡æ€»æŒ‡æ ‡: {summary_file.name}")
        print(f"   - é—®ç­”å¯¹æ¯”: qa_comparison_{timestamp}.json")
        print(f"   - é—®ç­”å¯¹æ¯”(æ˜“è¯»): qa_comparison_{timestamp}.txt")
        print(f"   - CSVå¯¹æ¯”: metrics_comparison_{timestamp}.csv")
        print(f"   - MarkdownæŠ¥å‘Š: evaluation_report_{timestamp}.md")
        if config.SAVE_DETAILED_RESULTS:
            print(f"   - è¯¦ç»†ç»“æœ: detailed_results_{timestamp}.json")
        if config.SAVE_SUMMARY_CHARTS:
            print(f"   - å¯è§†åŒ–å›¾è¡¨: charts_{timestamp}/")
    
    def _save_qa_comparison(self, results: Dict, output_path: Path, timestamp: str):
        """
        ä¿å­˜é—®é¢˜å’Œä¸¤ä¸ªç³»ç»Ÿç­”æ¡ˆçš„å¯¹æ¯”
        
        Args:
            results: å®Œæ•´çš„è¯„ä¼°ç»“æœ
            output_path: è¾“å‡ºè·¯å¾„
            timestamp: æ—¶é—´æˆ³
        """
        # 1. ä¿å­˜JSONæ ¼å¼ï¼ˆä¾¿äºç¨‹åºå¤„ç†ï¼‰
        qa_comparison_file = output_path / f"qa_comparison_{timestamp}.json"
        
        qa_comparisons = []
        
        for result in results.get('detailed_results', []):
            qa_comparison = {
                'question': result['question'],
                'expected_answer': result['expected_answer'],
                'question_type': result.get('question_type', 'unknown'),
                'original_system': {
                    'final_answer': result['original_system']['final_answer'],
                    'rewritten_query': result['original_system'].get('rewritten_query', ''),
                    'best_metrics': self._get_best_metrics(result['original_system']['metrics'])
                },
                'enhanced_system': {
                    'final_answer': result['enhanced_system']['final_answer'],
                    'rewritten_query': result['enhanced_system'].get('rewritten_query', ''),
                    'best_metrics': self._get_best_metrics(result['enhanced_system']['metrics'])
                },
                'improvement': self._calculate_improvement(
                    result['original_system']['metrics'],
                    result['enhanced_system']['metrics']
                )
            }
            qa_comparisons.append(qa_comparison)
        
        # ä¿å­˜JSONæ ¼å¼
        with open(qa_comparison_file, 'w', encoding='utf-8') as f:
            json.dump(qa_comparisons, f, ensure_ascii=False, indent=2)
        
        # 2. ä¿å­˜æ˜“è¯»çš„æ–‡æœ¬æ ¼å¼
        txt_comparison_file = output_path / f"qa_comparison_{timestamp}.txt"
        with open(txt_comparison_file, 'w', encoding='utf-8') as f:
            f.write("é—®é¢˜å’Œç­”æ¡ˆå¯¹æ¯”æŠ¥å‘Š\n")
            f.write("=" * 80 + "\n\n")
            f.write(f"ç”Ÿæˆæ—¶é—´: {results.get('timestamp', timestamp)}\n")
            f.write(f"æ€»é—®é¢˜æ•°: {results.get('total_questions', len(qa_comparisons))}\n\n")
            
            for i, qa in enumerate(qa_comparisons, 1):
                f.write(f"é—®é¢˜ {i}:\n")
                f.write("-" * 40 + "\n")
                f.write(f"é—®é¢˜: {qa['question']}\n")
                f.write(f"æœŸæœ›ç­”æ¡ˆ: {qa['expected_answer']}\n")
                f.write(f"é—®é¢˜ç±»å‹: {qa['question_type']}\n\n")
                
                f.write("åŸå§‹ç³»ç»Ÿ:\n")
                f.write(f"  é‡å†™æŸ¥è¯¢: {qa['original_system']['rewritten_query']}\n")
                f.write(f"  æœ€ç»ˆç­”æ¡ˆ: {qa['original_system']['final_answer']}\n")
                f.write(f"  æœ€ä½³æŒ‡æ ‡: {qa['original_system']['best_metrics']}\n\n")
                
                f.write("å¢å¼ºç³»ç»Ÿ:\n")
                f.write(f"  é‡å†™æŸ¥è¯¢: {qa['enhanced_system']['rewritten_query']}\n")
                f.write(f"  æœ€ç»ˆç­”æ¡ˆ: {qa['enhanced_system']['final_answer']}\n")
                f.write(f"  æœ€ä½³æŒ‡æ ‡: {qa['enhanced_system']['best_metrics']}\n\n")
                
                f.write("æ”¹è¿›æƒ…å†µ:\n")
                for metric, improvement in qa['improvement'].items():
                    f.write(f"  {metric}: {improvement:+.1f}%\n")
                
                f.write("\n" + "=" * 80 + "\n\n")
        
        # 3. ä¿å­˜ç®€åŒ–çš„CSVæ ¼å¼ï¼ˆä¾¿äºExcelæŸ¥çœ‹ï¼‰
        csv_comparison_file = output_path / f"qa_comparison_{timestamp}.csv"
        import csv
        
        with open(csv_comparison_file, 'w', newline='', encoding='utf-8-sig') as f:
            writer = csv.writer(f)
            
            # å†™å…¥æ ‡é¢˜è¡Œ
            writer.writerow([
                'é—®é¢˜', 'æœŸæœ›ç­”æ¡ˆ', 'é—®é¢˜ç±»å‹',
                'åŸå§‹ç³»ç»Ÿç­”æ¡ˆ', 'å¢å¼ºç³»ç»Ÿç­”æ¡ˆ',
                'åŸå§‹ç³»ç»Ÿé‡å†™æŸ¥è¯¢', 'å¢å¼ºç³»ç»Ÿé‡å†™æŸ¥è¯¢',
                'Precisionæ”¹è¿›%', 'Recallæ”¹è¿›%', 'nDCGæ”¹è¿›%'
            ])
            
            # å†™å…¥æ•°æ®è¡Œ
            for qa in qa_comparisons:
                writer.writerow([
                    qa['question'],
                    qa['expected_answer'],
                    qa['question_type'],
                    qa['original_system']['final_answer'],
                    qa['enhanced_system']['final_answer'],
                    qa['original_system']['rewritten_query'],
                    qa['enhanced_system']['rewritten_query'],
                    f"{qa['improvement'].get('precision', 0):.1f}",
                    f"{qa['improvement'].get('recall', 0):.1f}",
                    f"{qa['improvement'].get('ndcg', 0):.1f}"
                ])
    
    def _get_best_metrics(self, metrics: Dict) -> Dict:
        """è·å–æœ€ä½³Kå€¼çš„æŒ‡æ ‡"""
        best_metrics = {}
        for k_key, k_metrics in metrics.items():
            if isinstance(k_metrics, dict):
                for metric_name, value in k_metrics.items():
                    if metric_name not in best_metrics or value > best_metrics[metric_name]:
                        best_metrics[metric_name] = value
        return best_metrics
    
    def _calculate_improvement(self, original_metrics: Dict, enhanced_metrics: Dict) -> Dict:
        """è®¡ç®—æ”¹è¿›å¹…åº¦"""
        original_best = self._get_best_metrics(original_metrics)
        enhanced_best = self._get_best_metrics(enhanced_metrics)
        
        improvement = {}
        for metric in ['precision', 'recall', 'ndcg']:
            original_val = original_best.get(metric, 0)
            enhanced_val = enhanced_best.get(metric, 0)
            
            if original_val > 0:
                improvement[metric] = ((enhanced_val - original_val) / original_val) * 100
            else:
                improvement[metric] = 0 if enhanced_val == 0 else 100
        
        return improvement

    def _save_metrics_csv(self, summary: Dict, output_path: Path, timestamp: str):
        """ä¿å­˜æŒ‡æ ‡å¯¹æ¯”çš„CSVæ–‡ä»¶"""
        import csv
        
        csv_file = output_path / f"metrics_comparison_{timestamp}.csv"
        
        with open(csv_file, 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            
            # å†™å…¥æ ‡é¢˜è¡Œ
            writer.writerow(['System', 'Metric', 'K', 'Mean', 'Std', 'Count'])
            
            # å†™å…¥æ•°æ®
            for system_name in ['original_system', 'enhanced_system']:
                if system_name in summary:
                    system_data = summary[system_name]
                    system_display_name = 'åŸå§‹ç³»ç»Ÿ' if system_name == 'original_system' else 'å¢å¼ºç³»ç»Ÿ'
                    
                    for metric_name, metric_data in system_data.items():
                        # è§£ææŒ‡æ ‡åç§° (å¦‚ "precision@1")
                        if '@' in metric_name:
                            metric_type, k_value = metric_name.split('@')
                            writer.writerow([
                                system_display_name,
                                metric_type.upper(),
                                k_value,
                                f"{metric_data['mean']:.4f}",
                                f"{metric_data['std']:.4f}",
                                metric_data['count']
                            ])
    
    def _generate_markdown_report(self, results: Dict, output_path: Path, timestamp: str):
        """ç”ŸæˆMarkdownæ ¼å¼çš„è¯„ä¼°æŠ¥å‘Š"""
        report_file = output_path / f"evaluation_report_{timestamp}.md"
        
        summary = results['summary']
        total_questions = results['total_questions']
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(f"# æ£€ç´¢ç³»ç»Ÿè¯„ä¼°æŠ¥å‘Š\n\n")
            f.write(f"**ç”Ÿæˆæ—¶é—´**: {timestamp}\n")
            f.write(f"**è¯„ä¼°é—®é¢˜æ•°**: {total_questions}\n")
            f.write(f"**åµŒå…¥æ¨¡å‹**: {config.EMBEDDING_MODEL}\n\n")
            
            # ç³»ç»Ÿå¯¹æ¯”è¡¨æ ¼
            f.write("## ğŸ“Š ç³»ç»Ÿæ€§èƒ½å¯¹æ¯”\n\n")
            
            # ä¸ºæ¯ä¸ªKå€¼åˆ›å»ºå¯¹æ¯”è¡¨æ ¼
            k_values = [1, 3, 5, 10]
            for k in k_values:
                f.write(f"### K={k} æŒ‡æ ‡å¯¹æ¯”\n\n")
                f.write("| æŒ‡æ ‡ | åŸå§‹ç³»ç»Ÿ | å¢å¼ºç³»ç»Ÿ | æ”¹è¿›å¹…åº¦ |\n")
                f.write("|------|----------|----------|----------|\n")
                
                for metric_type in ['precision', 'recall', 'ndcg']:
                    metric_key = f'{metric_type}@{k}'
                    
                    if ('original_system' in summary and metric_key in summary['original_system'] and
                        'enhanced_system' in summary and metric_key in summary['enhanced_system']):
                        
                        orig_val = summary['original_system'][metric_key]['mean']
                        enh_val = summary['enhanced_system'][metric_key]['mean']
                        
                        if orig_val > 0:
                            improvement = ((enh_val - orig_val) / orig_val) * 100
                            improvement_str = f"{improvement:+.2f}%"
                        else:
                            improvement_str = "N/A"
                        
                        f.write(f"| {metric_type.upper()}@{k} | {orig_val:.4f} | {enh_val:.4f} | {improvement_str} |\n")
                
                f.write("\n")
            
            # è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯
            f.write("## ğŸ“ˆ è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯\n\n")
            
            for system_name, system_display in [('original_system', 'åŸå§‹ç³»ç»Ÿ'), ('enhanced_system', 'å¢å¼ºç³»ç»Ÿ')]:
                if system_name in summary:
                    f.write(f"### {system_display}\n\n")
                    f.write("| æŒ‡æ ‡ | å‡å€¼ | æ ‡å‡†å·® | æ ·æœ¬æ•° |\n")
                    f.write("|------|------|--------|--------|\n")
                    
                    system_data = summary[system_name]
                    for metric_name, metric_data in sorted(system_data.items()):
                        f.write(f"| {metric_name} | {metric_data['mean']:.4f} | {metric_data['std']:.4f} | {metric_data['count']} |\n")
                    
                    f.write("\n")
            
            # ç»“è®ºå’Œå»ºè®®
            f.write("## ğŸ’¡ ç»“è®ºå’Œå»ºè®®\n\n")
            
            # è®¡ç®—æ€»ä½“æ”¹è¿›æƒ…å†µ
            improvements = []
            if 'original_system' in summary and 'enhanced_system' in summary:
                for k in k_values:
                    for metric_type in ['precision', 'recall', 'ndcg']:
                        metric_key = f'{metric_type}@{k}'
                        if (metric_key in summary['original_system'] and 
                            metric_key in summary['enhanced_system']):
                            orig_val = summary['original_system'][metric_key]['mean']
                            enh_val = summary['enhanced_system'][metric_key]['mean']
                            if orig_val > 0:
                                improvement = ((enh_val - orig_val) / orig_val) * 100
                                improvements.append(improvement)
            
            if improvements:
                avg_improvement = sum(improvements) / len(improvements)
                if avg_improvement > 5:
                    f.write("âœ… **å¢å¼ºç³»ç»Ÿè¡¨ç°æ˜¾è‘—ä¼˜äºåŸå§‹ç³»ç»Ÿ**\n\n")
                elif avg_improvement > 0:
                    f.write("âœ… **å¢å¼ºç³»ç»Ÿè¡¨ç°ç•¥ä¼˜äºåŸå§‹ç³»ç»Ÿ**\n\n")
                else:
                    f.write("âš ï¸ **å¢å¼ºç³»ç»Ÿè¡¨ç°ä¸åŸå§‹ç³»ç»Ÿç›¸å½“æˆ–ç•¥å·®**\n\n")
                
                f.write(f"- å¹³å‡æ”¹è¿›å¹…åº¦: {avg_improvement:.2f}%\n")
                f.write(f"- æœ€å¤§æ”¹è¿›å¹…åº¦: {max(improvements):.2f}%\n")
                f.write(f"- æœ€å°æ”¹è¿›å¹…åº¦: {min(improvements):.2f}%\n\n")
            
            f.write("### æ”¹è¿›å»ºè®®\n\n")
            f.write("1. **ç»§ç»­ä¼˜åŒ–é‡æ’ç®—æ³•**: è°ƒæ•´å¤šä¿¡å·èåˆçš„æƒé‡\n")
            f.write("2. **æ‰©å±•è®­ç»ƒæ•°æ®**: å¢åŠ æ›´å¤šæ ·åŒ–çš„é—®é¢˜ç±»å‹\n")
            f.write("3. **ä¼˜åŒ–åµŒå…¥æ¨¡æ¿**: è¿›ä¸€æ­¥æ”¹è¿›è‡ªç„¶è¯­è¨€æ¨¡æ¿\n")
            f.write("4. **å¼•å…¥ç”¨æˆ·åé¦ˆ**: åŸºäºå®é™…ä½¿ç”¨åé¦ˆæŒç»­ä¼˜åŒ–\n")
    
    def _save_visualization_charts(self, summary: Dict, output_path: Path, timestamp: str):
        """ä¿å­˜å¯è§†åŒ–å›¾è¡¨"""
        try:
            import matplotlib.pyplot as plt
            import numpy as np
            
            # åˆ›å»ºå›¾è¡¨ç›®å½•
            charts_dir = output_path / f"charts_{timestamp}"
            charts_dir.mkdir(exist_ok=True)
            
            # è®¾ç½®ä¸­æ–‡å­—ä½“
            plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei']
            plt.rcParams['axes.unicode_minus'] = False
            
            # 1. æŒ‡æ ‡å¯¹æ¯”æŸ±çŠ¶å›¾
            self._create_metrics_bar_chart(summary, charts_dir)
            
            # 2. Kå€¼è¶‹åŠ¿å›¾
            self._create_k_trend_chart(summary, charts_dir)
            
            # 3. æ”¹è¿›å¹…åº¦å›¾
            self._create_improvement_chart(summary, charts_dir)
            
            print(f"ğŸ“Š å¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜åˆ°: {charts_dir}")
            
        except ImportError:
            print("âš ï¸ matplotlibæœªå®‰è£…ï¼Œè·³è¿‡å›¾è¡¨ç”Ÿæˆ")
        except Exception as e:
            print(f"âš ï¸ å›¾è¡¨ç”Ÿæˆå¤±è´¥: {e}")
    
    def _create_metrics_bar_chart(self, summary: Dict, charts_dir: Path):
        """åˆ›å»ºæŒ‡æ ‡å¯¹æ¯”æŸ±çŠ¶å›¾"""
        import matplotlib.pyplot as plt
        import numpy as np
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        fig.suptitle('æ£€ç´¢ç³»ç»Ÿæ€§èƒ½å¯¹æ¯”', fontsize=16, fontweight='bold')
        
        k_values = [1, 3, 5, 10]
        metrics = ['precision', 'recall', 'ndcg']
        
        for i, k in enumerate(k_values):
            ax = axes[i//2, i%2]
            
            x = np.arange(len(metrics))
            width = 0.35
            
            orig_values = []
            enh_values = []
            
            for metric in metrics:
                metric_key = f'{metric}@{k}'
                orig_val = summary.get('original_system', {}).get(metric_key, {}).get('mean', 0)
                enh_val = summary.get('enhanced_system', {}).get(metric_key, {}).get('mean', 0)
                orig_values.append(orig_val)
                enh_values.append(enh_val)
            
            ax.bar(x - width/2, orig_values, width, label='åŸå§‹ç³»ç»Ÿ', alpha=0.8)
            ax.bar(x + width/2, enh_values, width, label='å¢å¼ºç³»ç»Ÿ', alpha=0.8)
            
            ax.set_xlabel('æŒ‡æ ‡ç±»å‹')
            ax.set_ylabel('åˆ†æ•°')
            ax.set_title(f'K={k} æŒ‡æ ‡å¯¹æ¯”')
            ax.set_xticks(x)
            ax.set_xticklabels([m.upper() for m in metrics])
            ax.legend()
            ax.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(charts_dir / 'metrics_comparison.png', dpi=300, bbox_inches='tight')
        plt.close()
    
    def _create_k_trend_chart(self, summary: Dict, charts_dir: Path):
        """åˆ›å»ºKå€¼è¶‹åŠ¿å›¾"""
        import matplotlib.pyplot as plt
        
        fig, axes = plt.subplots(1, 3, figsize=(18, 6))
        fig.suptitle('ä¸åŒKå€¼ä¸‹çš„æ€§èƒ½è¶‹åŠ¿', fontsize=16, fontweight='bold')
        
        k_values = [1, 3, 5, 10]
        metrics = ['precision', 'recall', 'ndcg']
        
        for i, metric in enumerate(metrics):
            ax = axes[i]
            
            orig_values = []
            enh_values = []
            
            for k in k_values:
                metric_key = f'{metric}@{k}'
                orig_val = summary.get('original_system', {}).get(metric_key, {}).get('mean', 0)
                enh_val = summary.get('enhanced_system', {}).get(metric_key, {}).get('mean', 0)
                orig_values.append(orig_val)
                enh_values.append(enh_val)
            
            ax.plot(k_values, orig_values, 'o-', label='åŸå§‹ç³»ç»Ÿ', linewidth=2, markersize=8)
            ax.plot(k_values, enh_values, 's-', label='å¢å¼ºç³»ç»Ÿ', linewidth=2, markersize=8)
            
            ax.set_xlabel('Kå€¼')
            ax.set_ylabel(f'{metric.upper()}åˆ†æ•°')
            ax.set_title(f'{metric.upper()}@K è¶‹åŠ¿')
            ax.legend()
            ax.grid(True, alpha=0.3)
            ax.set_xticks(k_values)
        
        plt.tight_layout()
        plt.savefig(charts_dir / 'k_trend_analysis.png', dpi=300, bbox_inches='tight')
        plt.close()
    
    def _create_improvement_chart(self, summary: Dict, charts_dir: Path):
        """åˆ›å»ºæ”¹è¿›å¹…åº¦å›¾"""
        import matplotlib.pyplot as plt
        import numpy as np
        
        fig, ax = plt.subplots(figsize=(12, 8))
        
        k_values = [1, 3, 5, 10]
        metrics = ['precision', 'recall', 'ndcg']
        
        improvements = []
        labels = []
        
        for k in k_values:
            for metric in metrics:
                metric_key = f'{metric}@{k}'
                if (summary.get('original_system', {}).get(metric_key) and 
                    summary.get('enhanced_system', {}).get(metric_key)):
                    
                    orig_val = summary['original_system'][metric_key]['mean']
                    enh_val = summary['enhanced_system'][metric_key]['mean']
                    
                    if orig_val > 0:
                        improvement = ((enh_val - orig_val) / orig_val) * 100
                        improvements.append(improvement)
                        labels.append(f'{metric.upper()}@{k}')
        
        if improvements:
            colors = ['green' if imp > 0 else 'red' for imp in improvements]
            
            bars = ax.barh(labels, improvements, color=colors, alpha=0.7)
            
            # æ·»åŠ æ•°å€¼æ ‡ç­¾
            for bar, imp in zip(bars, improvements):
                width = bar.get_width()
                ax.text(width + (0.5 if width > 0 else -0.5), bar.get_y() + bar.get_height()/2, 
                       f'{imp:+.1f}%', ha='left' if width > 0 else 'right', va='center')
            
            ax.set_xlabel('æ”¹è¿›å¹…åº¦ (%)')
            ax.set_title('å¢å¼ºç³»ç»Ÿç›¸å¯¹äºåŸå§‹ç³»ç»Ÿçš„æ”¹è¿›å¹…åº¦', fontsize=14, fontweight='bold')
            ax.axvline(x=0, color='black', linestyle='-', alpha=0.3)
            ax.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(charts_dir / 'improvement_analysis.png', dpi=300, bbox_inches='tight')
        plt.close()
    
    def print_summary_report(self, results: Dict):
        """æ‰“å°æ±‡æ€»æŠ¥å‘Š"""
        summary = results['summary']
        total_questions = results['total_questions']
        
        print(f"\nğŸ“Š è¯„ä¼°æŠ¥å‘Š (å…± {total_questions} ä¸ªé—®é¢˜)")
        print("=" * 80)
        
        # å¯¹æ¯”ä¸¤ä¸ªç³»ç»Ÿ
        systems = [
            ('åŸå§‹ç³»ç»Ÿ', 'original_system'),
            ('å¢å¼ºç³»ç»Ÿ', 'enhanced_system')
        ]
        
        for system_name, system_key in systems:
            print(f"\nğŸ” {system_name}:")
            print("-" * 40)
            
            if system_key in summary:
                metrics = summary[system_key]
                
                # æŒ‰Kå€¼åˆ†ç»„æ˜¾ç¤º
                k_values = [1, 3, 5, 10]
                for k in k_values:
                    print(f"\n  K={k}:")
                    
                    for metric_type in ['precision', 'recall', 'ndcg']:
                        metric_key = f'{metric_type}@{k}'
                        if metric_key in metrics:
                            mean_val = metrics[metric_key]['mean']
                            std_val = metrics[metric_key]['std']
                            print(f"    {metric_type.upper()}@{k}: {mean_val:.4f} (Â±{std_val:.4f})")
        
        # è®¡ç®—æ”¹è¿›å¹…åº¦
        print(f"\nğŸ“ˆ æ”¹è¿›å¹…åº¦:")
        print("-" * 40)
        
        if 'original_system' in summary and 'enhanced_system' in summary:
            orig_metrics = summary['original_system']
            enh_metrics = summary['enhanced_system']
            
            for k in [1, 3, 5, 10]:
                print(f"\n  K={k}:")
                for metric_type in ['precision', 'recall', 'ndcg']:
                    metric_key = f'{metric_type}@{k}'
                    if metric_key in orig_metrics and metric_key in enh_metrics:
                        orig_val = orig_metrics[metric_key]['mean']
                        enh_val = enh_metrics[metric_key]['mean']
                        
                        if orig_val > 0:
                            improvement = ((enh_val - orig_val) / orig_val) * 100
                            print(f"    {metric_type.upper()}@{k}: {improvement:+.2f}%")

# ä¸»è¯„ä¼°å‡½æ•°
def run_comprehensive_evaluation():
    """è¿è¡Œå…¨é¢çš„è¯„ä¼°"""
    print("ğŸš€ å¯åŠ¨æ£€ç´¢ç³»ç»Ÿå…¨é¢è¯„ä¼°")
    print("ğŸ¯ è¯„ä¼°å†…å®¹: Precision@K, Recall@K, nDCG@K")
    print("ğŸ“Š å¯¹æ¯”ç³»ç»Ÿ: åŸå§‹ç³»ç»Ÿ vs å¢å¼ºç³»ç»Ÿ")
    print("=" * 60)
    
    # åˆå§‹åŒ–è¯„ä¼°å™¨
    evaluator = RetrievalEvaluator()
    
    # åŠ è½½è¯„ä¼°æ•°æ®
    import config
    questions = evaluator.load_qa_dataset(limit=config.EVALUATION_SAMPLE_SIZE)
    
    if not questions:
        print("âŒ æ— æ³•åŠ è½½è¯„ä¼°æ•°æ®")
        print("ğŸ’¡ è¯·ç¡®ä¿qa_datasetsç›®å½•ä¸­æœ‰QAæ•°æ®æ–‡ä»¶")
        return
    
    # è¿è¡Œè¯„ä¼°
    print(f"ğŸ”„ å¼€å§‹è¯„ä¼° {len(questions)} ä¸ªé—®é¢˜...")
    results = evaluator.evaluate_dataset(questions, k_values=config.EVALUATION_K_VALUES)
    
    # ä¿å­˜ç»“æœåˆ°æœ¬åœ°ç£ç›˜
    print(f"\nğŸ’¾ ä¿å­˜è¯„ä¼°ç»“æœ...")
    evaluator.save_evaluation_results(results)
    
    # æ‰“å°æ§åˆ¶å°æŠ¥å‘Š
    evaluator.print_summary_report(results)
    
    print(f"\nâœ… è¯„ä¼°å®Œæˆï¼")
    print(f"ğŸ“ æ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ° {config.EVALUATION_OUTPUT_DIR}/ ç›®å½•")
    print(f"ğŸ“Š åŒ…å«JSONã€CSVã€MarkdownæŠ¥å‘Šå’Œå¯è§†åŒ–å›¾è¡¨")

def run_quick_evaluation(sample_size: int = 20):
    """è¿è¡Œå¿«é€Ÿè¯„ä¼°ï¼ˆå°‘é‡æ ·æœ¬ï¼‰"""
    print("âš¡ å¯åŠ¨å¿«é€Ÿè¯„ä¼°")
    print(f"ğŸ“Š æ ·æœ¬æ•°é‡: {sample_size}")
    print("=" * 40)
    
    evaluator = RetrievalEvaluator()
    
    # åŠ è½½å°‘é‡æ•°æ®è¿›è¡Œå¿«é€Ÿæµ‹è¯•
    questions = evaluator.load_qa_dataset(limit=sample_size)
    
    if not questions:
        print("âŒ æ— æ³•åŠ è½½è¯„ä¼°æ•°æ®")
        return
    
    # è¿è¡Œè¯„ä¼°
    results = evaluator.evaluate_dataset(questions, k_values=[1, 3, 5])
    
    # ä¿å­˜ç»“æœ
    evaluator.save_evaluation_results(results)
    
    # æ‰“å°ç®€åŒ–æŠ¥å‘Š
    evaluator.print_summary_report(results)
    
    print(f"\nâœ… å¿«é€Ÿè¯„ä¼°å®Œæˆï¼")

def run_scan_all_evaluation(dataset_path: str = "qa_datasets", k_values: list = None, output_dir: str = None):
    """è¿è¡Œå…¨é‡æ‰«æè¯„ä¼° - æ‰«æQAæ•°æ®é›†çš„å…¨éƒ¨æ•°æ®"""
    import config
    
    k_values = k_values or config.EVALUATION_K_VALUES
    
    print("ğŸ” å¯åŠ¨å…¨é‡æ‰«æè¯„ä¼°")
    print(f"ï¿½ æ•°è‡ªæ®é›†è·¯å¾„: {dataset_path}")
    print(f"ï¿½ Kæœ¬å€¼: {k_values}")
    print("=" * 50)
    
    evaluator = RetrievalEvaluator()
    
    # åŠ è½½å…¨éƒ¨æ•°æ®
    questions = evaluator.load_qa_dataset(dataset_path=dataset_path, scan_all=True)
    
    if not questions:
        print("âŒ æ— æ³•åŠ è½½è¯„ä¼°æ•°æ®")
        return
    
    print(f"\nğŸš€ å¼€å§‹è¯„ä¼° {len(questions)} ä¸ªé—®é¢˜...")
    print("âš ï¸ æ³¨æ„: å…¨é‡è¯„ä¼°å¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´ï¼Œè¯·è€å¿ƒç­‰å¾…")
    
    # è¿è¡Œè¯„ä¼°
    results = evaluator.evaluate_dataset(questions, k_values=k_values)
    
    # ä¿å­˜ç»“æœ
    if output_dir:
        evaluator.save_evaluation_results(results, output_dir)
    else:
        evaluator.save_evaluation_results(results)
    
    # æ‰“å°è¯¦ç»†æŠ¥å‘Š
    evaluator.print_summary_report(results)
    
    # æ‰“å°æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯
    print(f"\nğŸ“Š æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯:")
    question_types = {}
    source_files = {}
    
    for q in questions:
        q_type = q.get('question_type', 'unknown')
        source_file = q.get('source_file', 'unknown')
        
        question_types[q_type] = question_types.get(q_type, 0) + 1
        source_files[source_file] = source_files.get(source_file, 0) + 1
    
    print(f"   é—®é¢˜ç±»å‹åˆ†å¸ƒ:")
    for q_type, count in sorted(question_types.items()):
        percentage = (count / len(questions)) * 100
        print(f"     - {q_type}: {count} ({percentage:.1f}%)")
    
    print(f"   æ•°æ®æ–‡ä»¶åˆ†å¸ƒ:")
    for source_file, count in sorted(source_files.items()):
        percentage = (count / len(questions)) * 100
        print(f"     - {source_file}: {count} ({percentage:.1f}%)")
    
    print(f"\nâœ… å…¨é‡æ‰«æè¯„ä¼°å®Œæˆï¼")

def run_custom_evaluation(sample_size: int = None, k_values: list = None, output_dir: str = None):
    """è¿è¡Œè‡ªå®šä¹‰è¯„ä¼°"""
    import config
    
    sample_size = sample_size or config.EVALUATION_SAMPLE_SIZE
    k_values = k_values or config.EVALUATION_K_VALUES
    
    print("ğŸ”§ å¯åŠ¨è‡ªå®šä¹‰è¯„ä¼°")
    print(f"ğŸ“Š æ ·æœ¬æ•°é‡: {sample_size}")
    print(f"ğŸ“ˆ Kå€¼: {k_values}")
    print("=" * 40)
    
    evaluator = RetrievalEvaluator()
    
    # åŠ è½½æ•°æ®
    questions = evaluator.load_qa_dataset(limit=sample_size)
    
    if not questions:
        print("âŒ æ— æ³•åŠ è½½è¯„ä¼°æ•°æ®")
        return
    
    # è¿è¡Œè¯„ä¼°
    results = evaluator.evaluate_dataset(questions, k_values=k_values)
    
    # ä¿å­˜ç»“æœ
    if output_dir:
        evaluator.save_evaluation_results(results, output_dir)
    else:
        evaluator.save_evaluation_results(results)
    
    # æ‰“å°æŠ¥å‘Š
    evaluator.print_summary_report(results)
    
    print(f"\nâœ… è‡ªå®šä¹‰è¯„ä¼°å®Œæˆï¼")

if __name__ == '__main__':
    import argparse
    
    parser = argparse.ArgumentParser(description="æ£€ç´¢ç³»ç»Ÿè¯„ä¼°")
    parser.add_argument('--mode', choices=['full', 'quick', 'custom', 'scan-all'], default='full',
                       help='è¯„ä¼°æ¨¡å¼: full(å®Œæ•´è¯„ä¼°), quick(å¿«é€Ÿè¯„ä¼°), custom(è‡ªå®šä¹‰è¯„ä¼°), scan-all(å…¨é‡æ‰«æ)')
    parser.add_argument('--sample-size', type=int, default=None,
                       help='è¯„ä¼°æ ·æœ¬æ•°é‡ (scan-allæ¨¡å¼ä¸‹å¿½ç•¥)')
    parser.add_argument('--k-values', nargs='+', type=int, default=None,
                       help='Kå€¼åˆ—è¡¨ï¼Œå¦‚: --k-values 1 3 5 10')
    parser.add_argument('--output-dir', type=str, default=None,
                       help='è¾“å‡ºç›®å½•')
    parser.add_argument('--qa-path', type=str, default="qa_datasets",
                       help='QAæ•°æ®é›†è·¯å¾„ (é»˜è®¤: qa_datasets)')
    
    args = parser.parse_args()
    
    if args.mode == 'full':
        run_comprehensive_evaluation()
    elif args.mode == 'quick':
        sample_size = args.sample_size or 20
        run_quick_evaluation(sample_size)
    elif args.mode == 'custom':
        run_custom_evaluation(
            sample_size=args.sample_size,
            k_values=args.k_values,
            output_dir=args.output_dir
        )
    elif args.mode == 'scan-all':
        run_scan_all_evaluation(
            dataset_path=args.qa_path,
            k_values=args.k_values,
            output_dir=args.output_dir
        )