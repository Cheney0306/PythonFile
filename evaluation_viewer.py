# evaluation_viewer.py - è¯„ä¼°ç»“æœæŸ¥çœ‹å™¨

import json
import pandas as pd
from pathlib import Path
from typing import Dict, List
import argparse

def list_evaluation_results(evaluation_dir: str = "evaluation"):
    """åˆ—å‡ºæ‰€æœ‰è¯„ä¼°ç»“æœæ–‡ä»¶"""
    eval_path = Path(evaluation_dir)
    
    if not eval_path.exists():
        print(f"âŒ è¯„ä¼°ç›®å½•ä¸å­˜åœ¨: {evaluation_dir}")
        return []
    
    # æŸ¥æ‰¾æ‰€æœ‰è¯„ä¼°ç»“æœæ–‡ä»¶
    summary_files = list(eval_path.glob("evaluation_summary_*.json"))
    
    if not summary_files:
        print(f"ğŸ“‚ è¯„ä¼°ç›®å½• {evaluation_dir} ä¸­æ²¡æœ‰æ‰¾åˆ°è¯„ä¼°ç»“æœ")
        return []
    
    print(f"ğŸ“Š æ‰¾åˆ° {len(summary_files)} ä¸ªè¯„ä¼°ç»“æœ:")
    print("=" * 60)
    
    results_info = []
    
    for i, file_path in enumerate(sorted(summary_files, reverse=True), 1):
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            timestamp = data.get('timestamp', 'unknown')
            total_questions = data.get('total_questions', 0)
            embedding_model = data.get('evaluation_config', {}).get('embedding_model', 'unknown')
            
            print(f"{i}. æ—¶é—´æˆ³: {timestamp}")
            print(f"   æ–‡ä»¶: {file_path.name}")
            print(f"   é—®é¢˜æ•°: {total_questions}")
            print(f"   æ¨¡å‹: {embedding_model}")
            print()
            
            results_info.append({
                'index': i,
                'timestamp': timestamp,
                'file_path': file_path,
                'total_questions': total_questions,
                'embedding_model': embedding_model
            })
            
        except Exception as e:
            print(f"âš ï¸ è¯»å–æ–‡ä»¶ {file_path} å¤±è´¥: {e}")
    
    return results_info

def view_evaluation_summary(file_path: Path):
    """æŸ¥çœ‹è¯„ä¼°ç»“æœæ‘˜è¦"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        print(f"ğŸ“Š è¯„ä¼°ç»“æœæ‘˜è¦")
        print("=" * 50)
        print(f"æ—¶é—´æˆ³: {data.get('timestamp', 'unknown')}")
        print(f"é—®é¢˜æ€»æ•°: {data.get('total_questions', 0)}")
        print(f"åµŒå…¥æ¨¡å‹: {data.get('evaluation_config', {}).get('embedding_model', 'unknown')}")
        
        summary_metrics = data.get('summary_metrics', {})
        
        if 'original_system' in summary_metrics and 'enhanced_system' in summary_metrics:
            print(f"\nğŸ”„ ç³»ç»Ÿå¯¹æ¯”:")
            print("-" * 30)
            
            # åˆ›å»ºå¯¹æ¯”è¡¨æ ¼
            k_values = [1, 3, 5, 10]
            metrics = ['precision', 'recall', 'ndcg']
            
            for k in k_values:
                print(f"\nK={k} æŒ‡æ ‡å¯¹æ¯”:")
                print(f"{'æŒ‡æ ‡':<12} {'åŸå§‹ç³»ç»Ÿ':<12} {'å¢å¼ºç³»ç»Ÿ':<12} {'æ”¹è¿›å¹…åº¦':<12}")
                print("-" * 50)
                
                for metric in metrics:
                    metric_key = f'{metric}@{k}'
                    
                    orig_data = summary_metrics['original_system'].get(metric_key, {})
                    enh_data = summary_metrics['enhanced_system'].get(metric_key, {})
                    
                    if orig_data and enh_data:
                        orig_val = orig_data['mean']
                        enh_val = enh_data['mean']
                        
                        if orig_val > 0:
                            improvement = ((enh_val - orig_val) / orig_val) * 100
                            improvement_str = f"{improvement:+.2f}%"
                        else:
                            improvement_str = "N/A"
                        
                        print(f"{metric.upper():<12} {orig_val:<12.4f} {enh_val:<12.4f} {improvement_str:<12}")
        
        return True
        
    except Exception as e:
        print(f"âŒ è¯»å–è¯„ä¼°ç»“æœå¤±è´¥: {e}")
        return False

def compare_multiple_evaluations(evaluation_dir: str = "evaluation"):
    """å¯¹æ¯”å¤šä¸ªè¯„ä¼°ç»“æœ"""
    eval_path = Path(evaluation_dir)
    summary_files = list(eval_path.glob("evaluation_summary_*.json"))
    
    if len(summary_files) < 2:
        print("âŒ éœ€è¦è‡³å°‘2ä¸ªè¯„ä¼°ç»“æœæ‰èƒ½è¿›è¡Œå¯¹æ¯”")
        return
    
    print(f"ğŸ“ˆ å¯¹æ¯” {len(summary_files)} ä¸ªè¯„ä¼°ç»“æœ")
    print("=" * 60)
    
    # æ”¶é›†æ‰€æœ‰ç»“æœæ•°æ®
    all_results = []
    
    for file_path in sorted(summary_files):
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            timestamp = data.get('timestamp', 'unknown')
            summary_metrics = data.get('summary_metrics', {})
            
            if 'enhanced_system' in summary_metrics:
                # æå–å¢å¼ºç³»ç»Ÿçš„å…³é”®æŒ‡æ ‡
                enh_metrics = summary_metrics['enhanced_system']
                
                result_row = {'timestamp': timestamp}
                
                for k in [1, 3, 5]:
                    for metric in ['precision', 'recall', 'ndcg']:
                        metric_key = f'{metric}@{k}'
                        if metric_key in enh_metrics:
                            result_row[metric_key] = enh_metrics[metric_key]['mean']
                
                all_results.append(result_row)
                
        except Exception as e:
            print(f"âš ï¸ å¤„ç†æ–‡ä»¶ {file_path} å¤±è´¥: {e}")
    
    if all_results:
        # åˆ›å»ºDataFrameå¹¶æ˜¾ç¤º
        df = pd.DataFrame(all_results)
        df = df.set_index('timestamp')
        
        print("å¢å¼ºç³»ç»Ÿæ€§èƒ½è¶‹åŠ¿ (å‡å€¼):")
        print(df.round(4))
        
        # è®¡ç®—æ”¹è¿›è¶‹åŠ¿
        if len(df) > 1:
            print(f"\nğŸ“ˆ æ€§èƒ½å˜åŒ–è¶‹åŠ¿:")
            for col in df.columns:
                first_val = df[col].iloc[0]
                last_val = df[col].iloc[-1]
                if first_val > 0:
                    change = ((last_val - first_val) / first_val) * 100
                    print(f"{col}: {change:+.2f}%")

def export_to_excel(evaluation_dir: str = "evaluation", output_file: str = "evaluation_comparison.xlsx"):
    """å¯¼å‡ºè¯„ä¼°ç»“æœåˆ°Excel"""
    eval_path = Path(evaluation_dir)
    summary_files = list(eval_path.glob("evaluation_summary_*.json"))
    
    if not summary_files:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°è¯„ä¼°ç»“æœæ–‡ä»¶")
        return
    
    print(f"ğŸ“Š å¯¼å‡º {len(summary_files)} ä¸ªè¯„ä¼°ç»“æœåˆ°Excel...")
    
    # æ”¶é›†æ•°æ®
    original_data = []
    enhanced_data = []
    
    for file_path in sorted(summary_files):
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            timestamp = data.get('timestamp', 'unknown')
            summary_metrics = data.get('summary_metrics', {})
            
            # åŸå§‹ç³»ç»Ÿæ•°æ®
            if 'original_system' in summary_metrics:
                row = {'timestamp': timestamp, 'system': 'original'}
                for metric_name, metric_data in summary_metrics['original_system'].items():
                    row[metric_name] = metric_data['mean']
                original_data.append(row)
            
            # å¢å¼ºç³»ç»Ÿæ•°æ®
            if 'enhanced_system' in summary_metrics:
                row = {'timestamp': timestamp, 'system': 'enhanced'}
                for metric_name, metric_data in summary_metrics['enhanced_system'].items():
                    row[metric_name] = metric_data['mean']
                enhanced_data.append(row)
                
        except Exception as e:
            print(f"âš ï¸ å¤„ç†æ–‡ä»¶ {file_path} å¤±è´¥: {e}")
    
    # åˆ›å»ºExcelæ–‡ä»¶
    with pd.ExcelWriter(output_file, engine='openpyxl') as writer:
        if original_data:
            df_orig = pd.DataFrame(original_data)
            df_orig.to_excel(writer, sheet_name='åŸå§‹ç³»ç»Ÿ', index=False)
        
        if enhanced_data:
            df_enh = pd.DataFrame(enhanced_data)
            df_enh.to_excel(writer, sheet_name='å¢å¼ºç³»ç»Ÿ', index=False)
        
        # å¦‚æœä¸¤ä¸ªç³»ç»Ÿéƒ½æœ‰æ•°æ®ï¼Œåˆ›å»ºå¯¹æ¯”è¡¨
        if original_data and enhanced_data:
            # åˆ›å»ºå¯¹æ¯”æ•°æ®
            comparison_data = []
            
            for orig_row, enh_row in zip(original_data, enhanced_data):
                if orig_row['timestamp'] == enh_row['timestamp']:
                    comp_row = {'timestamp': orig_row['timestamp']}
                    
                    for metric in ['precision@1', 'precision@3', 'precision@5', 
                                  'recall@1', 'recall@3', 'recall@5',
                                  'ndcg@1', 'ndcg@3', 'ndcg@5']:
                        if metric in orig_row and metric in enh_row:
                            orig_val = orig_row[metric]
                            enh_val = enh_row[metric]
                            
                            comp_row[f'{metric}_original'] = orig_val
                            comp_row[f'{metric}_enhanced'] = enh_val
                            
                            if orig_val > 0:
                                improvement = ((enh_val - orig_val) / orig_val) * 100
                                comp_row[f'{metric}_improvement'] = improvement
                    
                    comparison_data.append(comp_row)
            
            if comparison_data:
                df_comp = pd.DataFrame(comparison_data)
                df_comp.to_excel(writer, sheet_name='ç³»ç»Ÿå¯¹æ¯”', index=False)
    
    print(f"âœ… è¯„ä¼°ç»“æœå·²å¯¼å‡ºåˆ°: {output_file}")

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="è¯„ä¼°ç»“æœæŸ¥çœ‹å™¨")
    parser.add_argument('--action', choices=['list', 'view', 'compare', 'export'], 
                       default='list', help='æ“ä½œç±»å‹')
    parser.add_argument('--dir', type=str, default='evaluation', 
                       help='è¯„ä¼°ç»“æœç›®å½•')
    parser.add_argument('--index', type=int, help='æŸ¥çœ‹æŒ‡å®šç´¢å¼•çš„è¯„ä¼°ç»“æœ')
    parser.add_argument('--output', type=str, default='evaluation_comparison.xlsx',
                       help='Excelå¯¼å‡ºæ–‡ä»¶å')
    
    args = parser.parse_args()
    
    if args.action == 'list':
        results_info = list_evaluation_results(args.dir)
        
        if results_info:
            print("ğŸ’¡ ä½¿ç”¨æ–¹æ³•:")
            print(f"   æŸ¥çœ‹è¯¦æƒ…: python evaluation_viewer.py --action view --index <åºå·>")
            print(f"   å¯¹æ¯”ç»“æœ: python evaluation_viewer.py --action compare")
            print(f"   å¯¼å‡ºExcel: python evaluation_viewer.py --action export")
    
    elif args.action == 'view':
        if args.index is None:
            print("âŒ è¯·æŒ‡å®šè¦æŸ¥çœ‹çš„è¯„ä¼°ç»“æœç´¢å¼•: --index <åºå·>")
            return
        
        results_info = list_evaluation_results(args.dir)
        
        if args.index <= 0 or args.index > len(results_info):
            print(f"âŒ ç´¢å¼•è¶…å‡ºèŒƒå›´ï¼Œè¯·é€‰æ‹© 1-{len(results_info)}")
            return
        
        selected_result = results_info[args.index - 1]
        view_evaluation_summary(selected_result['file_path'])
    
    elif args.action == 'compare':
        compare_multiple_evaluations(args.dir)
    
    elif args.action == 'export':
        export_to_excel(args.dir, args.output)

if __name__ == '__main__':
    main()