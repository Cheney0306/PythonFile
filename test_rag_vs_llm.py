# test_rag_vs_llm.py - æµ‹è¯•RAG vs LLMè¯„ä¼°ç³»ç»Ÿ

import sys
from pathlib import Path
sys.path.append(str(Path(__file__).parent))

from rag_vs_llm_evaluation import RAGvsLLMEvaluator

def test_single_question():
    """æµ‹è¯•å•ä¸ªé—®é¢˜çš„è¯„ä¼°"""
    print("ğŸ§ª æµ‹è¯•å•ä¸ªé—®é¢˜è¯„ä¼°")
    print("=" * 40)
    
    evaluator = RAGvsLLMEvaluator()
    
    # æµ‹è¯•é—®é¢˜
    test_qa = {
        'question': 'Who is the leader of Belgium?',
        'expected_answer': 'Philippe of Belgium',
        'question_type': 'sub',
        'source_file': 'test'
    }
    
    print(f"ğŸ“ æµ‹è¯•é—®é¢˜: {test_qa['question']}")
    print(f"ğŸ“‹ æœŸæœ›ç­”æ¡ˆ: {test_qa['expected_answer']}")
    
    # è¯„ä¼°å•ä¸ªé—®é¢˜
    result = evaluator.evaluate_single_question(test_qa)
    
    print(f"\nğŸ” è¯„ä¼°ç»“æœ:")
    print(f"RAGç­”æ¡ˆ: {result['rag_answer']}")
    print(f"LLMç­”æ¡ˆ: {result['llm_answer']}")
    
    print(f"\nğŸ“Š RAGåˆ†æ•°:")
    for metric, score in result['rag_scores'].items():
        print(f"  {metric}: {score:.4f}")
    
    print(f"\nğŸ“Š LLMåˆ†æ•°:")
    for metric, score in result['llm_scores'].items():
        print(f"  {metric}: {score:.4f}")
    
    print("\n" + "=" * 40)
    print("ğŸ¯ å•ä¸ªé—®é¢˜æµ‹è¯•å®Œæˆï¼")

def test_answer_similarity():
    """æµ‹è¯•ç­”æ¡ˆç›¸ä¼¼åº¦è¯„ä¼°"""
    print("\nğŸ§ª æµ‹è¯•ç­”æ¡ˆç›¸ä¼¼åº¦è¯„ä¼°")
    print("=" * 40)
    
    evaluator = RAGvsLLMEvaluator()
    
    test_cases = [
        ("Philippe of Belgium", "Philippe of Belgium"),  # ç²¾ç¡®åŒ¹é…
        ("King Philippe", "Philippe of Belgium"),        # éƒ¨åˆ†åŒ¹é…
        ("Belgium's leader is Philippe", "Philippe of Belgium"),  # åŒ…å«åŒ¹é…
        ("Charles Michel", "Philippe of Belgium"),       # ä¸åŒ¹é…
        ("Unknown", "Philippe of Belgium")               # å®Œå…¨ä¸åŒ¹é…
    ]
    
    for predicted, expected in test_cases:
        scores = evaluator.evaluate_answer_similarity(predicted, expected)
        print(f"\né¢„æµ‹: '{predicted}' vs æœŸæœ›: '{expected}'")
        print(f"  ç²¾ç¡®åŒ¹é…: {scores['exact_match']:.2f}")
        print(f"  åŒ…å«åŒ¹é…: {scores['contains_match']:.2f}")
        print(f"  è¯æ±‡é‡å : {scores['word_overlap']:.2f}")
        print(f"  ç»¼åˆåˆ†æ•°: {scores['composite_score']:.2f}")

def show_usage_examples():
    """æ˜¾ç¤ºä½¿ç”¨ç¤ºä¾‹"""
    print("\nğŸ’¡ RAG vs LLMè¯„ä¼°ä½¿ç”¨ç¤ºä¾‹:")
    print("=" * 40)
    print("1. å¿«é€Ÿè¯„ä¼°ï¼ˆ20ä¸ªé—®é¢˜ï¼‰:")
    print("   python rag_vs_llm_evaluation.py --mode quick")
    print()
    print("2. å¿«é€Ÿè¯„ä¼°ï¼ˆè‡ªå®šä¹‰æ•°é‡ï¼‰:")
    print("   python rag_vs_llm_evaluation.py --mode quick --sample-size 50")
    print()
    print("3. å®Œæ•´è¯„ä¼°ï¼ˆæ‰€æœ‰é—®é¢˜ï¼‰:")
    print("   python rag_vs_llm_evaluation.py --mode full")
    print()
    print("4. æŒ‡å®šæ•°æ®é›†è·¯å¾„:")
    print("   python rag_vs_llm_evaluation.py --mode quick --qa-path custom_qa_datasets")
    print("=" * 40)

if __name__ == "__main__":
    # æ˜¾ç¤ºä½¿ç”¨ç¤ºä¾‹
    show_usage_examples()
    
    # æµ‹è¯•ç­”æ¡ˆç›¸ä¼¼åº¦è¯„ä¼°
    test_answer_similarity()
    
    # æµ‹è¯•å•ä¸ªé—®é¢˜è¯„ä¼°ï¼ˆéœ€è¦APIå¯†é’¥ï¼‰
    try:
        test_single_question()
    except Exception as e:
        print(f"\nâš ï¸ å•ä¸ªé—®é¢˜æµ‹è¯•è·³è¿‡: {e}")
        print("ğŸ’¡ è¯·ç¡®ä¿è®¾ç½®äº†OpenAI APIå¯†é’¥ä»¥è¿›è¡Œå®Œæ•´æµ‹è¯•")