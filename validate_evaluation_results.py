#!/usr/bin/env python3
# validate_evaluation_results.py - éªŒè¯RAG vs LLMè¯„ä¼°ç»“æœçš„æ­£ç¡®ç‡

import json
import os
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Tuple
import csv

class EvaluationResultValidator:
    """è¯„ä¼°ç»“æœéªŒè¯å™¨"""
    
    def __init__(self, evaluation_dir: str = "evaluation"):
        self.evaluation_dir = Path(evaluation_dir)
        self.output_dir = self.evaluation_dir / "evaluation_result"
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        self.output_dir.mkdir(exist_ok=True)
    
    def find_simple_qa_records_files(self) -> List[Path]:
        """æŸ¥æ‰¾æ‰€æœ‰simple_qa_recordsæ–‡ä»¶"""
        pattern = "simple_qa_records_*.jsonl"
        files = list(self.evaluation_dir.glob(pattern))
        
        print(f"ğŸ” åœ¨ {self.evaluation_dir} ä¸­æ‰¾åˆ° {len(files)} ä¸ªè¯„ä¼°ç»“æœæ–‡ä»¶:")
        for file in files:
            print(f"   - {file.name}")
        
        return files
    
    def load_qa_records(self, file_path: Path) -> List[Dict]:
        """åŠ è½½QAè®°å½•"""
        records = []
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                for line_num, line in enumerate(f, 1):
                    line = line.strip()
                    if line:
                        try:
                            record = json.loads(line)
                            records.append(record)
                        except json.JSONDecodeError as e:
                            print(f"âš  ç¬¬ {line_num} è¡ŒJSONè§£æé”™è¯¯: {e}")
            
            print(f"âœ… æˆåŠŸåŠ è½½ {len(records)} æ¡QAè®°å½•")
            
        except Exception as e:
            print(f"âŒ åŠ è½½æ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {e}")
        
        return records
    
    def normalize_answer(self, answer: str) -> str:
        """æ ‡å‡†åŒ–ç­”æ¡ˆæ–‡æœ¬ï¼Œç”¨äºæ¯”è¾ƒ"""
        if not answer:
            return ""
        
        # è½¬æ¢ä¸ºå°å†™å¹¶å»é™¤é¦–å°¾ç©ºæ ¼
        normalized = answer.lower().strip()
        
        # å»é™¤æ ‡ç‚¹ç¬¦å·
        import string
        normalized = normalized.translate(str.maketrans('', '', string.punctuation))
        
        # å»é™¤å¤šä½™ç©ºæ ¼
        normalized = ' '.join(normalized.split())
        
        return normalized
    
    def is_answer_correct(self, predicted: str, expected: str) -> bool:
        """åˆ¤æ–­ç­”æ¡ˆæ˜¯å¦æ­£ç¡®"""
        pred_norm = self.normalize_answer(predicted)
        exp_norm = self.normalize_answer(expected)
        
        if not pred_norm or not exp_norm:
            return False
        
        # ç²¾ç¡®åŒ¹é…
        if pred_norm == exp_norm:
            return True
        
        # åŒ…å«åŒ¹é…ï¼ˆé¢„æµ‹ç­”æ¡ˆåŒ…å«æœŸæœ›ç­”æ¡ˆæˆ–åä¹‹ï¼‰
        if exp_norm in pred_norm or pred_norm in exp_norm:
            return True
        
        return False
    
    def validate_records(self, records: List[Dict]) -> Dict:
        """éªŒè¯QAè®°å½•çš„æ­£ç¡®ç‡"""
        results = {
            'total_questions': len(records),
            'rag_correct': 0,
            'llm_correct': 0,
            'both_correct': 0,
            'both_wrong': 0,
            'rag_only_correct': 0,
            'llm_only_correct': 0,
            'detailed_results': []
        }
        
        print(f"\nğŸ” å¼€å§‹éªŒè¯ {len(records)} æ¡QAè®°å½•...")
        
        for i, record in enumerate(records, 1):
            question = record.get('question', '')
            expected = record.get('expected_answer', '')
            rag_answer = record.get('rag_answer', '')
            llm_answer = record.get('llm_answer', '')
            
            # åˆ¤æ–­æ­£ç¡®æ€§
            rag_correct = self.is_answer_correct(rag_answer, expected)
            llm_correct = self.is_answer_correct(llm_answer, expected)
            
            # ç»Ÿè®¡
            if rag_correct:
                results['rag_correct'] += 1
            if llm_correct:
                results['llm_correct'] += 1
            
            if rag_correct and llm_correct:
                results['both_correct'] += 1
            elif not rag_correct and not llm_correct:
                results['both_wrong'] += 1
            elif rag_correct and not llm_correct:
                results['rag_only_correct'] += 1
            elif not rag_correct and llm_correct:
                results['llm_only_correct'] += 1
            
            # ä¿å­˜è¯¦ç»†ç»“æœ
            detailed_result = {
                'question_id': i,
                'question': question,
                'expected_answer': expected,
                'rag_answer': rag_answer,
                'llm_answer': llm_answer,
                'rag_correct': rag_correct,
                'llm_correct': llm_correct,
                'rag_normalized': self.normalize_answer(rag_answer),
                'llm_normalized': self.normalize_answer(llm_answer),
                'expected_normalized': self.normalize_answer(expected)
            }
            results['detailed_results'].append(detailed_result)
        
        # è®¡ç®—æ­£ç¡®ç‡
        total = results['total_questions']
        if total > 0:
            results['rag_accuracy'] = results['rag_correct'] / total
            results['llm_accuracy'] = results['llm_correct'] / total
        else:
            results['rag_accuracy'] = 0.0
            results['llm_accuracy'] = 0.0
        
        return results
    
    def save_validation_results(self, results: Dict, source_file: str):
        """ä¿å­˜éªŒè¯ç»“æœ"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        base_name = Path(source_file).stem
        
        # 1. ä¿å­˜JSONæ ¼å¼çš„å®Œæ•´ç»“æœ
        json_file = self.output_dir / f"{base_name}_validation_{timestamp}.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        # 2. ä¿å­˜CSVæ ¼å¼çš„è¯¦ç»†ç»“æœ
        csv_file = self.output_dir / f"{base_name}_detailed_{timestamp}.csv"
        with open(csv_file, 'w', newline='', encoding='utf-8-sig') as f:
            writer = csv.writer(f)
            
            # å†™å…¥æ ‡é¢˜è¡Œ
            writer.writerow([
                'é—®é¢˜ID', 'é—®é¢˜', 'æœŸæœ›ç­”æ¡ˆ', 'RAGç­”æ¡ˆ', 'LLMç­”æ¡ˆ',
                'RAGæ­£ç¡®', 'LLMæ­£ç¡®', 'RAGæ ‡å‡†åŒ–', 'LLMæ ‡å‡†åŒ–', 'æœŸæœ›æ ‡å‡†åŒ–'
            ])
            
            # å†™å…¥æ•°æ®è¡Œ
            for detail in results['detailed_results']:
                writer.writerow([
                    detail['question_id'],
                    detail['question'],
                    detail['expected_answer'],
                    detail['rag_answer'],
                    detail['llm_answer'],
                    'âœ“' if detail['rag_correct'] else 'âœ—',
                    'âœ“' if detail['llm_correct'] else 'âœ—',
                    detail['rag_normalized'],
                    detail['llm_normalized'],
                    detail['expected_normalized']
                ])
        
        # 3. ä¿å­˜æ±‡æ€»æŠ¥å‘Š
        report_file = self.output_dir / f"{base_name}_summary_{timestamp}.txt"
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(f"RAG vs LLM è¯„ä¼°ç»“æœéªŒè¯æŠ¥å‘Š\n")
            f.write(f"=" * 50 + "\n\n")
            f.write(f"æºæ–‡ä»¶: {source_file}\n")
            f.write(f"éªŒè¯æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            f.write(f"ğŸ“Š æ€»ä½“ç»Ÿè®¡:\n")
            f.write(f"   æ€»é—®é¢˜æ•°: {results['total_questions']}\n")
            f.write(f"   RAGæ­£ç¡®æ•°: {results['rag_correct']}\n")
            f.write(f"   LLMæ­£ç¡®æ•°: {results['llm_correct']}\n\n")
            
            f.write(f"ğŸ“ˆ æ­£ç¡®ç‡:\n")
            f.write(f"   RAGæ­£ç¡®ç‡: {results['rag_accuracy']:.2%}\n")
            f.write(f"   LLMæ­£ç¡®ç‡: {results['llm_accuracy']:.2%}\n\n")
            
            f.write(f"ğŸ” è¯¦ç»†åˆ†æ:\n")
            f.write(f"   ä¸¤è€…éƒ½æ­£ç¡®: {results['both_correct']} ({results['both_correct']/results['total_questions']:.2%})\n")
            f.write(f"   ä¸¤è€…éƒ½é”™è¯¯: {results['both_wrong']} ({results['both_wrong']/results['total_questions']:.2%})\n")
            f.write(f"   ä»…RAGæ­£ç¡®: {results['rag_only_correct']} ({results['rag_only_correct']/results['total_questions']:.2%})\n")
            f.write(f"   ä»…LLMæ­£ç¡®: {results['llm_only_correct']} ({results['llm_only_correct']/results['total_questions']:.2%})\n\n")
            
            # æ·»åŠ é”™è¯¯æ¡ˆä¾‹åˆ†æ
            f.write(f"âŒ é”™è¯¯æ¡ˆä¾‹åˆ†æ:\n")
            f.write(f"-" * 30 + "\n")
            
            error_count = 0
            for detail in results['detailed_results']:
                if not detail['rag_correct'] or not detail['llm_correct']:
                    error_count += 1
                    if error_count <= 10:  # åªæ˜¾ç¤ºå‰10ä¸ªé”™è¯¯æ¡ˆä¾‹
                        f.write(f"\næ¡ˆä¾‹ {error_count}:\n")
                        f.write(f"  é—®é¢˜: {detail['question']}\n")
                        f.write(f"  æœŸæœ›: {detail['expected_answer']}\n")
                        f.write(f"  RAG: {detail['rag_answer']} {'âœ“' if detail['rag_correct'] else 'âœ—'}\n")
                        f.write(f"  LLM: {detail['llm_answer']} {'âœ“' if detail['llm_correct'] else 'âœ—'}\n")
        
        print(f"\nğŸ’¾ éªŒè¯ç»“æœå·²ä¿å­˜:")
        print(f"   - å®Œæ•´ç»“æœ: {json_file.name}")
        print(f"   - è¯¦ç»†CSV: {csv_file.name}")
        print(f"   - æ±‡æ€»æŠ¥å‘Š: {report_file.name}")
        
        return {
            'json_file': json_file,
            'csv_file': csv_file,
            'report_file': report_file
        }
    
    def print_summary(self, results: Dict):
        """æ‰“å°éªŒè¯ç»“æœæ‘˜è¦"""
        total = results['total_questions']
        
        print(f"\nğŸ“Š éªŒè¯ç»“æœæ‘˜è¦:")
        print(f"=" * 40)
        print(f"æ€»é—®é¢˜æ•°: {total}")
        print(f"RAGæ­£ç¡®æ•°: {results['rag_correct']} ({results['rag_accuracy']:.2%})")
        print(f"LLMæ­£ç¡®æ•°: {results['llm_correct']} ({results['llm_accuracy']:.2%})")
        print()
        print(f"è¯¦ç»†åˆ†æ:")
        print(f"  ä¸¤è€…éƒ½æ­£ç¡®: {results['both_correct']} ({results['both_correct']/total:.2%})")
        print(f"  ä¸¤è€…éƒ½é”™è¯¯: {results['both_wrong']} ({results['both_wrong']/total:.2%})")
        print(f"  ä»…RAGæ­£ç¡®: {results['rag_only_correct']} ({results['rag_only_correct']/total:.2%})")
        print(f"  ä»…LLMæ­£ç¡®: {results['llm_only_correct']} ({results['llm_only_correct']/total:.2%})")
        
        # åˆ¤æ–­å“ªä¸ªç³»ç»Ÿè¡¨ç°æ›´å¥½
        if results['rag_accuracy'] > results['llm_accuracy']:
            diff = results['rag_accuracy'] - results['llm_accuracy']
            print(f"\nğŸ† RAGç³»ç»Ÿè¡¨ç°æ›´å¥½ï¼Œé¢†å…ˆ {diff:.2%}")
        elif results['llm_accuracy'] > results['rag_accuracy']:
            diff = results['llm_accuracy'] - results['rag_accuracy']
            print(f"\nğŸ† LLMç³»ç»Ÿè¡¨ç°æ›´å¥½ï¼Œé¢†å…ˆ {diff:.2%}")
        else:
            print(f"\nğŸ¤ ä¸¤ä¸ªç³»ç»Ÿè¡¨ç°ç›¸å½“")

def validate_specific_file(file_path: str):
    """éªŒè¯æŒ‡å®šçš„è¯„ä¼°ç»“æœæ–‡ä»¶"""
    print(f"ğŸ¯ éªŒè¯æŒ‡å®šæ–‡ä»¶: {file_path}")
    print("=" * 60)
    
    validator = EvaluationResultValidator()
    
    file_path_obj = Path(file_path)
    if not file_path_obj.exists():
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        return
    
    # åŠ è½½QAè®°å½•
    records = validator.load_qa_records(file_path_obj)
    if not records:
        print("âŒ æ— æ³•åŠ è½½QAè®°å½•")
        return
    
    # éªŒè¯ç»“æœ
    results = validator.validate_records(records)
    
    # ä¿å­˜ç»“æœ
    saved_files = validator.save_validation_results(results, file_path_obj.name)
    
    # æ‰“å°æ‘˜è¦
    validator.print_summary(results)
    
    print(f"\nâœ… éªŒè¯å®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ° evaluation_result æ–‡ä»¶å¤¹")

def validate_all_files():
    """éªŒè¯æ‰€æœ‰è¯„ä¼°ç»“æœæ–‡ä»¶"""
    print("ğŸ” éªŒè¯æ‰€æœ‰è¯„ä¼°ç»“æœæ–‡ä»¶")
    print("=" * 50)
    
    validator = EvaluationResultValidator()
    
    # æŸ¥æ‰¾æ‰€æœ‰æ–‡ä»¶
    files = validator.find_simple_qa_records_files()
    
    if not files:
        print("âŒ æœªæ‰¾åˆ°ä»»ä½•è¯„ä¼°ç»“æœæ–‡ä»¶")
        return
    
    # éªŒè¯æ¯ä¸ªæ–‡ä»¶
    for i, file_path in enumerate(files, 1):
        print(f"\nğŸ“„ éªŒè¯æ–‡ä»¶ {i}/{len(files)}: {file_path.name}")
        print("-" * 40)
        
        # åŠ è½½QAè®°å½•
        records = validator.load_qa_records(file_path)
        if not records:
            continue
        
        # éªŒè¯ç»“æœ
        results = validator.validate_records(records)
        
        # ä¿å­˜ç»“æœ
        saved_files = validator.save_validation_results(results, file_path.name)
        
        # æ‰“å°æ‘˜è¦
        validator.print_summary(results)
    
    print(f"\nâœ… æ‰€æœ‰æ–‡ä»¶éªŒè¯å®Œæˆï¼")

if __name__ == '__main__':
    import argparse
    
    parser = argparse.ArgumentParser(description="éªŒè¯RAG vs LLMè¯„ä¼°ç»“æœçš„æ­£ç¡®ç‡")
    parser.add_argument('--file', type=str, 
                       help='æŒ‡å®šè¦éªŒè¯çš„æ–‡ä»¶è·¯å¾„ (ä¾‹å¦‚: evaluation/simple_qa_records_20250831_005030.jsonl)')
    parser.add_argument('--all', action='store_true',
                       help='éªŒè¯evaluationç›®å½•ä¸‹çš„æ‰€æœ‰æ–‡ä»¶')
    
    args = parser.parse_args()
    
    if args.file:
        validate_specific_file(args.file)
    elif args.all:
        validate_all_files()
    else:
        # é»˜è®¤éªŒè¯æŒ‡å®šæ–‡ä»¶
        target_file = r"D:\PythonFile\newSystem\evaluation\simple_qa_records_20250831_005030.jsonl"
        validate_specific_file(target_file)