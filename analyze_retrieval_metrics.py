#!/usr/bin/env python3
# analyze_retrieval_metrics.py - åˆ†ææ£€ç´¢æŒ‡æ ‡æ¥è¯Šæ–­RAGé—®é¢˜

import json
from pathlib import Path
from collections import defaultdict
import numpy as np

class RetrievalMetricsAnalyzer:
    """æ£€ç´¢æŒ‡æ ‡åˆ†æå™¨"""
    
    def __init__(self):
        pass
    
    def load_evaluation_results(self, results_file: str):
        """åŠ è½½è¯„ä¼°ç»“æœ"""
        try:
            with open(results_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            return data
        except Exception as e:
            print(f"âŒ åŠ è½½è¯„ä¼°ç»“æœå¤±è´¥: {e}")
            return None
    
    def analyze_retrieval_performance(self, results_data):
        """åˆ†ææ£€ç´¢æ€§èƒ½æŒ‡æ ‡"""
        print("ğŸ¯ æ£€ç´¢æ€§èƒ½æŒ‡æ ‡åˆ†æ")
        print("=" * 50)
        
        # æå–æ£€ç´¢æŒ‡æ ‡
        retrieval_metrics = []
        answer_correctness = []
        
        for result in results_data.get('results', []):
            if 'rag_retrieval_metrics' in result:
                metrics = result['rag_retrieval_metrics']
                retrieval_metrics.append(metrics)
                
                # åŒæ—¶è®°å½•ç­”æ¡ˆæ­£ç¡®æ€§
                rag_correct = result['rag_scores']['composite_score'] > 0.5
                answer_correctness.append(rag_correct)
        
        if not retrieval_metrics:
            print("âŒ æœªæ‰¾åˆ°æ£€ç´¢æŒ‡æ ‡æ•°æ®")
            return
        
        print(f"ğŸ“Š åˆ†æ {len(retrieval_metrics)} ä¸ªé—®é¢˜çš„æ£€ç´¢æŒ‡æ ‡")
        
        # è®¡ç®—å¹³å‡æŒ‡æ ‡
        avg_metrics = self._calculate_average_metrics(retrieval_metrics)
        
        # åˆ†ææŒ‡æ ‡å«ä¹‰
        self._interpret_metrics(avg_metrics)
        
        # åˆ†ææ£€ç´¢ä¸ç­”æ¡ˆè´¨é‡çš„å…³ç³»
        self._analyze_retrieval_answer_correlation(retrieval_metrics, answer_correctness)
        
        # ç”Ÿæˆè¯Šæ–­æŠ¥å‘Š
        self._generate_diagnostic_report(avg_metrics, answer_correctness)
        
        return avg_metrics
    
    def _calculate_average_metrics(self, retrieval_metrics):
        """è®¡ç®—å¹³å‡æ£€ç´¢æŒ‡æ ‡"""
        metrics_sum = defaultdict(list)
        
        for metrics in retrieval_metrics:
            for metric_name, value in metrics.items():
                if isinstance(value, (int, float)):
                    metrics_sum[metric_name].append(value)
        
        avg_metrics = {}
        for metric_name, values in metrics_sum.items():
            if values:
                avg_metrics[metric_name] = {
                    'mean': np.mean(values),
                    'std': np.std(values),
                    'min': np.min(values),
                    'max': np.max(values)
                }
        
        return avg_metrics
    
    def _interpret_metrics(self, avg_metrics):
        """è§£é‡Šæ£€ç´¢æŒ‡æ ‡çš„å«ä¹‰"""
        print(f"\nğŸ“ˆ æ£€ç´¢æŒ‡æ ‡è¯¦ç»†åˆ†æ:")
        print("-" * 40)
        
        # Precision@k åˆ†æ
        precision_metrics = {k: v for k, v in avg_metrics.items() if 'precision@' in k}
        if precision_metrics:
            print(f"\nğŸ¯ Precision@k (æ£€ç´¢ç²¾åº¦):")
            for metric, stats in precision_metrics.items():
                k = metric.split('@')[1]
                mean_val = stats['mean']
                print(f"   {metric}: {mean_val:.4f} (Â±{stats['std']:.4f})")
                
                # è§£é‡Šå«ä¹‰
                if mean_val < 0.3:
                    print(f"     âŒ å¾ˆä½ - å‰{k}ä¸ªç»“æœä¸­ç›¸å…³æ–‡æ¡£å¾ˆå°‘")
                elif mean_val < 0.5:
                    print(f"     âš ï¸ åä½ - å‰{k}ä¸ªç»“æœä¸­ç›¸å…³æ–‡æ¡£ä¸è¶³ä¸€åŠ")
                elif mean_val < 0.7:
                    print(f"     âœ… ä¸­ç­‰ - å‰{k}ä¸ªç»“æœä¸­æœ‰é€‚é‡ç›¸å…³æ–‡æ¡£")
                else:
                    print(f"     ğŸ† å¾ˆå¥½ - å‰{k}ä¸ªç»“æœä¸­å¤§éƒ¨åˆ†éƒ½ç›¸å…³")
        
        # Recall@k åˆ†æ
        recall_metrics = {k: v for k, v in avg_metrics.items() if 'recall@' in k}
        if recall_metrics:
            print(f"\nğŸ” Recall@k (æ£€ç´¢å¬å›ç‡):")
            for metric, stats in recall_metrics.items():
                k = metric.split('@')[1]
                mean_val = stats['mean']
                print(f"   {metric}: {mean_val:.4f} (Â±{stats['std']:.4f})")
                
                # è§£é‡Šå«ä¹‰
                if mean_val < 0.3:
                    print(f"     âŒ å¾ˆä½ - é—æ¼äº†å¤§é‡ç›¸å…³æ–‡æ¡£")
                elif mean_val < 0.5:
                    print(f"     âš ï¸ åä½ - é—æ¼äº†è¾ƒå¤šç›¸å…³æ–‡æ¡£")
                elif mean_val < 0.7:
                    print(f"     âœ… ä¸­ç­‰ - æ‰¾åˆ°äº†å¤§éƒ¨åˆ†ç›¸å…³æ–‡æ¡£")
                else:
                    print(f"     ğŸ† å¾ˆå¥½ - æ‰¾åˆ°äº†å‡ ä¹æ‰€æœ‰ç›¸å…³æ–‡æ¡£")
        
        # nDCG@k åˆ†æ
        ndcg_metrics = {k: v for k, v in avg_metrics.items() if 'ndcg@' in k}
        if ndcg_metrics:
            print(f"\nğŸ“Š nDCG@k (æ’åºè´¨é‡):")
            for metric, stats in ndcg_metrics.items():
                k = metric.split('@')[1]
                mean_val = stats['mean']
                print(f"   {metric}: {mean_val:.4f} (Â±{stats['std']:.4f})")
                
                # è§£é‡Šå«ä¹‰
                if mean_val < 0.3:
                    print(f"     âŒ å¾ˆå·® - ç›¸å…³æ–‡æ¡£æ’åºå¾ˆé å")
                elif mean_val < 0.5:
                    print(f"     âš ï¸ åå·® - ç›¸å…³æ–‡æ¡£æ’åºä¸å¤Ÿé å‰")
                elif mean_val < 0.7:
                    print(f"     âœ… ä¸­ç­‰ - ç›¸å…³æ–‡æ¡£æ’åºè¾ƒä¸ºåˆç†")
                else:
                    print(f"     ğŸ† å¾ˆå¥½ - ç›¸å…³æ–‡æ¡£æ’åºå¾ˆé å‰")
    
    def _analyze_retrieval_answer_correlation(self, retrieval_metrics, answer_correctness):
        """åˆ†ææ£€ç´¢è´¨é‡ä¸ç­”æ¡ˆæ­£ç¡®æ€§çš„å…³ç³»"""
        print(f"\nğŸ”— æ£€ç´¢è´¨é‡ä¸ç­”æ¡ˆæ­£ç¡®æ€§å…³è”åˆ†æ:")
        print("-" * 40)
        
        # æŒ‰ç­”æ¡ˆæ­£ç¡®æ€§åˆ†ç»„åˆ†ææ£€ç´¢æŒ‡æ ‡
        correct_metrics = []
        incorrect_metrics = []
        
        for i, is_correct in enumerate(answer_correctness):
            if i < len(retrieval_metrics):
                if is_correct:
                    correct_metrics.append(retrieval_metrics[i])
                else:
                    incorrect_metrics.append(retrieval_metrics[i])
        
        print(f"   æ­£ç¡®ç­”æ¡ˆæ•°: {len(correct_metrics)}")
        print(f"   é”™è¯¯ç­”æ¡ˆæ•°: {len(incorrect_metrics)}")
        
        # å¯¹æ¯”å…³é”®æŒ‡æ ‡
        key_metrics = ['precision@1', 'recall@1', 'ndcg@1', 'precision@5', 'recall@5', 'ndcg@5']
        
        for metric in key_metrics:
            if correct_metrics and incorrect_metrics:
                correct_values = [m.get(metric, 0) for m in correct_metrics if metric in m]
                incorrect_values = [m.get(metric, 0) for m in incorrect_metrics if metric in m]
                
                if correct_values and incorrect_values:
                    correct_avg = np.mean(correct_values)
                    incorrect_avg = np.mean(incorrect_values)
                    diff = correct_avg - incorrect_avg
                    
                    print(f"\n   {metric}:")
                    print(f"     æ­£ç¡®ç­”æ¡ˆæ—¶: {correct_avg:.4f}")
                    print(f"     é”™è¯¯ç­”æ¡ˆæ—¶: {incorrect_avg:.4f}")
                    print(f"     å·®å¼‚: {diff:+.4f}")
                    
                    if abs(diff) > 0.1:
                        if diff > 0:
                            print(f"     ğŸ’¡ æ£€ç´¢è´¨é‡ä¸ç­”æ¡ˆæ­£ç¡®æ€§æ­£ç›¸å…³")
                        else:
                            print(f"     âš ï¸ æ£€ç´¢è´¨é‡ä¸ç­”æ¡ˆæ­£ç¡®æ€§è´Ÿç›¸å…³(å¼‚å¸¸)")
                    else:
                        print(f"     â“ æ£€ç´¢è´¨é‡ä¸ç­”æ¡ˆæ­£ç¡®æ€§å…³è”è¾ƒå¼±")
    
    def _generate_diagnostic_report(self, avg_metrics, answer_correctness):
        """ç”Ÿæˆè¯Šæ–­æŠ¥å‘Š"""
        print(f"\nğŸ”§ é—®é¢˜è¯Šæ–­ä¸æ”¹è¿›å»ºè®®:")
        print("=" * 40)
        
        issues = []
        suggestions = []
        
        # åˆ†æ Precision@1
        precision_1 = avg_metrics.get('precision@1', {}).get('mean', 0)
        if precision_1 < 0.3:
            issues.append("Precision@1è¿‡ä½ - æœ€ç›¸å…³çš„æ£€ç´¢ç»“æœè´¨é‡å·®")
            suggestions.extend([
                "ğŸ”§ ä¼˜åŒ–åµŒå…¥æ¨¡å‹æˆ–æŸ¥è¯¢é¢„å¤„ç†",
                "ğŸ”§ è°ƒæ•´ç›¸ä¼¼åº¦è®¡ç®—æ–¹æ³•",
                "ğŸ”§ æ”¹è¿›æ•°æ®è´¨é‡å’Œæ ‡æ³¨"
            ])
        
        # åˆ†æ Recall@5
        recall_5 = avg_metrics.get('recall@5', {}).get('mean', 0)
        if recall_5 < 0.5:
            issues.append("Recall@5è¿‡ä½ - æ£€ç´¢èŒƒå›´ä¸å¤Ÿå¹¿")
            suggestions.extend([
                "ğŸ“ˆ å¢åŠ æ£€ç´¢æ•°é‡ (n_results)",
                "ğŸ“ˆ é™ä½ç›¸ä¼¼åº¦é˜ˆå€¼",
                "ğŸ“ˆ ä½¿ç”¨å¤šç§æ£€ç´¢ç­–ç•¥"
            ])
        
        # åˆ†æ nDCG@k
        ndcg_1 = avg_metrics.get('ndcg@1', {}).get('mean', 0)
        ndcg_5 = avg_metrics.get('ndcg@5', {}).get('mean', 0)
        
        if ndcg_1 < 0.4:
            issues.append("nDCG@1è¿‡ä½ - æ’åºç®—æ³•æœ‰é—®é¢˜")
            suggestions.extend([
                "ğŸ¯ æ”¹è¿›é‡æ’åºç®—æ³•",
                "ğŸ¯ ä¼˜åŒ–ç›¸ä¼¼åº¦æƒé‡",
                "ğŸ¯ ä½¿ç”¨å­¦ä¹ æ’åºæ–¹æ³•"
            ])
        
        if ndcg_5 > ndcg_1 * 1.5:
            issues.append("nDCGå·®å¼‚å¤§ - ç›¸å…³æ–‡æ¡£æ’åºé å")
            suggestions.extend([
                "â¬†ï¸ ä¼˜åŒ–ç¬¬ä¸€é˜¶æ®µæ£€ç´¢",
                "â¬†ï¸ æ”¹è¿›æŸ¥è¯¢æ‰©å±•ç­–ç•¥"
            ])
        
        # åˆ†æç­”æ¡ˆæ­£ç¡®ç‡
        correct_rate = sum(answer_correctness) / len(answer_correctness) if answer_correctness else 0
        if correct_rate < 0.5:
            issues.append(f"ç­”æ¡ˆæ­£ç¡®ç‡è¿‡ä½ ({correct_rate:.2%})")
            suggestions.extend([
                "ğŸ’¬ ä¼˜åŒ–ç­”æ¡ˆæå–é€»è¾‘",
                "ğŸ’¬ æ”¹è¿›CoTKR prompt",
                "ğŸ’¬ è°ƒæ•´LLMå‚æ•°"
            ])
        
        # è¾“å‡ºè¯Šæ–­ç»“æœ
        if issues:
            print(f"\nâŒ å‘ç°çš„é—®é¢˜:")
            for i, issue in enumerate(issues, 1):
                print(f"   {i}. {issue}")
        
        if suggestions:
            print(f"\nğŸ’¡ æ”¹è¿›å»ºè®®:")
            unique_suggestions = list(set(suggestions))
            for i, suggestion in enumerate(unique_suggestions, 1):
                print(f"   {i}. {suggestion}")
        
        # ç”Ÿæˆä¼˜å…ˆçº§å»ºè®®
        self._generate_priority_suggestions(avg_metrics, correct_rate)
    
    def _generate_priority_suggestions(self, avg_metrics, correct_rate):
        """ç”Ÿæˆä¼˜å…ˆçº§æ”¹è¿›å»ºè®®"""
        print(f"\nğŸ¯ ä¼˜å…ˆçº§æ”¹è¿›å»ºè®®:")
        print("-" * 30)
        
        precision_1 = avg_metrics.get('precision@1', {}).get('mean', 0)
        recall_5 = avg_metrics.get('recall@5', {}).get('mean', 0)
        ndcg_1 = avg_metrics.get('ndcg@1', {}).get('mean', 0)
        
        priority_actions = []
        
        # é«˜ä¼˜å…ˆçº§
        if precision_1 < 0.2:
            priority_actions.append(("é«˜", "ç«‹å³æ£€æŸ¥æ•°æ®è´¨é‡å’ŒåµŒå…¥æ¨¡å‹"))
        
        if recall_5 < 0.3:
            priority_actions.append(("é«˜", "å¢åŠ æ£€ç´¢æ•°é‡åˆ°10-15ä¸ª"))
        
        if correct_rate < 0.3:
            priority_actions.append(("é«˜", "æ£€æŸ¥ç­”æ¡ˆæå–é€»è¾‘"))
        
        # ä¸­ä¼˜å…ˆçº§
        if ndcg_1 < 0.4:
            priority_actions.append(("ä¸­", "ä¼˜åŒ–é‡æ’åºç®—æ³•"))
        
        if precision_1 < 0.5:
            priority_actions.append(("ä¸­", "æ”¹è¿›æŸ¥è¯¢é¢„å¤„ç†"))
        
        # ä½ä¼˜å…ˆçº§
        if recall_5 < 0.7:
            priority_actions.append(("ä½", "è€ƒè™‘æŸ¥è¯¢æ‰©å±•"))
        
        # è¾“å‡ºä¼˜å…ˆçº§å»ºè®®
        for priority, action in priority_actions:
            print(f"   ã€{priority}ã€‘ {action}")

def analyze_specific_evaluation_file(file_path: str):
    """åˆ†ææŒ‡å®šçš„è¯„ä¼°ç»“æœæ–‡ä»¶"""
    print(f"ğŸ” åˆ†æè¯„ä¼°ç»“æœæ–‡ä»¶: {file_path}")
    print("=" * 60)
    
    analyzer = RetrievalMetricsAnalyzer()
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not Path(file_path).exists():
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        return
    
    # åŠ è½½è¯„ä¼°ç»“æœ
    results_data = analyzer.load_evaluation_results(file_path)
    if not results_data:
        return
    
    # åˆ†ææ£€ç´¢æ€§èƒ½
    avg_metrics = analyzer.analyze_retrieval_performance(results_data)
    
    print(f"\nâœ… åˆ†æå®Œæˆï¼")
    print(f"\nğŸ“‹ å…³é”®å‘ç°:")
    print(f"   - é€šè¿‡æ£€ç´¢æŒ‡æ ‡å¯ä»¥ç²¾ç¡®å®šä½é—®é¢˜")
    print(f"   - Precision@k åæ˜ æ£€ç´¢ç²¾åº¦")
    print(f"   - Recall@k åæ˜ æ£€ç´¢è¦†ç›–åº¦") 
    print(f"   - nDCG@k åæ˜ æ’åºè´¨é‡")
    print(f"   - è¿™äº›æŒ‡æ ‡çš„ç»„åˆå¯ä»¥æŒ‡å¯¼å…·ä½“çš„æ”¹è¿›æ–¹å‘")

def main():
    """ä¸»å‡½æ•°"""
    # æŸ¥æ‰¾æœ€æ–°çš„è¯„ä¼°ç»“æœæ–‡ä»¶
    evaluation_dir = Path("evaluation")
    
    if not evaluation_dir.exists():
        print("âŒ evaluation ç›®å½•ä¸å­˜åœ¨")
        return
    
    # æŸ¥æ‰¾å®Œæ•´çš„è¯„ä¼°ç»“æœæ–‡ä»¶
    result_files = list(evaluation_dir.glob("rag_vs_llm_full_results_*.json"))
    
    if not result_files:
        print("âŒ æœªæ‰¾åˆ°è¯„ä¼°ç»“æœæ–‡ä»¶")
        print("ğŸ’¡ è¯·å…ˆè¿è¡Œ: python rag_vs_llm_evaluation.py --mode quick")
        return
    
    # ä½¿ç”¨æœ€æ–°çš„æ–‡ä»¶
    latest_file = max(result_files, key=lambda x: x.stat().st_mtime)
    
    print(f"ğŸ“ æ‰¾åˆ°è¯„ä¼°ç»“æœæ–‡ä»¶: {latest_file.name}")
    
    # åˆ†ææ–‡ä»¶
    analyze_specific_evaluation_file(str(latest_file))

if __name__ == '__main__':
    main()