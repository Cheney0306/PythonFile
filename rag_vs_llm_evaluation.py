# rag_vs_llm_evaluation.py - RAGç³»ç»Ÿä¸çº¯LLMå¯¹æ¯”è¯„ä¼°

import json
import random
import numpy as np
from typing import List, Dict, Tuple, Optional
from pathlib import Path
from tqdm import tqdm
import math
from collections import defaultdict
from datetime import datetime
import argparse

# å¯¼å…¥æˆ‘ä»¬çš„ç³»ç»Ÿ
from enhanced_retrieval_engine import EnhancedRetrievalEngine
import config

class RAGvsLLMEvaluator:
    """RAGç³»ç»Ÿä¸çº¯LLMå¯¹æ¯”è¯„ä¼°å™¨"""
    
    def __init__(self):
        self.enhanced_engine = EnhancedRetrievalEngine()
        self.openai_api_key = config.OPENAI_API_KEY
        
        # æ£€æŸ¥APIå¯†é’¥
        if not self.openai_api_key or self.openai_api_key == "your-openai-api-key-here":
            print("âš ï¸ è­¦å‘Š: OpenAI APIå¯†é’¥æœªè®¾ç½®ï¼Œå°†æ— æ³•è¿›è¡ŒLLMè¯„ä¼°")
    
    def load_qa_dataset(self, dataset_path: str = "qa_datasets", limit: int = None, scan_all: bool = False) -> List[Dict]:
        """åŠ è½½QAæ•°æ®é›†"""
        qa_files = list(Path(dataset_path).glob("*.json"))
        
        if not qa_files:
            print(f"âŒ åœ¨ {dataset_path} ä¸­æœªæ‰¾åˆ°QAæ•°æ®é›†æ–‡ä»¶")
            return []
        
        all_questions = []
        
        print(f"ğŸ“ æ‰¾åˆ° {len(qa_files)} ä¸ªQAæ•°æ®é›†æ–‡ä»¶:")
        for qa_file in qa_files:
            print(f"   - {qa_file.name}")
        
        for qa_file in qa_files:
            try:
                with open(qa_file, 'r', encoding='utf-8') as f:
                    qa_data = json.load(f)
                
                file_questions = 0
                for qa_item in qa_data:
                    if 'question' in qa_item and 'answer' in qa_item:
                        all_questions.append({
                            'question': qa_item['question'],
                            'expected_answer': qa_item['answer'],
                            'question_type': qa_item.get('question_type', 'unknown'),
                            'source_text': qa_item.get('source_text', ''),
                            'triple': qa_item.get('triple'),
                            'schema': qa_item.get('schema'),
                            'source_file': qa_file.name
                        })
                        file_questions += 1
                
                print(f"   âœ… {qa_file.name}: {file_questions} ä¸ªé—®é¢˜")
                        
            except Exception as e:
                print(f"âš  åŠ è½½æ–‡ä»¶ {qa_file} æ—¶å‡ºé”™: {e}")
        
        print(f"ğŸ“Š æ€»è®¡åŠ è½½: {len(all_questions)} ä¸ªé—®é¢˜")
        
        # æ ¹æ®å‚æ•°å†³å®šæ˜¯å¦é™åˆ¶æ•°é‡
        if scan_all:
            print(f"ğŸ” æ‰«æå…¨éƒ¨æ¨¡å¼: ä½¿ç”¨å…¨éƒ¨ {len(all_questions)} ä¸ªé—®é¢˜")
            final_questions = all_questions
        else:
            if limit and len(all_questions) > limit:
                final_questions = random.sample(all_questions, limit)
                print(f"ğŸ² éšæœºé‡‡æ ·: ä» {len(all_questions)} ä¸ªé—®é¢˜ä¸­é€‰æ‹© {len(final_questions)} ä¸ª")
            else:
                final_questions = all_questions
                print(f"âœ… ä½¿ç”¨å…¨éƒ¨ {len(final_questions)} ä¸ªé—®é¢˜")
        
        print(f"âœ… æœ€ç»ˆè¯„ä¼°é—®é¢˜æ•°: {len(final_questions)}")
        return final_questions
    
    def call_pure_llm(self, question: str) -> str:
        """è°ƒç”¨çº¯LLMè·å–ç­”æ¡ˆ"""
        try:
            from openai import OpenAI
            client = OpenAI(api_key=self.openai_api_key)
            
            # æ„é€ ä¸¥æ ¼çš„promptï¼Œè¦æ±‚LLMåªå›ç­”é—®é¢˜ï¼Œä¸è¦å¤šè¯´
            prompt = f"""You are a helpful assistant. Answer the following question directly and concisely. 

IMPORTANT INSTRUCTIONS:
- Provide ONLY the direct answer to the question
- Do NOT add explanations, context, or additional information
- Do NOT say "The answer is..." or similar phrases
- Keep your response as brief as possible
- If you don't know the answer, just say "Unknown"

Question: {question}

Answer:"""
            
            response = client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[
                    {
                        "role": "system",
                        "content": "You are a helpful assistant that provides direct, concise answers without additional explanations."
                    },
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                temperature=0.1,  # ä½æ¸©åº¦ç¡®ä¿ä¸€è‡´æ€§
                max_tokens=50     # é™åˆ¶è¾“å‡ºé•¿åº¦
            )
            
            answer = response.choices[0].message.content.strip()
            return answer
            
        except Exception as e:
            print(f"âš  LLMè°ƒç”¨å¤±è´¥: {e}")
            return "Error: LLM call failed"
    
    def evaluate_answer_similarity(self, predicted: str, expected: str) -> Dict[str, float]:
        """è¯„ä¼°ç­”æ¡ˆç›¸ä¼¼åº¦"""
        predicted = predicted.lower().strip()
        expected = expected.lower().strip()
        
        # 1. ç²¾ç¡®åŒ¹é…
        exact_match = 1.0 if predicted == expected else 0.0
        
        # 2. åŒ…å«åŒ¹é…
        contains_match = 1.0 if expected in predicted or predicted in expected else 0.0
        
        # 3. è¯æ±‡é‡å åº¦
        pred_words = set(predicted.split())
        exp_words = set(expected.split())
        
        if len(exp_words) == 0:
            word_overlap = 0.0
        else:
            word_overlap = len(pred_words.intersection(exp_words)) / len(exp_words)
        
        # 4. ç»¼åˆåˆ†æ•° (åŠ æƒå¹³å‡)
        composite_score = (exact_match * 0.5 + contains_match * 0.3 + word_overlap * 0.2)
        
        return {
            'exact_match': exact_match,
            'contains_match': contains_match,
            'word_overlap': word_overlap,
            'composite_score': composite_score
        }
    
    def evaluate_single_question(self, qa_item: Dict) -> Dict:
        """è¯„ä¼°å•ä¸ªé—®é¢˜"""
        question = qa_item['question']
        expected_answer = qa_item['expected_answer']
        
        # 1. è·å–RAGç³»ç»Ÿç­”æ¡ˆ
        try:
            rag_result = self.enhanced_engine.retrieve_and_rewrite(question)
            rag_answer = rag_result.get('final_answer', 'No answer')
        except Exception as e:
            rag_answer = f"Error: {e}"
        
        # 2. è·å–çº¯LLMç­”æ¡ˆ
        llm_answer = self.call_pure_llm(question)
        
        # 3. è¯„ä¼°ä¸¤ä¸ªç­”æ¡ˆ
        rag_scores = self.evaluate_answer_similarity(rag_answer, expected_answer)
        llm_scores = self.evaluate_answer_similarity(llm_answer, expected_answer)
        
        return {
            'question': question,
            'expected_answer': expected_answer,
            'rag_answer': rag_answer,
            'llm_answer': llm_answer,
            'rag_scores': rag_scores,
            'llm_scores': llm_scores,
            'question_type': qa_item.get('question_type', 'unknown'),
            'source_file': qa_item.get('source_file', 'unknown')
        }
    
    def evaluate_dataset(self, questions: List[Dict]) -> Dict:
        """è¯„ä¼°æ•´ä¸ªæ•°æ®é›†"""
        print(f"ğŸ”„ å¼€å§‹è¯„ä¼° {len(questions)} ä¸ªé—®é¢˜...")
        print("âš ï¸ æ³¨æ„: åŒ…å«LLMè°ƒç”¨ï¼Œå¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´")
        
        results = []
        
        for qa_item in tqdm(questions, desc="è¯„ä¼°è¿›åº¦"):
            result = self.evaluate_single_question(qa_item)
            results.append(result)
        
        # è®¡ç®—æ±‡æ€»ç»Ÿè®¡
        summary = self.calculate_summary_statistics(results)
        
        return {
            'results': results,
            'summary': summary,
            'total_questions': len(questions),
            'timestamp': datetime.now().isoformat(),
            'evaluation_config': {
                'embedding_model': config.EMBEDDING_MODEL,
                'llm_model': 'gpt-3.5-turbo'
            }
        }
    
    def calculate_summary_statistics(self, results: List[Dict]) -> Dict:
        """è®¡ç®—æ±‡æ€»ç»Ÿè®¡"""
        rag_metrics = defaultdict(list)
        llm_metrics = defaultdict(list)
        
        # æŒ‰é—®é¢˜ç±»å‹åˆ†ç»„
        by_type = defaultdict(lambda: {'rag': defaultdict(list), 'llm': defaultdict(list)})
        
        for result in results:
            q_type = result['question_type']
            
            # æ”¶é›†RAGæŒ‡æ ‡
            for metric, score in result['rag_scores'].items():
                rag_metrics[metric].append(score)
                by_type[q_type]['rag'][metric].append(score)
            
            # æ”¶é›†LLMæŒ‡æ ‡
            for metric, score in result['llm_scores'].items():
                llm_metrics[metric].append(score)
                by_type[q_type]['llm'][metric].append(score)
        
        # è®¡ç®—æ€»ä½“ç»Ÿè®¡
        def calc_stats(values):
            if not values:
                return {'mean': 0.0, 'std': 0.0}
            return {
                'mean': np.mean(values),
                'std': np.std(values)
            }
        
        summary = {
            'overall': {
                'rag': {metric: calc_stats(scores) for metric, scores in rag_metrics.items()},
                'llm': {metric: calc_stats(scores) for metric, scores in llm_metrics.items()}
            },
            'by_question_type': {}
        }
        
        # æŒ‰é—®é¢˜ç±»å‹ç»Ÿè®¡
        for q_type, type_data in by_type.items():
            summary['by_question_type'][q_type] = {
                'rag': {metric: calc_stats(scores) for metric, scores in type_data['rag'].items()},
                'llm': {metric: calc_stats(scores) for metric, scores in type_data['llm'].items()}
            }
        
        return summary
    
    def save_evaluation_results(self, results: Dict, output_dir: str = "evaluation"):
        """ä¿å­˜è¯„ä¼°ç»“æœ"""
        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ä¿å­˜å®Œæ•´ç»“æœ
        full_results_file = output_path / f"rag_vs_llm_full_results_{timestamp}.json"
        with open(full_results_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜æ±‡æ€»ç»“æœ
        summary_file = output_path / f"rag_vs_llm_summary_{timestamp}.json"
        summary_data = {
            'timestamp': results['timestamp'],
            'total_questions': results['total_questions'],
            'evaluation_config': results['evaluation_config'],
            'summary': results['summary']
        }
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary_data, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜é—®ç­”å¯¹æ¯”
        self._save_qa_comparison(results, output_path, timestamp)
        
        # ç”ŸæˆMarkdownæŠ¥å‘Š
        self._generate_markdown_report(results, output_path, timestamp)
        
        print(f"ğŸ’¾ è¯„ä¼°ç»“æœå·²ä¿å­˜åˆ°ç›®å½•: {output_dir}")
        print(f"ğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:")
        print(f"   - å®Œæ•´ç»“æœ: {full_results_file.name}")
        print(f"   - æ±‡æ€»ç»“æœ: {summary_file.name}")
        print(f"   - é—®ç­”å¯¹æ¯”: rag_vs_llm_qa_comparison_{timestamp}.json")
        print(f"   - é—®ç­”å¯¹æ¯”(æ˜“è¯»): rag_vs_llm_qa_comparison_{timestamp}.txt")
        print(f"   - é—®ç­”å¯¹æ¯”(CSV): rag_vs_llm_qa_comparison_{timestamp}.csv")
        print(f"   - MarkdownæŠ¥å‘Š: rag_vs_llm_report_{timestamp}.md")
    
    def _save_qa_comparison(self, results: Dict, output_path: Path, timestamp: str):
        """
        ä¿å­˜RAGç³»ç»Ÿä¸LLMçš„é—®ç­”å¯¹æ¯”
        
        Args:
            results: å®Œæ•´çš„è¯„ä¼°ç»“æœ
            output_path: è¾“å‡ºè·¯å¾„
            timestamp: æ—¶é—´æˆ³
        """
        # 1. ä¿å­˜JSONæ ¼å¼ï¼ˆä¾¿äºç¨‹åºå¤„ç†ï¼‰
        qa_comparison_file = output_path / f"rag_vs_llm_qa_comparison_{timestamp}.json"
        
        qa_comparisons = []
        
        for result in results.get('results', []):
            qa_comparison = {
                'question': result['question'],
                'expected_answer': result['expected_answer'],
                'question_type': result.get('question_type', 'unknown'),
                'rag_system': {
                    'answer': result['rag_answer'],
                    'scores': {
                        'exact_match': result['rag_scores']['exact_match'],
                        'contains_match': result['rag_scores']['contains_match'],
                        'word_overlap': result['rag_scores']['word_overlap'],
                        'composite_score': result['rag_scores']['composite_score']
                    }
                },
                'llm_system': {
                    'answer': result['llm_answer'],
                    'scores': {
                        'exact_match': result['llm_scores']['exact_match'],
                        'contains_match': result['llm_scores']['contains_match'],
                        'word_overlap': result['llm_scores']['word_overlap'],
                        'composite_score': result['llm_scores']['composite_score']
                    }
                },
                'winner': self._determine_winner(result['rag_scores'], result['llm_scores']),
                'score_difference': {
                    'exact_match': result['rag_scores']['exact_match'] - result['llm_scores']['exact_match'],
                    'contains_match': result['rag_scores']['contains_match'] - result['llm_scores']['contains_match'],
                    'word_overlap': result['rag_scores']['word_overlap'] - result['llm_scores']['word_overlap'],
                    'composite_score': result['rag_scores']['composite_score'] - result['llm_scores']['composite_score']
                }
            }
            qa_comparisons.append(qa_comparison)
        
        # ä¿å­˜JSONæ ¼å¼
        with open(qa_comparison_file, 'w', encoding='utf-8') as f:
            json.dump(qa_comparisons, f, ensure_ascii=False, indent=2)
        
        # 2. ä¿å­˜æ˜“è¯»çš„æ–‡æœ¬æ ¼å¼
        txt_comparison_file = output_path / f"rag_vs_llm_qa_comparison_{timestamp}.txt"
        with open(txt_comparison_file, 'w', encoding='utf-8') as f:
            f.write("RAGç³»ç»Ÿ vs çº¯LLM é—®ç­”å¯¹æ¯”æŠ¥å‘Š\n")
            f.write("=" * 80 + "\n\n")
            f.write(f"ç”Ÿæˆæ—¶é—´: {results.get('timestamp', timestamp)}\n")
            f.write(f"æ€»é—®é¢˜æ•°: {results.get('total_questions', len(qa_comparisons))}\n\n")
            
            # ç»Ÿè®¡èƒœè´Ÿæƒ…å†µ
            rag_wins = sum(1 for qa in qa_comparisons if qa['winner'] == 'RAG')
            llm_wins = sum(1 for qa in qa_comparisons if qa['winner'] == 'LLM')
            ties = sum(1 for qa in qa_comparisons if qa['winner'] == 'TIE')
            
            total_questions = len(qa_comparisons)
            
            f.write(f"èƒœè´Ÿç»Ÿè®¡:\n")
            if total_questions > 0:
                f.write(f"  RAGç³»ç»Ÿè·èƒœ: {rag_wins} æ¬¡ ({rag_wins/total_questions*100:.1f}%)\n")
                f.write(f"  çº¯LLMè·èƒœ: {llm_wins} æ¬¡ ({llm_wins/total_questions*100:.1f}%)\n")
                f.write(f"  å¹³å±€: {ties} æ¬¡ ({ties/total_questions*100:.1f}%)\n\n")
            else:
                f.write(f"  RAGç³»ç»Ÿè·èƒœ: {rag_wins} æ¬¡ (0.0%)\n")
                f.write(f"  çº¯LLMè·èƒœ: {llm_wins} æ¬¡ (0.0%)\n")
                f.write(f"  å¹³å±€: {ties} æ¬¡ (0.0%)\n\n")
            
            for i, qa in enumerate(qa_comparisons, 1):
                f.write(f"é—®é¢˜ {i}:\n")
                f.write("-" * 40 + "\n")
                f.write(f"é—®é¢˜: {qa['question']}\n")
                f.write(f"æœŸæœ›ç­”æ¡ˆ: {qa['expected_answer']}\n")
                f.write(f"é—®é¢˜ç±»å‹: {qa['question_type']}\n\n")
                
                f.write("RAGç³»ç»Ÿ:\n")
                f.write(f"  ç­”æ¡ˆ: {qa['rag_system']['answer']}\n")
                f.write(f"  ç²¾ç¡®åŒ¹é…: {qa['rag_system']['scores']['exact_match']:.3f}\n")
                f.write(f"  åŒ…å«åŒ¹é…: {qa['rag_system']['scores']['contains_match']:.3f}\n")
                f.write(f"  è¯æ±‡é‡å : {qa['rag_system']['scores']['word_overlap']:.3f}\n")
                f.write(f"  ç»¼åˆåˆ†æ•°: {qa['rag_system']['scores']['composite_score']:.3f}\n\n")
                
                f.write("çº¯LLM:\n")
                f.write(f"  ç­”æ¡ˆ: {qa['llm_system']['answer']}\n")
                f.write(f"  ç²¾ç¡®åŒ¹é…: {qa['llm_system']['scores']['exact_match']:.3f}\n")
                f.write(f"  åŒ…å«åŒ¹é…: {qa['llm_system']['scores']['contains_match']:.3f}\n")
                f.write(f"  è¯æ±‡é‡å : {qa['llm_system']['scores']['word_overlap']:.3f}\n")
                f.write(f"  ç»¼åˆåˆ†æ•°: {qa['llm_system']['scores']['composite_score']:.3f}\n\n")
                
                f.write(f"èƒœè´Ÿç»“æœ: {qa['winner']}\n")
                f.write(f"åˆ†æ•°å·®å¼‚ (RAG - LLM):\n")
                f.write(f"  ç²¾ç¡®åŒ¹é…: {qa['score_difference']['exact_match']:+.3f}\n")
                f.write(f"  åŒ…å«åŒ¹é…: {qa['score_difference']['contains_match']:+.3f}\n")
                f.write(f"  è¯æ±‡é‡å : {qa['score_difference']['word_overlap']:+.3f}\n")
                f.write(f"  ç»¼åˆåˆ†æ•°: {qa['score_difference']['composite_score']:+.3f}\n")
                
                f.write("\n" + "=" * 80 + "\n\n")
        
        # 3. ä¿å­˜CSVæ ¼å¼ï¼ˆä¾¿äºExcelæŸ¥çœ‹ï¼‰
        csv_comparison_file = output_path / f"rag_vs_llm_qa_comparison_{timestamp}.csv"
        import csv
        
        with open(csv_comparison_file, 'w', newline='', encoding='utf-8-sig') as f:
            writer = csv.writer(f)
            
            # å†™å…¥æ ‡é¢˜è¡Œ
            writer.writerow([
                'é—®é¢˜', 'æœŸæœ›ç­”æ¡ˆ', 'é—®é¢˜ç±»å‹',
                'RAGç­”æ¡ˆ', 'LLMç­”æ¡ˆ', 'èƒœè´Ÿç»“æœ',
                'RAGç²¾ç¡®åŒ¹é…', 'LLMç²¾ç¡®åŒ¹é…', 'ç²¾ç¡®åŒ¹é…å·®å¼‚',
                'RAGåŒ…å«åŒ¹é…', 'LLMåŒ…å«åŒ¹é…', 'åŒ…å«åŒ¹é…å·®å¼‚',
                'RAGè¯æ±‡é‡å ', 'LLMè¯æ±‡é‡å ', 'è¯æ±‡é‡å å·®å¼‚',
                'RAGç»¼åˆåˆ†æ•°', 'LLMç»¼åˆåˆ†æ•°', 'ç»¼åˆåˆ†æ•°å·®å¼‚'
            ])
            
            # å†™å…¥æ•°æ®è¡Œ
            for qa in qa_comparisons:
                writer.writerow([
                    qa['question'],
                    qa['expected_answer'],
                    qa['question_type'],
                    qa['rag_system']['answer'],
                    qa['llm_system']['answer'],
                    qa['winner'],
                    f"{qa['rag_system']['scores']['exact_match']:.3f}",
                    f"{qa['llm_system']['scores']['exact_match']:.3f}",
                    f"{qa['score_difference']['exact_match']:+.3f}",
                    f"{qa['rag_system']['scores']['contains_match']:.3f}",
                    f"{qa['llm_system']['scores']['contains_match']:.3f}",
                    f"{qa['score_difference']['contains_match']:+.3f}",
                    f"{qa['rag_system']['scores']['word_overlap']:.3f}",
                    f"{qa['llm_system']['scores']['word_overlap']:.3f}",
                    f"{qa['score_difference']['word_overlap']:+.3f}",
                    f"{qa['rag_system']['scores']['composite_score']:.3f}",
                    f"{qa['llm_system']['scores']['composite_score']:.3f}",
                    f"{qa['score_difference']['composite_score']:+.3f}"
                ])
    
    def _determine_winner(self, rag_scores: Dict, llm_scores: Dict) -> str:
        """æ ¹æ®ç»¼åˆåˆ†æ•°åˆ¤æ–­èƒœè´Ÿ"""
        rag_composite = rag_scores['composite_score']
        llm_composite = llm_scores['composite_score']
        
        if rag_composite > llm_composite:
            return 'RAG'
        elif llm_composite > rag_composite:
            return 'LLM'
        else:
            return 'TIE'

    def _generate_markdown_report(self, results: Dict, output_path: Path, timestamp: str):
        """ç”ŸæˆMarkdownæŠ¥å‘Š"""
        report_file = output_path / f"rag_vs_llm_report_{timestamp}.md"
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(f"# RAGç³»ç»Ÿ vs çº¯LLM è¯„ä¼°æŠ¥å‘Š\n\n")
            f.write(f"**ç”Ÿæˆæ—¶é—´**: {results['timestamp']}\n")
            f.write(f"**è¯„ä¼°é—®é¢˜æ•°**: {results['total_questions']}\n")
            f.write(f"**RAGæ¨¡å‹**: {results['evaluation_config']['embedding_model']}\n")
            f.write(f"**LLMæ¨¡å‹**: {results['evaluation_config']['llm_model']}\n\n")
            
            # æ€»ä½“æ€§èƒ½å¯¹æ¯”
            f.write("## ğŸ“Š æ€»ä½“æ€§èƒ½å¯¹æ¯”\n\n")
            
            overall = results['summary']['overall']
            
            f.write("| æŒ‡æ ‡ | RAGç³»ç»Ÿ | çº¯LLM | RAGä¼˜åŠ¿ |\n")
            f.write("|------|---------|-------|--------|\n")
            
            for metric in ['exact_match', 'contains_match', 'word_overlap', 'composite_score']:
                rag_score = overall['rag'][metric]['mean']
                llm_score = overall['llm'][metric]['mean']
                improvement = ((rag_score - llm_score) / llm_score * 100) if llm_score > 0 else 0
                
                f.write(f"| {metric} | {rag_score:.4f} | {llm_score:.4f} | {improvement:+.1f}% |\n")
            
            # æŒ‰é—®é¢˜ç±»å‹åˆ†æ
            f.write("\n## ğŸ“‹ æŒ‰é—®é¢˜ç±»å‹åˆ†æ\n\n")
            
            for q_type, type_data in results['summary']['by_question_type'].items():
                f.write(f"### {q_type.upper()} ç±»å‹é—®é¢˜\n\n")
                
                f.write("| æŒ‡æ ‡ | RAGç³»ç»Ÿ | çº¯LLM | RAGä¼˜åŠ¿ |\n")
                f.write("|------|---------|-------|--------|\n")
                
                for metric in ['exact_match', 'contains_match', 'word_overlap', 'composite_score']:
                    rag_score = type_data['rag'][metric]['mean']
                    llm_score = type_data['llm'][metric]['mean']
                    improvement = ((rag_score - llm_score) / llm_score * 100) if llm_score > 0 else 0
                    
                    f.write(f"| {metric} | {rag_score:.4f} | {llm_score:.4f} | {improvement:+.1f}% |\n")
                
                f.write("\n")
            
            # ç¤ºä¾‹å¯¹æ¯”
            f.write("## ğŸ“ ç­”æ¡ˆç¤ºä¾‹å¯¹æ¯”\n\n")
            
            # é€‰æ‹©å‡ ä¸ªæœ‰ä»£è¡¨æ€§çš„ä¾‹å­
            sample_results = results['results'][:5]
            
            for i, result in enumerate(sample_results, 1):
                f.write(f"### ç¤ºä¾‹ {i}\n\n")
                f.write(f"**é—®é¢˜**: {result['question']}\n\n")
                f.write(f"**æ ‡å‡†ç­”æ¡ˆ**: {result['expected_answer']}\n\n")
                f.write(f"**RAGç­”æ¡ˆ**: {result['rag_answer']}\n\n")
                f.write(f"**LLMç­”æ¡ˆ**: {result['llm_answer']}\n\n")
                f.write(f"**RAGç»¼åˆåˆ†æ•°**: {result['rag_scores']['composite_score']:.4f}\n\n")
                f.write(f"**LLMç»¼åˆåˆ†æ•°**: {result['llm_scores']['composite_score']:.4f}\n\n")
                f.write("---\n\n")
    
    def print_summary_report(self, results: Dict):
        """æ‰“å°æ±‡æ€»æŠ¥å‘Š"""
        print(f"\nğŸ“Š RAG vs LLM è¯„ä¼°æŠ¥å‘Š (å…± {results['total_questions']} ä¸ªé—®é¢˜)")
        print("=" * 80)
        
        overall = results['summary']['overall']
        
        print(f"\nğŸ” æ€»ä½“æ€§èƒ½å¯¹æ¯”:")
        print("-" * 40)
        
        metrics_names = {
            'exact_match': 'ç²¾ç¡®åŒ¹é…',
            'contains_match': 'åŒ…å«åŒ¹é…', 
            'word_overlap': 'è¯æ±‡é‡å ',
            'composite_score': 'ç»¼åˆåˆ†æ•°'
        }
        
        for metric, name in metrics_names.items():
            rag_score = overall['rag'][metric]['mean']
            rag_std = overall['rag'][metric]['std']
            llm_score = overall['llm'][metric]['mean']
            llm_std = overall['llm'][metric]['std']
            
            improvement = ((rag_score - llm_score) / llm_score * 100) if llm_score > 0 else 0
            
            print(f"\n  {name}:")
            print(f"    RAGç³»ç»Ÿ: {rag_score:.4f} (Â±{rag_std:.4f})")
            print(f"    çº¯LLM:   {llm_score:.4f} (Â±{llm_std:.4f})")
            print(f"    RAGä¼˜åŠ¿: {improvement:+.1f}%")
        
        # æŒ‰é—®é¢˜ç±»å‹æ˜¾ç¤ºæœ€ä½³è¡¨ç°
        print(f"\nğŸ“‹ æŒ‰é—®é¢˜ç±»å‹è¡¨ç° (ç»¼åˆåˆ†æ•°):")
        print("-" * 40)
        
        for q_type, type_data in results['summary']['by_question_type'].items():
            rag_score = type_data['rag']['composite_score']['mean']
            llm_score = type_data['llm']['composite_score']['mean']
            improvement = ((rag_score - llm_score) / llm_score * 100) if llm_score > 0 else 0
            
            print(f"  {q_type.upper()}: RAG {rag_score:.4f} vs LLM {llm_score:.4f} ({improvement:+.1f}%)")


def run_quick_rag_vs_llm_evaluation(sample_size: int = 20):
    """è¿è¡Œå¿«é€ŸRAG vs LLMè¯„ä¼°"""
    print("âš¡ å¯åŠ¨å¿«é€ŸRAG vs LLMè¯„ä¼°")
    print(f"ğŸ“Š æ ·æœ¬æ•°é‡: {sample_size}")
    print("=" * 50)
    
    evaluator = RAGvsLLMEvaluator()
    
    # åŠ è½½å°‘é‡æ•°æ®è¿›è¡Œå¿«é€Ÿæµ‹è¯•
    questions = evaluator.load_qa_dataset(limit=sample_size)
    
    if not questions:
        print("âŒ æ— æ³•åŠ è½½è¯„ä¼°æ•°æ®")
        return
    
    # è¿è¡Œè¯„ä¼°
    results = evaluator.evaluate_dataset(questions)
    
    # ä¿å­˜ç»“æœ
    evaluator.save_evaluation_results(results)
    
    # æ‰“å°ç®€åŒ–æŠ¥å‘Š
    evaluator.print_summary_report(results)
    
    print(f"\nâœ… å¿«é€ŸRAG vs LLMè¯„ä¼°å®Œæˆï¼")


def run_full_rag_vs_llm_evaluation(dataset_path: str = "qa_datasets"):
    """è¿è¡Œå®Œæ•´RAG vs LLMè¯„ä¼°"""
    print("ğŸ” å¯åŠ¨å®Œæ•´RAG vs LLMè¯„ä¼°")
    print(f"ğŸ“ æ•°æ®é›†è·¯å¾„: {dataset_path}")
    print("=" * 50)
    
    evaluator = RAGvsLLMEvaluator()
    
    # åŠ è½½å…¨éƒ¨æ•°æ®
    questions = evaluator.load_qa_dataset(dataset_path=dataset_path, scan_all=True)
    
    if not questions:
        print("âŒ æ— æ³•åŠ è½½è¯„ä¼°æ•°æ®")
        return
    
    print(f"\nğŸš€ å¼€å§‹è¯„ä¼° {len(questions)} ä¸ªé—®é¢˜...")
    print("âš ï¸ æ³¨æ„: å®Œæ•´è¯„ä¼°åŒ…å«å¤§é‡LLMè°ƒç”¨ï¼Œå¯èƒ½éœ€è¦å¾ˆé•¿æ—¶é—´å’Œè¾ƒé«˜è´¹ç”¨")
    
    user_input = input("æ˜¯å¦ç»§ç»­ï¼Ÿ(y/n): ")
    if user_input.lower() != 'y':
        print("âŒ ç”¨æˆ·å–æ¶ˆè¯„ä¼°")
        return
    
    # è¿è¡Œè¯„ä¼°
    results = evaluator.evaluate_dataset(questions)
    
    # ä¿å­˜ç»“æœ
    evaluator.save_evaluation_results(results)
    
    # æ‰“å°è¯¦ç»†æŠ¥å‘Š
    evaluator.print_summary_report(results)
    
    print(f"\nâœ… å®Œæ•´RAG vs LLMè¯„ä¼°å®Œæˆï¼")


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description="RAGç³»ç»Ÿä¸çº¯LLMå¯¹æ¯”è¯„ä¼°")
    parser.add_argument('--mode', choices=['quick', 'full'], default='quick',
                       help='è¯„ä¼°æ¨¡å¼: quick(å¿«é€Ÿè¯„ä¼°), full(å®Œæ•´è¯„ä¼°)')
    parser.add_argument('--sample-size', type=int, default=20,
                       help='å¿«é€Ÿè¯„ä¼°çš„æ ·æœ¬æ•°é‡')
    parser.add_argument('--qa-path', type=str, default="qa_datasets",
                       help='QAæ•°æ®é›†è·¯å¾„')
    
    args = parser.parse_args()
    
    if args.mode == 'quick':
        run_quick_rag_vs_llm_evaluation(args.sample_size)
    elif args.mode == 'full':
        run_full_rag_vs_llm_evaluation(args.qa_path)