# rag_vs_llm_evaluation.py - RAGç³»ç»Ÿä¸çº¯LLMå¯¹æ¯”è¯„ä¼°

import json
import random
import numpy as np
from typing import List, Dict, Tuple, Optional
from pathlib import Path
from tqdm import tqdm
import math
from collections import defaultdict
from datetime import datetime
import argparse

# å¯¼å…¥æˆ‘ä»¬çš„ç³»ç»Ÿ
from enhanced_retrieval_engine import EnhancedRetrievalEngine
import config

class RAGvsLLMEvaluator:
    """RAGç³»ç»Ÿä¸çº¯LLMå¯¹æ¯”è¯„ä¼°å™¨"""
    
    def __init__(self):
        self.enhanced_engine = EnhancedRetrievalEngine()
        self.openai_api_key = config.OPENAI_API_KEY
        
        # æ£€æŸ¥APIå¯†é’¥
        if not self.openai_api_key or self.openai_api_key == "your-openai-api-key-here":
            print("âš ï¸ è­¦å‘Š: OpenAI APIå¯†é’¥æœªè®¾ç½®ï¼Œå°†æ— æ³•è¿›è¡ŒLLMè¯„ä¼°")
    
    def load_qa_dataset(self, dataset_path: str = "qa_datasets", limit: int = None, scan_all: bool = False) -> List[Dict]:
        """åŠ è½½QAæ•°æ®é›†"""
        qa_files = list(Path(dataset_path).glob("*.json"))
        
        if not qa_files:
            print(f"âŒ åœ¨ {dataset_path} ä¸­æœªæ‰¾åˆ°QAæ•°æ®é›†æ–‡ä»¶")
            return []
        
        all_questions = []
        
        print(f"ğŸ“ æ‰¾åˆ° {len(qa_files)} ä¸ªQAæ•°æ®é›†æ–‡ä»¶:")
        for qa_file in qa_files:
            print(f"   - {qa_file.name}")
        
        for qa_file in qa_files:
            try:
                with open(qa_file, 'r', encoding='utf-8') as f:
                    qa_data = json.load(f)
                
                file_questions = 0
                for qa_item in qa_data:
                    if 'question' in qa_item and 'answer' in qa_item:
                        all_questions.append({
                            'question': qa_item['question'],
                            'expected_answer': qa_item['answer'],
                            'question_type': qa_item.get('question_type', 'unknown'),
                            'source_text': qa_item.get('source_text', ''),
                            'triple': qa_item.get('triple'),
                            'schema': qa_item.get('schema'),
                            'source_file': qa_file.name
                        })
                        file_questions += 1
                
                print(f"   âœ… {qa_file.name}: {file_questions} ä¸ªé—®é¢˜")
                        
            except Exception as e:
                print(f"âš  åŠ è½½æ–‡ä»¶ {qa_file} æ—¶å‡ºé”™: {e}")
        
        print(f"ğŸ“Š æ€»è®¡åŠ è½½: {len(all_questions)} ä¸ªé—®é¢˜")
        
        # æ ¹æ®å‚æ•°å†³å®šæ˜¯å¦é™åˆ¶æ•°é‡
        if scan_all:
            print(f"ğŸ” æ‰«æå…¨éƒ¨æ¨¡å¼: ä½¿ç”¨å…¨éƒ¨ {len(all_questions)} ä¸ªé—®é¢˜")
            final_questions = all_questions
        else:
            if limit and len(all_questions) > limit:
                final_questions = random.sample(all_questions, limit)
                print(f"ğŸ² éšæœºé‡‡æ ·: ä» {len(all_questions)} ä¸ªé—®é¢˜ä¸­é€‰æ‹© {len(final_questions)} ä¸ª")
            else:
                final_questions = all_questions
                print(f"âœ… ä½¿ç”¨å…¨éƒ¨ {len(final_questions)} ä¸ªé—®é¢˜")
        
        print(f"âœ… æœ€ç»ˆè¯„ä¼°é—®é¢˜æ•°: {len(final_questions)}")
        return final_questions
    
    def call_pure_llm(self, question: str) -> str:
        """è°ƒç”¨çº¯LLMè·å–ç­”æ¡ˆ"""
        try:
            from openai import OpenAI
            client = OpenAI(api_key=self.openai_api_key)
            
            # æ„é€ ä¸¥æ ¼çš„promptï¼Œè¦æ±‚LLMåªå›ç­”é—®é¢˜ï¼Œä¸è¦å¤šè¯´
            prompt = f"""You are a helpful assistant. Answer the following question directly and concisely. 

IMPORTANT INSTRUCTIONS:
- Provide ONLY the direct answer to the question
- Do NOT add explanations, context, or additional information
- Do NOT say "The answer is..." or similar phrases
- Keep your response as brief as possible
- If you don't know the answer, just say "Unknown"

Question: {question}

Answer:"""
            
            response = client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[
                    {
                        "role": "system",
                        "content": "You are a helpful assistant that provides direct, concise answers without additional explanations."
                    },
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                temperature=0.1,  # ä½æ¸©åº¦ç¡®ä¿ä¸€è‡´æ€§
                max_tokens=50     # é™åˆ¶è¾“å‡ºé•¿åº¦
            )
            
            answer = response.choices[0].message.content.strip()
            return answer
            
        except Exception as e:
            print(f"âš  LLMè°ƒç”¨å¤±è´¥: {e}")
            return "Error: LLM call failed"
    
    def calculate_retrieval_metrics(self, retrieved_items: List[Dict], expected_answer: str, 
                                  original_triple: List[str] = None, k_values: List[int] = [1, 3, 5]) -> Dict:
        """
        è®¡ç®—æ£€ç´¢æŒ‡æ ‡: Precision@k, Recall@k, nDCG@k
        
        Args:
            retrieved_items: æ£€ç´¢åˆ°çš„é¡¹ç›®åˆ—è¡¨
            expected_answer: æœŸæœ›ç­”æ¡ˆ
            original_triple: ç”Ÿæˆé—®é¢˜æ‰€ç”¨çš„åŸå§‹ä¸‰å…ƒç»„
            k_values: è¦è®¡ç®—çš„kå€¼åˆ—è¡¨
        """
        if not retrieved_items:
            return {f'precision@{k}': 0.0 for k in k_values} | \
                   {f'recall@{k}': 0.0 for k in k_values} | \
                   {f'ndcg@{k}': 0.0 for k in k_values}
        
        # è®¡ç®—æ¯ä¸ªæ£€ç´¢é¡¹çš„ç›¸å…³æ€§åˆ†æ•°
        relevance_scores = []
        
        for item in retrieved_items:
            if original_triple and 'triple' in item:
                # åŸºäºä¸‰å…ƒç»„çš„ç›¸å…³æ€§åˆ¤æ–­
                retrieved_triple = item['triple']
                relevance, is_relevant = self._calculate_triple_relevance(retrieved_triple, original_triple)
            else:
                # å›é€€åˆ°åŸºäºè¯æ±‡é‡å çš„ç›¸å…³æ€§åˆ¤æ–­
                relevance, is_relevant = self._calculate_text_relevance(item, expected_answer)
            
            relevance_scores.append((relevance, is_relevant))
        
        metrics = {}
        
        for k in k_values:
            k = min(k, len(retrieved_items))  # ç¡®ä¿kä¸è¶…è¿‡æ£€ç´¢é¡¹æ•°é‡
            
            # Precision@k
            relevant_at_k = sum(score[1] for score in relevance_scores[:k])
            precision_k = relevant_at_k / k if k > 0 else 0.0
            metrics[f'precision@{k}'] = precision_k
            
            # Recall@k (å‡è®¾æ€»ç›¸å…³æ–‡æ¡£æ•°ä¸º1ï¼Œå³æœŸæœ›ç­”æ¡ˆ)
            total_relevant = 1  # ç®€åŒ–å‡è®¾
            recall_k = min(relevant_at_k / total_relevant, 1.0) if total_relevant > 0 else 0.0
            metrics[f'recall@{k}'] = recall_k
            
            # nDCG@k
            dcg_k = 0.0
            for i in range(k):
                relevance = relevance_scores[i][0]
                dcg_k += relevance / math.log2(i + 2)  # i+2 because log2(1) = 0
            
            # ç†æƒ³DCG (å‡è®¾æœ€ä½³æ’åº)
            ideal_relevances = sorted([score[0] for score in relevance_scores], reverse=True)[:k]
            idcg_k = 0.0
            for i, relevance in enumerate(ideal_relevances):
                idcg_k += relevance / math.log2(i + 2)
            
            ndcg_k = dcg_k / idcg_k if idcg_k > 0 else 0.0
            metrics[f'ndcg@{k}'] = ndcg_k
        
        return metrics
    
    def _calculate_triple_relevance(self, retrieved_triple: List[str], original_triple: List[str]) -> Tuple[float, int]:
        """
        åŸºäºä¸‰å…ƒç»„è®¡ç®—ç›¸å…³æ€§åˆ†æ•°
        
        Args:
            retrieved_triple: æ£€ç´¢åˆ°çš„ä¸‰å…ƒç»„
            original_triple: åŸå§‹ä¸‰å…ƒç»„
            
        Returns:
            (relevance_score, is_relevant): ç›¸å…³æ€§åˆ†æ•°å’ŒäºŒè¿›åˆ¶ç›¸å…³æ€§
        """
        if not retrieved_triple or not original_triple:
            return 0.0, 0
        
        # ç¡®ä¿ä¸¤ä¸ªä¸‰å…ƒç»„éƒ½æœ‰3ä¸ªå…ƒç´ 
        if len(retrieved_triple) != 3 or len(original_triple) != 3:
            return 0.0, 0
        
        # è®¡ç®—åŒ¹é…çš„å…ƒç´ æ•°é‡
        matches = 0
        for i in range(3):
            if retrieved_triple[i] == original_triple[i]:
                matches += 1
        
        # ç›¸å…³æ€§åˆ¤æ–­é€»è¾‘:
        # - 3ä¸ªå…ƒç´ å®Œå…¨åŒ¹é…: æœ€ç›¸å…³ (relevance=1.0, is_relevant=1)
        # - 2ä¸ªå…ƒç´ åŒ¹é…: éƒ¨åˆ†ç›¸å…³ (relevance=0.6, is_relevant=1)  
        # - 1ä¸ªæˆ–0ä¸ªå…ƒç´ åŒ¹é…: ä¸ç›¸å…³ (relevance=0.0, is_relevant=0)
        
        if matches == 3:
            return 1.0, 1  # å®Œå…¨ç›¸å…³
        elif matches == 2:
            return 0.6, 1  # éƒ¨åˆ†ç›¸å…³
        else:
            return 0.0, 0  # ä¸ç›¸å…³
    
    def _calculate_text_relevance(self, item: Dict, expected_answer: str) -> Tuple[float, int]:
        """
        åŸºäºæ–‡æœ¬é‡å è®¡ç®—ç›¸å…³æ€§åˆ†æ•° (å›é€€æ–¹æ³•)
        
        Args:
            item: æ£€ç´¢é¡¹
            expected_answer: æœŸæœ›ç­”æ¡ˆ
            
        Returns:
            (relevance_score, is_relevant): ç›¸å…³æ€§åˆ†æ•°å’ŒäºŒè¿›åˆ¶ç›¸å…³æ€§
        """
        expected_words = set(expected_answer.lower().split())
        
        # ä»æ£€ç´¢é¡¹ä¸­æå–æ–‡æœ¬å†…å®¹
        item_text = ""
        if 'text' in item and item['text']:
            item_text = item['text']
        elif 'document' in item and item['document']:
            item_text = item['document']
        elif 'triple' in item:
            # ä»ä¸‰å…ƒç»„æ„å»ºæ–‡æœ¬
            triple = item['triple']
            item_text = f"{triple[0]} {triple[1]} {triple[2]}"
        
        # è®¡ç®—ç›¸å…³æ€§åˆ†æ•° (åŸºäºè¯æ±‡é‡å )
        item_words = set(item_text.lower().split())
        if len(expected_words) > 0:
            overlap = len(item_words.intersection(expected_words))
            relevance = overlap / len(expected_words)
        else:
            relevance = 0.0
        
        # è½¬æ¢ä¸ºäºŒè¿›åˆ¶ç›¸å…³æ€§ (é˜ˆå€¼ä¸º0.1)
        is_relevant = 1 if relevance > 0.1 else 0
        return relevance, is_relevant

    def evaluate_answer_similarity(self, predicted: str, expected: str) -> Dict[str, float]:
        """è¯„ä¼°ç­”æ¡ˆç›¸ä¼¼åº¦"""
        predicted = predicted.lower().strip()
        expected = expected.lower().strip()
        
        # 1. ç²¾ç¡®åŒ¹é…
        exact_match = 1.0 if predicted == expected else 0.0
        
        # 2. åŒ…å«åŒ¹é…
        contains_match = 1.0 if expected in predicted or predicted in expected else 0.0
        
        # 3. è¯æ±‡é‡å åº¦
        pred_words = set(predicted.split())
        exp_words = set(expected.split())
        
        if len(exp_words) == 0:
            word_overlap = 0.0
        else:
            word_overlap = len(pred_words.intersection(exp_words)) / len(exp_words)
        
        # 4. ç»¼åˆåˆ†æ•° (åŠ æƒå¹³å‡)
        composite_score = (exact_match * 0.5 + contains_match * 0.3 + word_overlap * 0.2)
        
        return {
            'exact_match': exact_match,
            'contains_match': contains_match,
            'word_overlap': word_overlap,
            'composite_score': composite_score
        }
    
    def evaluate_single_question(self, qa_item: Dict) -> Dict:
        """è¯„ä¼°å•ä¸ªé—®é¢˜"""
        question = qa_item['question']
        expected_answer = qa_item['expected_answer']
        
        # 1. è·å–RAGç³»ç»Ÿç­”æ¡ˆå’Œæ£€ç´¢è¯¦æƒ…
        rag_retrieval_metrics = {}
        try:
            rag_result = self.enhanced_engine.retrieve_and_rewrite(question)
            rag_answer = rag_result.get('final_answer', 'No answer')
            
            # è®¡ç®—æ£€ç´¢æŒ‡æ ‡
            retrieved_items = rag_result.get('retrieved_items', [])
            if retrieved_items:
                # è·å–åŸå§‹ä¸‰å…ƒç»„ä¿¡æ¯
                original_triple = qa_item.get('triple', None)
                rag_retrieval_metrics = self.calculate_retrieval_metrics(
                    retrieved_items, expected_answer, original_triple, k_values=[1, 3, 5]
                )
            
        except Exception as e:
            rag_answer = f"Error: {e}"
            rag_retrieval_metrics = {
                'precision@1': 0.0, 'precision@3': 0.0, 'precision@5': 0.0,
                'recall@1': 0.0, 'recall@3': 0.0, 'recall@5': 0.0,
                'ndcg@1': 0.0, 'ndcg@3': 0.0, 'ndcg@5': 0.0
            }
        
        # 2. è·å–çº¯LLMç­”æ¡ˆ
        llm_answer = self.call_pure_llm(question)
        
        # 3. è¯„ä¼°ä¸¤ä¸ªç­”æ¡ˆ
        rag_scores = self.evaluate_answer_similarity(rag_answer, expected_answer)
        llm_scores = self.evaluate_answer_similarity(llm_answer, expected_answer)
        
        return {
            'question': question,
            'expected_answer': expected_answer,
            'rag_answer': rag_answer,
            'llm_answer': llm_answer,
            'rag_scores': rag_scores,
            'llm_scores': llm_scores,
            'rag_retrieval_metrics': rag_retrieval_metrics,
            'question_type': qa_item.get('question_type', 'unknown'),
            'source_file': qa_item.get('source_file', 'unknown')
        }
    
    def evaluate_dataset(self, questions: List[Dict]) -> Dict:
        """è¯„ä¼°æ•´ä¸ªæ•°æ®é›†"""
        print(f"ğŸ”„ å¼€å§‹è¯„ä¼° {len(questions)} ä¸ªé—®é¢˜...")
        print("âš ï¸ æ³¨æ„: åŒ…å«LLMè°ƒç”¨ï¼Œå¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´")
        
        results = []
        
        for qa_item in tqdm(questions, desc="è¯„ä¼°è¿›åº¦"):
            result = self.evaluate_single_question(qa_item)
            results.append(result)
        
        # è®¡ç®—æ±‡æ€»ç»Ÿè®¡
        summary = self.calculate_summary_statistics(results)
        
        return {
            'results': results,
            'summary': summary,
            'total_questions': len(questions),
            'timestamp': datetime.now().isoformat(),
            'evaluation_config': {
                'embedding_model': config.EMBEDDING_MODEL,
                'llm_model': 'gpt-3.5-turbo'
            }
        }
    
    def calculate_summary_statistics(self, results: List[Dict]) -> Dict:
        """è®¡ç®—æ±‡æ€»ç»Ÿè®¡"""
        rag_metrics = defaultdict(list)
        llm_metrics = defaultdict(list)
        rag_retrieval_metrics = defaultdict(list)
        
        # æŒ‰é—®é¢˜ç±»å‹åˆ†ç»„
        by_type = defaultdict(lambda: {
            'rag': defaultdict(list), 
            'llm': defaultdict(list),
            'rag_retrieval': defaultdict(list)
        })
        
        for result in results:
            q_type = result['question_type']
            
            # æ”¶é›†RAGç­”æ¡ˆè´¨é‡æŒ‡æ ‡
            for metric, score in result['rag_scores'].items():
                rag_metrics[metric].append(score)
                by_type[q_type]['rag'][metric].append(score)
            
            # æ”¶é›†RAGæ£€ç´¢æŒ‡æ ‡
            for metric, score in result.get('rag_retrieval_metrics', {}).items():
                rag_retrieval_metrics[metric].append(score)
                by_type[q_type]['rag_retrieval'][metric].append(score)
            
            # æ”¶é›†LLMæŒ‡æ ‡
            for metric, score in result['llm_scores'].items():
                llm_metrics[metric].append(score)
                by_type[q_type]['llm'][metric].append(score)
        
        # è®¡ç®—æ€»ä½“ç»Ÿè®¡
        def calc_stats(values):
            if not values:
                return {'mean': 0.0, 'std': 0.0}
            return {
                'mean': np.mean(values),
                'std': np.std(values)
            }
        
        summary = {
            'overall': {
                'rag': {metric: calc_stats(scores) for metric, scores in rag_metrics.items()},
                'llm': {metric: calc_stats(scores) for metric, scores in llm_metrics.items()},
                'rag_retrieval': {metric: calc_stats(scores) for metric, scores in rag_retrieval_metrics.items()}
            },
            'by_question_type': {}
        }
        
        # æŒ‰é—®é¢˜ç±»å‹ç»Ÿè®¡
        for q_type, type_data in by_type.items():
            summary['by_question_type'][q_type] = {
                'rag': {metric: calc_stats(scores) for metric, scores in type_data['rag'].items()},
                'llm': {metric: calc_stats(scores) for metric, scores in type_data['llm'].items()},
                'rag_retrieval': {metric: calc_stats(scores) for metric, scores in type_data['rag_retrieval'].items()}
            }
        
        return summary
    
    def save_evaluation_results(self, results: Dict, output_dir: str = "evaluation"):
        """ä¿å­˜è¯„ä¼°ç»“æœ"""
        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ä¿å­˜å®Œæ•´ç»“æœ
        full_results_file = output_path / f"rag_vs_llm_full_results_{timestamp}.json"
        with open(full_results_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜æ±‡æ€»ç»“æœ
        summary_file = output_path / f"rag_vs_llm_summary_{timestamp}.json"
        summary_data = {
            'timestamp': results['timestamp'],
            'total_questions': results['total_questions'],
            'evaluation_config': results['evaluation_config'],
            'summary': results['summary']
        }
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary_data, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜é—®ç­”å¯¹æ¯”
        self._save_qa_comparison(results, output_path, timestamp)
        
        # ä¿å­˜ç®€åŒ–çš„é—®ç­”è®°å½• (æ¯è¡Œä¸€ä¸ªé—®é¢˜)
        self._save_simple_qa_records(results, output_path, timestamp)
        
        # ç”ŸæˆMarkdownæŠ¥å‘Š
        self._generate_markdown_report(results, output_path, timestamp)
        
        print(f"ğŸ’¾ è¯„ä¼°ç»“æœå·²ä¿å­˜åˆ°ç›®å½•: {output_dir}")
        print(f"ğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:")
        print(f"   - å®Œæ•´ç»“æœ: {full_results_file.name}")
        print(f"   - æ±‡æ€»ç»“æœ: {summary_file.name}")
        print(f"   - é—®ç­”å¯¹æ¯”: rag_vs_llm_qa_comparison_{timestamp}.json")
        print(f"   - é—®ç­”å¯¹æ¯”(æ˜“è¯»): rag_vs_llm_qa_comparison_{timestamp}.txt")
        print(f"   - é—®ç­”å¯¹æ¯”(CSV): rag_vs_llm_qa_comparison_{timestamp}.csv")
        print(f"   - ç®€åŒ–é—®ç­”è®°å½•: simple_qa_records_{timestamp}.jsonl")
        print(f"   - MarkdownæŠ¥å‘Š: rag_vs_llm_report_{timestamp}.md")
    
    def _save_qa_comparison(self, results: Dict, output_path: Path, timestamp: str):
        """
        ä¿å­˜RAGç³»ç»Ÿä¸LLMçš„é—®ç­”å¯¹æ¯”
        
        Args:
            results: å®Œæ•´çš„è¯„ä¼°ç»“æœ
            output_path: è¾“å‡ºè·¯å¾„
            timestamp: æ—¶é—´æˆ³
        """
        # 1. ä¿å­˜JSONæ ¼å¼ï¼ˆä¾¿äºç¨‹åºå¤„ç†ï¼‰
        qa_comparison_file = output_path / f"rag_vs_llm_qa_comparison_{timestamp}.json"
        
        qa_comparisons = []
        
        for result in results.get('results', []):
            qa_comparison = {
                'question': result['question'],
                'expected_answer': result['expected_answer'],
                'question_type': result.get('question_type', 'unknown'),
                'rag_system': {
                    'answer': result['rag_answer'],
                    'scores': {
                        'exact_match': result['rag_scores']['exact_match'],
                        'contains_match': result['rag_scores']['contains_match'],
                        'word_overlap': result['rag_scores']['word_overlap'],
                        'composite_score': result['rag_scores']['composite_score']
                    }
                },
                'llm_system': {
                    'answer': result['llm_answer'],
                    'scores': {
                        'exact_match': result['llm_scores']['exact_match'],
                        'contains_match': result['llm_scores']['contains_match'],
                        'word_overlap': result['llm_scores']['word_overlap'],
                        'composite_score': result['llm_scores']['composite_score']
                    }
                },
                'winner': self._determine_winner(result['rag_scores'], result['llm_scores']),
                'score_difference': {
                    'exact_match': result['rag_scores']['exact_match'] - result['llm_scores']['exact_match'],
                    'contains_match': result['rag_scores']['contains_match'] - result['llm_scores']['contains_match'],
                    'word_overlap': result['rag_scores']['word_overlap'] - result['llm_scores']['word_overlap'],
                    'composite_score': result['rag_scores']['composite_score'] - result['llm_scores']['composite_score']
                }
            }
            qa_comparisons.append(qa_comparison)
        
        # ä¿å­˜JSONæ ¼å¼
        with open(qa_comparison_file, 'w', encoding='utf-8') as f:
            json.dump(qa_comparisons, f, ensure_ascii=False, indent=2)
        
        # 2. ä¿å­˜æ˜“è¯»çš„æ–‡æœ¬æ ¼å¼
        txt_comparison_file = output_path / f"rag_vs_llm_qa_comparison_{timestamp}.txt"
        with open(txt_comparison_file, 'w', encoding='utf-8') as f:
            f.write("RAGç³»ç»Ÿ vs çº¯LLM é—®ç­”å¯¹æ¯”æŠ¥å‘Š\n")
            f.write("=" * 80 + "\n\n")
            f.write(f"ç”Ÿæˆæ—¶é—´: {results.get('timestamp', timestamp)}\n")
            f.write(f"æ€»é—®é¢˜æ•°: {results.get('total_questions', len(qa_comparisons))}\n\n")
            
            # ç»Ÿè®¡èƒœè´Ÿæƒ…å†µ
            rag_wins = sum(1 for qa in qa_comparisons if qa['winner'] == 'RAG')
            llm_wins = sum(1 for qa in qa_comparisons if qa['winner'] == 'LLM')
            ties = sum(1 for qa in qa_comparisons if qa['winner'] == 'TIE')
            
            total_questions = len(qa_comparisons)
            
            f.write(f"èƒœè´Ÿç»Ÿè®¡:\n")
            if total_questions > 0:
                f.write(f"  RAGç³»ç»Ÿè·èƒœ: {rag_wins} æ¬¡ ({rag_wins/total_questions*100:.1f}%)\n")
                f.write(f"  çº¯LLMè·èƒœ: {llm_wins} æ¬¡ ({llm_wins/total_questions*100:.1f}%)\n")
                f.write(f"  å¹³å±€: {ties} æ¬¡ ({ties/total_questions*100:.1f}%)\n\n")
            else:
                f.write(f"  RAGç³»ç»Ÿè·èƒœ: {rag_wins} æ¬¡ (0.0%)\n")
                f.write(f"  çº¯LLMè·èƒœ: {llm_wins} æ¬¡ (0.0%)\n")
                f.write(f"  å¹³å±€: {ties} æ¬¡ (0.0%)\n\n")
            
            for i, qa in enumerate(qa_comparisons, 1):
                f.write(f"é—®é¢˜ {i}:\n")
                f.write("-" * 40 + "\n")
                f.write(f"é—®é¢˜: {qa['question']}\n")
                f.write(f"æœŸæœ›ç­”æ¡ˆ: {qa['expected_answer']}\n")
                f.write(f"é—®é¢˜ç±»å‹: {qa['question_type']}\n\n")
                
                f.write("RAGç³»ç»Ÿ:\n")
                f.write(f"  ç­”æ¡ˆ: {qa['rag_system']['answer']}\n")
                f.write(f"  ç²¾ç¡®åŒ¹é…: {qa['rag_system']['scores']['exact_match']:.3f}\n")
                f.write(f"  åŒ…å«åŒ¹é…: {qa['rag_system']['scores']['contains_match']:.3f}\n")
                f.write(f"  è¯æ±‡é‡å : {qa['rag_system']['scores']['word_overlap']:.3f}\n")
                f.write(f"  ç»¼åˆåˆ†æ•°: {qa['rag_system']['scores']['composite_score']:.3f}\n\n")
                
                f.write("çº¯LLM:\n")
                f.write(f"  ç­”æ¡ˆ: {qa['llm_system']['answer']}\n")
                f.write(f"  ç²¾ç¡®åŒ¹é…: {qa['llm_system']['scores']['exact_match']:.3f}\n")
                f.write(f"  åŒ…å«åŒ¹é…: {qa['llm_system']['scores']['contains_match']:.3f}\n")
                f.write(f"  è¯æ±‡é‡å : {qa['llm_system']['scores']['word_overlap']:.3f}\n")
                f.write(f"  ç»¼åˆåˆ†æ•°: {qa['llm_system']['scores']['composite_score']:.3f}\n\n")
                
                f.write(f"èƒœè´Ÿç»“æœ: {qa['winner']}\n")
                f.write(f"åˆ†æ•°å·®å¼‚ (RAG - LLM):\n")
                f.write(f"  ç²¾ç¡®åŒ¹é…: {qa['score_difference']['exact_match']:+.3f}\n")
                f.write(f"  åŒ…å«åŒ¹é…: {qa['score_difference']['contains_match']:+.3f}\n")
                f.write(f"  è¯æ±‡é‡å : {qa['score_difference']['word_overlap']:+.3f}\n")
                f.write(f"  ç»¼åˆåˆ†æ•°: {qa['score_difference']['composite_score']:+.3f}\n")
                
                f.write("\n" + "=" * 80 + "\n\n")
        
        # 3. ä¿å­˜CSVæ ¼å¼ï¼ˆä¾¿äºExcelæŸ¥çœ‹ï¼‰
        csv_comparison_file = output_path / f"rag_vs_llm_qa_comparison_{timestamp}.csv"
        import csv
        
        with open(csv_comparison_file, 'w', newline='', encoding='utf-8-sig') as f:
            writer = csv.writer(f)
            
            # å†™å…¥æ ‡é¢˜è¡Œ
            writer.writerow([
                'é—®é¢˜', 'æœŸæœ›ç­”æ¡ˆ', 'é—®é¢˜ç±»å‹',
                'RAGç­”æ¡ˆ', 'LLMç­”æ¡ˆ', 'èƒœè´Ÿç»“æœ',
                'RAGç²¾ç¡®åŒ¹é…', 'LLMç²¾ç¡®åŒ¹é…', 'ç²¾ç¡®åŒ¹é…å·®å¼‚',
                'RAGåŒ…å«åŒ¹é…', 'LLMåŒ…å«åŒ¹é…', 'åŒ…å«åŒ¹é…å·®å¼‚',
                'RAGè¯æ±‡é‡å ', 'LLMè¯æ±‡é‡å ', 'è¯æ±‡é‡å å·®å¼‚',
                'RAGç»¼åˆåˆ†æ•°', 'LLMç»¼åˆåˆ†æ•°', 'ç»¼åˆåˆ†æ•°å·®å¼‚'
            ])
            
            # å†™å…¥æ•°æ®è¡Œ
            for qa in qa_comparisons:
                writer.writerow([
                    qa['question'],
                    qa['expected_answer'],
                    qa['question_type'],
                    qa['rag_system']['answer'],
                    qa['llm_system']['answer'],
                    qa['winner'],
                    f"{qa['rag_system']['scores']['exact_match']:.3f}",
                    f"{qa['llm_system']['scores']['exact_match']:.3f}",
                    f"{qa['score_difference']['exact_match']:+.3f}",
                    f"{qa['rag_system']['scores']['contains_match']:.3f}",
                    f"{qa['llm_system']['scores']['contains_match']:.3f}",
                    f"{qa['score_difference']['contains_match']:+.3f}",
                    f"{qa['rag_system']['scores']['word_overlap']:.3f}",
                    f"{qa['llm_system']['scores']['word_overlap']:.3f}",
                    f"{qa['score_difference']['word_overlap']:+.3f}",
                    f"{qa['rag_system']['scores']['composite_score']:.3f}",
                    f"{qa['llm_system']['scores']['composite_score']:.3f}",
                    f"{qa['score_difference']['composite_score']:+.3f}"
                ])
    
    def _determine_winner(self, rag_scores: Dict, llm_scores: Dict) -> str:
        """æ ¹æ®ç»¼åˆåˆ†æ•°åˆ¤æ–­èƒœè´Ÿ"""
        rag_composite = rag_scores['composite_score']
        llm_composite = llm_scores['composite_score']
        
        if rag_composite > llm_composite:
            return 'RAG'
        elif llm_composite > rag_composite:
            return 'LLM'
        else:
            return 'TIE'

    def _save_simple_qa_records(self, results: Dict, output_path: Path, timestamp: str):
        """
        ä¿å­˜ç®€åŒ–çš„é—®ç­”è®°å½•ï¼Œæ¯è¡Œä¸€ä¸ªJSONå¯¹è±¡
        æ ¼å¼: {"question": "...", "expected_answer": "...", "rag_answer": "...", "llm_answer": "..."}
        """
        simple_records_file = output_path / f"simple_qa_records_{timestamp}.jsonl"
        
        with open(simple_records_file, 'w', encoding='utf-8') as f:
            for result in results.get('results', []):
                record = {
                    "question": result['question'],
                    "expected_answer": result['expected_answer'],
                    "rag_answer": result['rag_answer'],
                    "llm_answer": result['llm_answer']
                }
                # å†™å…¥ä¸€è¡ŒJSON
                f.write(json.dumps(record, ensure_ascii=False) + '\n')

    def _generate_markdown_report(self, results: Dict, output_path: Path, timestamp: str):
        """ç”ŸæˆMarkdownæŠ¥å‘Š"""
        report_file = output_path / f"rag_vs_llm_report_{timestamp}.md"
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(f"# RAGç³»ç»Ÿ vs çº¯LLM è¯„ä¼°æŠ¥å‘Š\n\n")
            f.write(f"**ç”Ÿæˆæ—¶é—´**: {results['timestamp']}\n")
            f.write(f"**è¯„ä¼°é—®é¢˜æ•°**: {results['total_questions']}\n")
            f.write(f"**RAGæ¨¡å‹**: {results['evaluation_config']['embedding_model']}\n")
            f.write(f"**LLMæ¨¡å‹**: {results['evaluation_config']['llm_model']}\n\n")
            
            # æ€»ä½“æ€§èƒ½å¯¹æ¯”
            f.write("## ğŸ“Š æ€»ä½“æ€§èƒ½å¯¹æ¯”\n\n")
            
            overall = results['summary']['overall']
            
            f.write("### ç­”æ¡ˆè´¨é‡å¯¹æ¯”\n\n")
            f.write("| æŒ‡æ ‡ | RAGç³»ç»Ÿ | çº¯LLM | RAGä¼˜åŠ¿ |\n")
            f.write("|------|---------|-------|--------|\n")
            
            for metric in ['exact_match', 'contains_match', 'word_overlap', 'composite_score']:
                rag_score = overall['rag'][metric]['mean']
                llm_score = overall['llm'][metric]['mean']
                improvement = ((rag_score - llm_score) / llm_score * 100) if llm_score > 0 else 0
                
                f.write(f"| {metric} | {rag_score:.4f} | {llm_score:.4f} | {improvement:+.1f}% |\n")
            
            # RAGæ£€ç´¢æ€§èƒ½æŒ‡æ ‡
            if 'rag_retrieval' in overall and overall['rag_retrieval']:
                f.write("\n### RAGæ£€ç´¢æ€§èƒ½æŒ‡æ ‡\n\n")
                f.write("| æŒ‡æ ‡ | å¹³å‡å€¼ | æ ‡å‡†å·® |\n")
                f.write("|------|--------|--------|\n")
                
                retrieval_metrics = ['precision@1', 'precision@3', 'precision@5', 
                                   'recall@1', 'recall@3', 'recall@5',
                                   'ndcg@1', 'ndcg@3', 'ndcg@5']
                
                for metric in retrieval_metrics:
                    if metric in overall['rag_retrieval']:
                        mean_score = overall['rag_retrieval'][metric]['mean']
                        std_score = overall['rag_retrieval'][metric]['std']
                        f.write(f"| {metric} | {mean_score:.4f} | {std_score:.4f} |\n")
            
            # æŒ‰é—®é¢˜ç±»å‹åˆ†æ
            f.write("\n## ğŸ“‹ æŒ‰é—®é¢˜ç±»å‹åˆ†æ\n\n")
            
            for q_type, type_data in results['summary']['by_question_type'].items():
                f.write(f"### {q_type.upper()} ç±»å‹é—®é¢˜\n\n")
                
                f.write("| æŒ‡æ ‡ | RAGç³»ç»Ÿ | çº¯LLM | RAGä¼˜åŠ¿ |\n")
                f.write("|------|---------|-------|--------|\n")
                
                for metric in ['exact_match', 'contains_match', 'word_overlap', 'composite_score']:
                    rag_score = type_data['rag'][metric]['mean']
                    llm_score = type_data['llm'][metric]['mean']
                    improvement = ((rag_score - llm_score) / llm_score * 100) if llm_score > 0 else 0
                    
                    f.write(f"| {metric} | {rag_score:.4f} | {llm_score:.4f} | {improvement:+.1f}% |\n")
                
                f.write("\n")
            
            # ç¤ºä¾‹å¯¹æ¯”
            f.write("## ğŸ“ ç­”æ¡ˆç¤ºä¾‹å¯¹æ¯”\n\n")
            
            # é€‰æ‹©å‡ ä¸ªæœ‰ä»£è¡¨æ€§çš„ä¾‹å­
            sample_results = results['results'][:5]
            
            for i, result in enumerate(sample_results, 1):
                f.write(f"### ç¤ºä¾‹ {i}\n\n")
                f.write(f"**é—®é¢˜**: {result['question']}\n\n")
                f.write(f"**æ ‡å‡†ç­”æ¡ˆ**: {result['expected_answer']}\n\n")
                f.write(f"**RAGç­”æ¡ˆ**: {result['rag_answer']}\n\n")
                f.write(f"**LLMç­”æ¡ˆ**: {result['llm_answer']}\n\n")
                f.write(f"**RAGç»¼åˆåˆ†æ•°**: {result['rag_scores']['composite_score']:.4f}\n\n")
                f.write(f"**LLMç»¼åˆåˆ†æ•°**: {result['llm_scores']['composite_score']:.4f}\n\n")
                f.write("---\n\n")
    
    def print_summary_report(self, results: Dict):
        """æ‰“å°æ±‡æ€»æŠ¥å‘Š"""
        print(f"\nğŸ“Š RAG vs LLM è¯„ä¼°æŠ¥å‘Š (å…± {results['total_questions']} ä¸ªé—®é¢˜)")
        print("=" * 80)
        
        overall = results['summary']['overall']
        
        print(f"\nğŸ” æ€»ä½“æ€§èƒ½å¯¹æ¯”:")
        print("-" * 40)
        
        metrics_names = {
            'exact_match': 'ç²¾ç¡®åŒ¹é…',
            'contains_match': 'åŒ…å«åŒ¹é…', 
            'word_overlap': 'è¯æ±‡é‡å ',
            'composite_score': 'ç»¼åˆåˆ†æ•°'
        }
        
        for metric, name in metrics_names.items():
            rag_score = overall['rag'][metric]['mean']
            rag_std = overall['rag'][metric]['std']
            llm_score = overall['llm'][metric]['mean']
            llm_std = overall['llm'][metric]['std']
            
            improvement = ((rag_score - llm_score) / llm_score * 100) if llm_score > 0 else 0
            
            print(f"\n  {name}:")
            print(f"    RAGç³»ç»Ÿ: {rag_score:.4f} (Â±{rag_std:.4f})")
            print(f"    çº¯LLM:   {llm_score:.4f} (Â±{llm_std:.4f})")
            print(f"    RAGä¼˜åŠ¿: {improvement:+.1f}%")
        
        # æ˜¾ç¤ºRAGæ£€ç´¢æŒ‡æ ‡
        if 'rag_retrieval' in overall and overall['rag_retrieval']:
            print(f"\nğŸ¯ RAGæ£€ç´¢æ€§èƒ½æŒ‡æ ‡:")
            print("-" * 40)
            
            retrieval_metrics_names = {
                'precision@1': 'Precision@1',
                'precision@3': 'Precision@3',
                'precision@5': 'Precision@5',
                'recall@1': 'Recall@1',
                'recall@3': 'Recall@3',
                'recall@5': 'Recall@5',
                'ndcg@1': 'nDCG@1',
                'ndcg@3': 'nDCG@3',
                'ndcg@5': 'nDCG@5'
            }
            
            for metric, name in retrieval_metrics_names.items():
                if metric in overall['rag_retrieval']:
                    score = overall['rag_retrieval'][metric]['mean']
                    std = overall['rag_retrieval'][metric]['std']
                    print(f"  {name}: {score:.4f} (Â±{std:.4f})")
        
        # æŒ‰é—®é¢˜ç±»å‹æ˜¾ç¤ºæœ€ä½³è¡¨ç°
        print(f"\nğŸ“‹ æŒ‰é—®é¢˜ç±»å‹è¡¨ç° (ç»¼åˆåˆ†æ•°):")
        print("-" * 40)
        
        for q_type, type_data in results['summary']['by_question_type'].items():
            rag_score = type_data['rag']['composite_score']['mean']
            llm_score = type_data['llm']['composite_score']['mean']
            improvement = ((rag_score - llm_score) / llm_score * 100) if llm_score > 0 else 0
            
            print(f"  {q_type.upper()}: RAG {rag_score:.4f} vs LLM {llm_score:.4f} ({improvement:+.1f}%)")


def run_quick_rag_vs_llm_evaluation(sample_size: int = 20):
    """è¿è¡Œå¿«é€ŸRAG vs LLMè¯„ä¼°"""
    print("âš¡ å¯åŠ¨å¿«é€ŸRAG vs LLMè¯„ä¼°")
    print(f"ğŸ“Š æ ·æœ¬æ•°é‡: {sample_size}")
    print("=" * 50)
    
    evaluator = RAGvsLLMEvaluator()
    
    # åŠ è½½å°‘é‡æ•°æ®è¿›è¡Œå¿«é€Ÿæµ‹è¯•
    questions = evaluator.load_qa_dataset(limit=sample_size)
    
    if not questions:
        print("âŒ æ— æ³•åŠ è½½è¯„ä¼°æ•°æ®")
        return
    
    # è¿è¡Œè¯„ä¼°
    results = evaluator.evaluate_dataset(questions)
    
    # ä¿å­˜ç»“æœ
    evaluator.save_evaluation_results(results)
    
    # æ‰“å°ç®€åŒ–æŠ¥å‘Š
    evaluator.print_summary_report(results)
    
    print(f"\nâœ… å¿«é€ŸRAG vs LLMè¯„ä¼°å®Œæˆï¼")


def run_full_rag_vs_llm_evaluation(dataset_path: str = "qa_datasets"):
    """è¿è¡Œå®Œæ•´RAG vs LLMè¯„ä¼°"""
    print("ğŸ” å¯åŠ¨å®Œæ•´RAG vs LLMè¯„ä¼°")
    print(f"ğŸ“ æ•°æ®é›†è·¯å¾„: {dataset_path}")
    print("=" * 50)
    
    evaluator = RAGvsLLMEvaluator()
    
    # åŠ è½½å…¨éƒ¨æ•°æ®
    questions = evaluator.load_qa_dataset(dataset_path=dataset_path, scan_all=True)
    
    if not questions:
        print("âŒ æ— æ³•åŠ è½½è¯„ä¼°æ•°æ®")
        return
    
    print(f"\nğŸš€ å¼€å§‹è¯„ä¼° {len(questions)} ä¸ªé—®é¢˜...")
    print("âš ï¸ æ³¨æ„: å®Œæ•´è¯„ä¼°åŒ…å«å¤§é‡LLMè°ƒç”¨ï¼Œå¯èƒ½éœ€è¦å¾ˆé•¿æ—¶é—´å’Œè¾ƒé«˜è´¹ç”¨")
    
    user_input = input("æ˜¯å¦ç»§ç»­ï¼Ÿ(y/n): ")
    if user_input.lower() != 'y':
        print("âŒ ç”¨æˆ·å–æ¶ˆè¯„ä¼°")
        return
    
    # è¿è¡Œè¯„ä¼°
    results = evaluator.evaluate_dataset(questions)
    
    # ä¿å­˜ç»“æœ
    evaluator.save_evaluation_results(results)
    
    # æ‰“å°è¯¦ç»†æŠ¥å‘Š
    evaluator.print_summary_report(results)
    
    print(f"\nâœ… å®Œæ•´RAG vs LLMè¯„ä¼°å®Œæˆï¼")


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description="RAGç³»ç»Ÿä¸çº¯LLMå¯¹æ¯”è¯„ä¼°")
    parser.add_argument('--mode', choices=['quick', 'full'], default='quick',
                       help='è¯„ä¼°æ¨¡å¼: quick(å¿«é€Ÿè¯„ä¼°), full(å®Œæ•´è¯„ä¼°)')
    parser.add_argument('--sample-size', type=int, default=20,
                       help='å¿«é€Ÿè¯„ä¼°çš„æ ·æœ¬æ•°é‡')
    parser.add_argument('--qa-path', type=str, default="qa_datasets",
                       help='QAæ•°æ®é›†è·¯å¾„')
    
    args = parser.parse_args()
    
    if args.mode == 'quick':
        run_quick_rag_vs_llm_evaluation(args.sample_size)
    elif args.mode == 'full':
        run_full_rag_vs_llm_evaluation(args.qa_path)