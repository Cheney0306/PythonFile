# enhanced_embedding_system.py - å¢å¼ºçš„åµŒå…¥ç³»ç»Ÿ

import chromadb
from typing import List, Dict, Optional, Tuple
from tqdm import tqdm
import config
from data_loader import KnowledgeDataLoader
from embedding_client import EmbeddingClient
import numpy as np
from collections import defaultdict

# æ–°å¢ï¼šå¯¼å…¥sentence-transformersçš„CrossEncoder
try:
    from sentence_transformers import CrossEncoder
    CROSS_ENCODER_AVAILABLE = True
except ImportError:
    print("âš ï¸ sentence-transformersæœªå®‰è£…ï¼Œå°†ä½¿ç”¨åŸæœ‰çš„é‡æ’æ–¹æ³•")
    CROSS_ENCODER_AVAILABLE = False

class EnhancedVectorDatabaseManager:
    """å¢å¼ºçš„å‘é‡æ•°æ®åº“ç®¡ç†å™¨ - å®ç°æ›´å¥½çš„åµŒå…¥ç­–ç•¥å’Œå¤šé˜¶æ®µæ£€ç´¢"""
    
    def __init__(self):
        self.client = chromadb.PersistentClient(path=config.CHROMA_DB_PATH)
        self.collection = None
        self.embedding_client = EmbeddingClient()
        
        # ã€æ–°ã€‘åˆå§‹åŒ–Cross-Encoderé‡æ’æ¨¡å‹
        self.rerank_model = None
        if CROSS_ENCODER_AVAILABLE:
            try:
                print("ğŸ”„ åŠ è½½Cross-Encoderé‡æ’æ¨¡å‹...")
                self.rerank_model = CrossEncoder('BAAI/bge-reranker-base', max_length=512)
                print("âœ… Cross-Encoderæ¨¡å‹åŠ è½½æˆåŠŸ")
            except Exception as e:
                print(f"âš ï¸ Cross-Encoderæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
                self.rerank_model = None
        
    def initialize_collection(self, collection_name: str = None, reset: bool = False):
        """åˆå§‹åŒ–æˆ–é‡ç½®é›†åˆ"""
        if collection_name is None:
            collection_name = config.COLLECTION_NAME + "_enhanced"
            
        if reset and self.collection:
            try:
                self.client.delete_collection(name=collection_name)
                print(f"ğŸ—‘ å·²åˆ é™¤ç°æœ‰é›†åˆ: {collection_name}")
            except:
                pass
        
        self.collection = self.client.get_or_create_collection(
            name=collection_name,
            metadata={"hnsw:space": "cosine"}
        )
        
        print(f"âœ… å¢å¼ºé›†åˆåˆå§‹åŒ–å®Œæˆ: {collection_name}")
        print(f"   - å½“å‰æ–‡æ¡£æ•°é‡: {self.collection.count()}")
        
    def enhanced_triple_to_text(self, triple: tuple, schema: tuple) -> str:
        """
        å¢å¼ºçš„ä¸‰å…ƒç»„åˆ°æ–‡æœ¬è½¬æ¢ - ä½¿ç”¨æ›´è‡ªç„¶çš„æ¨¡æ¿å¥å­
        
        Args:
            triple: (subject, relation, object)
            schema: (subject_type, relation_type, object_type)
        """
        sub, rel, obj = triple
        schema_sub, schema_rel, schema_obj = schema
        
        # æ¸…ç†å®ä½“åç§°
        sub_clean = sub.replace('_', ' ')
        obj_clean = obj.replace('_', ' ')
        rel_clean = rel.replace('_', ' ')
        
        # ä½¿ç”¨æ›´è‡ªç„¶çš„æ¨¡æ¿å¥å­
        template = (f"An instance of a '{schema_sub}' named '{sub_clean}' "
                   f"has a relation '{rel_clean}' "
                   f"with an instance of a '{schema_obj}' which is '{obj_clean}'.")
        
        return template
    
    def create_enhanced_metadata(self, entry: Dict) -> Dict:
        """
        åˆ›å»ºå¢å¼ºçš„å…ƒæ•°æ®ï¼ŒåŒ…å«å®Œæ•´çš„ç»“æ„åŒ–ä¿¡æ¯
        
        Args:
            entry: åŒ…å«triple, schemaç­‰ä¿¡æ¯çš„æ¡ç›®
        """
        triple = entry["triple"]
        schema = entry["schema"]
        
        sub, rel, obj = triple
        sub_type, rel_type, obj_type = schema
        
        # æ¸…ç†åç§°
        sub_clean = sub.replace('_', ' ')
        obj_clean = obj.replace('_', ' ')
        rel_clean = rel.replace('_', ' ')
        
        metadata = {
            # åŸå§‹ä¸‰å…ƒç»„ä¿¡æ¯
            "sub": sub,
            "rel": rel, 
            "obj": obj,
            "sub_type": sub_type,
            "rel_type": rel_type,
            "obj_type": obj_type,
            
            # æ¸…ç†åçš„åç§°
            "sub_clean": sub_clean,
            "rel_clean": rel_clean,
            "obj_clean": obj_clean,
            
            # é¢å¤–çš„æ£€ç´¢è¾…åŠ©ä¿¡æ¯
            "entities": f"{sub_clean} {obj_clean}",  # å®ä½“ç»„åˆ
            "relation_context": f"{sub_type} {rel_clean} {obj_type}",  # å…³ç³»ä¸Šä¸‹æ–‡
            "full_context": f"{sub_clean} {rel_clean} {obj_clean} {sub_type} {obj_type}",  # å®Œæ•´ä¸Šä¸‹æ–‡
            
            # åŸæœ‰ä¿¡æ¯
            "source_file": entry.get("source_file", ""),
            "text": entry.get("text", "")
        }
        
        return metadata
    
    def populate_enhanced_database(self, knowledge_entries: Optional[List[Dict]] = None):
        """ä½¿ç”¨å¢å¼ºç­–ç•¥å¡«å……å‘é‡æ•°æ®åº“"""
        if not self.collection:
            self.initialize_collection()
        
        # å¦‚æœæ²¡æœ‰æä¾›æ•°æ®ï¼Œåˆ™åŠ è½½æ•°æ®
        if knowledge_entries is None:
            loader = KnowledgeDataLoader()
            knowledge_entries = loader.get_knowledge_entries()
        
        if not knowledge_entries:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°çŸ¥è¯†æ¡ç›®")
            return
        
        print(f"ğŸ”„ å¼€å§‹å¡«å……å¢å¼ºæ•°æ®åº“ï¼Œå…± {len(knowledge_entries)} ä¸ªæ¡ç›®")
        
        # å‡†å¤‡æ•°æ®
        ids = [entry['id'] for entry in knowledge_entries]
        
        # ä½¿ç”¨å¢å¼ºçš„æ–‡æœ¬è½¬æ¢
        documents = [self.enhanced_triple_to_text(entry["triple"], entry["schema"]) 
                    for entry in knowledge_entries]
        
        # åˆ›å»ºå¢å¼ºçš„å…ƒæ•°æ®
        metadatas = [self.create_enhanced_metadata(entry) for entry in knowledge_entries]
        
        # åˆ†æ‰¹å¤„ç†
        batch_size = config.BATCH_SIZE
        
        for i in tqdm(range(0, len(ids), batch_size), desc="å¢å¼ºåµŒå…¥å¤„ç†"):
            batch_ids = ids[i:i+batch_size]
            batch_documents = documents[i:i+batch_size]
            batch_metadatas = metadatas[i:i+batch_size]
            
            # è·å–åµŒå…¥å‘é‡
            batch_embeddings = self.embedding_client.get_embeddings_batch(batch_documents)
            
            if batch_embeddings:
                self.collection.add(
                    ids=batch_ids,
                    embeddings=batch_embeddings,
                    documents=batch_documents,
                    metadatas=batch_metadatas
                )
            else:
                print(f"âš  è·³è¿‡æ‰¹æ¬¡ {i}ï¼ŒåµŒå…¥å¤±è´¥")
        
        print(f"âœ… å¢å¼ºæ•°æ®åº“å¡«å……å®Œæˆï¼Œæ€»æ¡ç›®æ•°: {self.collection.count()}")
    
    def multi_stage_retrieval(self, query: str, n_results: int = 10, 
                             rerank_top_k: int = 20, rerank_method: str = 'original') -> List[Dict]:
        """
        å¤šé˜¶æ®µæ£€ç´¢é‡æ’
        
        Args:
            query: æŸ¥è¯¢é—®é¢˜
            n_results: æœ€ç»ˆè¿”å›çš„ç»“æœæ•°é‡
            rerank_top_k: ç¬¬ä¸€é˜¶æ®µæ£€ç´¢çš„æ•°é‡ï¼Œç”¨äºé‡æ’
            rerank_method: é‡æ’æ–¹æ³• ('original' æˆ– 'cross_encoder')
        """
        if not self.collection:
            print("âŒ é›†åˆæœªåˆå§‹åŒ–")
            return []
        
        # ç¬¬ä¸€é˜¶æ®µï¼šæ‰©å¤§æ£€ç´¢èŒƒå›´
        stage1_results = self._stage1_retrieval(query, rerank_top_k)
        
        if not stage1_results:
            return []
        
        # ç¬¬äºŒé˜¶æ®µï¼šé€‰æ‹©é‡æ’æ–¹æ³•
        if rerank_method == 'cross_encoder' and self.rerank_model is not None:
            stage2_results = self._stage2_cross_encoder_reranking(query, stage1_results)
        else:
            # ä½¿ç”¨åŸæœ‰çš„å¤šç­–ç•¥é‡æ’
            stage2_results = self._stage2_reranking(query, stage1_results)
        
        # è¿”å›Top-Kç»“æœ
        return stage2_results[:n_results]
    
    def _stage1_retrieval(self, query: str, n_results: int) -> List[Dict]:
        """ç¬¬ä¸€é˜¶æ®µï¼šåŸºç¡€å‘é‡æ£€ç´¢"""
        # è·å–æŸ¥è¯¢åµŒå…¥
        query_embedding = self.embedding_client.get_embeddings_batch([query])
        if not query_embedding:
            print("âŒ æŸ¥è¯¢åµŒå…¥å¤±è´¥")
            return []
        
        # æ‰§è¡Œå‘é‡æ£€ç´¢
        results = self.collection.query(
            query_embeddings=query_embedding,
            n_results=n_results
        )
        
        # æ ¼å¼åŒ–ç»“æœ
        formatted_results = []
        
        if results and results['ids'][0]:
            for i in range(len(results['ids'][0])):
                metadata = results['metadatas'][0][i]
                
                formatted_results.append({
                    'id': results['ids'][0][i],
                    'triple': (metadata['sub'], metadata['rel'], metadata['obj']),
                    'schema': (metadata['sub_type'], metadata['rel_type'], metadata['obj_type']),
                    'distance': results['distances'][0][i],
                    'document': results['documents'][0][i],
                    'text': metadata.get('text', ''),
                    'source_file': metadata.get('source_file', ''),
                    'metadata': metadata,
                    'stage1_score': 1 - results['distances'][0][i]  # è½¬æ¢ä¸ºç›¸ä¼¼åº¦åˆ†æ•°
                })
        
        return formatted_results
    
    def _stage2_reranking(self, query: str, candidates: List[Dict]) -> List[Dict]:
        """ç¬¬äºŒé˜¶æ®µï¼šå¤šç­–ç•¥é‡æ’"""
        
        # è®¡ç®—å¤šç§ç›¸å…³æ€§åˆ†æ•°
        for candidate in candidates:
            scores = {}
            
            # 1. å®ä½“åŒ¹é…åˆ†æ•°
            scores['entity_match'] = self._calculate_entity_match_score(query, candidate)
            
            # 2. å…³ç³»åŒ¹é…åˆ†æ•°  
            scores['relation_match'] = self._calculate_relation_match_score(query, candidate)
            
            # 3. ç±»å‹åŒ¹é…åˆ†æ•°
            scores['type_match'] = self._calculate_type_match_score(query, candidate)
            
            # 4. è¯­ä¹‰ç›¸ä¼¼åº¦åˆ†æ•° (æ¥è‡ªç¬¬ä¸€é˜¶æ®µ)
            scores['semantic_similarity'] = candidate['stage1_score']
            
            # 5. ç»¼åˆåˆ†æ•° (å¯è°ƒæƒé‡)
            weights = {
                'entity_match': 0.3,
                'relation_match': 0.25, 
                'type_match': 0.2,
                'semantic_similarity': 0.25
            }
            
            final_score = sum(weights[key] * scores[key] for key in weights)
            candidate['rerank_score'] = final_score
            candidate['detailed_scores'] = scores
        
        # æŒ‰é‡æ’åˆ†æ•°æ’åº
        candidates.sort(key=lambda x: x['rerank_score'], reverse=True)
        
        return candidates
    
    def _stage2_cross_encoder_reranking(self, query: str, candidates: List[Dict]) -> List[Dict]:
        """ç¬¬äºŒé˜¶æ®µï¼šã€æ–°æ–¹æ³•ã€‘ä½¿ç”¨Cross-Encoderæ¨¡å‹è¿›è¡Œé‡æ’"""
        if not candidates:
            return []
        
        if self.rerank_model is None:
            print("âš ï¸ Cross-Encoderæ¨¡å‹æœªåŠ è½½ï¼Œå›é€€åˆ°åŸæœ‰é‡æ’æ–¹æ³•")
            return self._stage2_reranking(query, candidates)
        
        try:
            # åˆ›å»ºæ¨¡å‹éœ€è¦è¾“å…¥çš„å¥å­å¯¹ï¼š(æŸ¥è¯¢, å€™é€‰æ–‡æ¡£çš„æ–‡æœ¬)
            sentence_pairs = [[query, candidate['document']] for candidate in candidates]
            
            # æ¨¡å‹ä¼šä¸ºæ¯ä¸ªå¥å­å¯¹è®¡ç®—ä¸€ä¸ªç›¸å…³æ€§åˆ†æ•°
            scores = self.rerank_model.predict(sentence_pairs)
            
            # å°†åˆ†æ•°æ·»åŠ å›å€™é€‰æ–‡æ¡£ä¸­
            for i in range(len(candidates)):
                candidates[i]['rerank_score'] = float(scores[i])
                candidates[i]['rerank_method'] = 'cross_encoder'
                # ä¿ç•™åŸæœ‰çš„è¯¦ç»†åˆ†æ•°ç”¨äºå¯¹æ¯”
                candidates[i]['original_stage1_score'] = candidates[i].get('stage1_score', 0)
            
            # æŒ‰æ–°çš„é‡æ’åˆ†æ•°é™åºæ’åº
            candidates.sort(key=lambda x: x['rerank_score'], reverse=True)
            
            return candidates
            
        except Exception as e:
            print(f"âš ï¸ Cross-Encoderé‡æ’å¤±è´¥: {e}")
            # å›é€€åˆ°åŸæœ‰é‡æ’æ–¹æ³•
            return self._stage2_reranking(query, candidates)
    
    def _calculate_entity_match_score(self, query: str, candidate: Dict) -> float:
        """è®¡ç®—å®ä½“åŒ¹é…åˆ†æ•°"""
        query_lower = query.lower()
        metadata = candidate['metadata']
        
        score = 0.0
        
        # æ£€æŸ¥ä¸»è¯­å®ä½“åŒ¹é…
        sub_clean = metadata['sub_clean'].lower()
        if sub_clean in query_lower:
            score += 0.5
        
        # æ£€æŸ¥å®¾è¯­å®ä½“åŒ¹é…
        obj_clean = metadata['obj_clean'].lower()
        if obj_clean in query_lower:
            score += 0.5
        
        # éƒ¨åˆ†åŒ¹é…
        sub_words = sub_clean.split()
        obj_words = obj_clean.split()
        query_words = query_lower.split()
        
        for word in sub_words + obj_words:
            if len(word) > 3 and word in query_words:
                score += 0.1
        
        return min(score, 1.0)  # é™åˆ¶åœ¨[0,1]èŒƒå›´
    
    def _calculate_relation_match_score(self, query: str, candidate: Dict) -> float:
        """è®¡ç®—å…³ç³»åŒ¹é…åˆ†æ•°"""
        query_lower = query.lower()
        metadata = candidate['metadata']
        
        score = 0.0
        
        # å…³ç³»è¯åŒ¹é…
        rel_clean = metadata['rel_clean'].lower()
        rel_words = rel_clean.split()
        
        # å®šä¹‰å…³ç³»å…³é”®è¯æ˜ å°„
        relation_keywords = {
            'leader': ['leader', 'president', 'king', 'queen', 'head', 'chief'],
            'location': ['location', 'located', 'place', 'where', 'country', 'city'],
            'capital': ['capital'],
            'type': ['type', 'kind', 'category'],
            'runway': ['runway', 'strip'],
            'owner': ['owner', 'owned', 'belong']
        }
        
        # æ£€æŸ¥ç›´æ¥åŒ¹é…
        for rel_word in rel_words:
            if rel_word in query_lower:
                score += 0.4
        
        # æ£€æŸ¥è¯­ä¹‰åŒ¹é…
        for rel_type, keywords in relation_keywords.items():
            if rel_type in rel_clean:
                for keyword in keywords:
                    if keyword in query_lower:
                        score += 0.3
                        break
        
        return min(score, 1.0)
    
    def _calculate_type_match_score(self, query: str, candidate: Dict) -> float:
        """è®¡ç®—ç±»å‹åŒ¹é…åˆ†æ•°"""
        query_lower = query.lower()
        metadata = candidate['metadata']
        
        score = 0.0
        
        # ç±»å‹å…³é”®è¯æ˜ å°„
        type_keywords = {
            'country': ['country', 'nation'],
            'airport': ['airport'],
            'city': ['city', 'town'],
            'person': ['person', 'people', 'who'],
            'organization': ['organization', 'company'],
            'location': ['location', 'place', 'where']
        }
        
        sub_type = metadata['sub_type'].lower()
        obj_type = metadata['obj_type'].lower()
        
        # æ£€æŸ¥ç±»å‹åŒ¹é…
        for entity_type in [sub_type, obj_type]:
            if entity_type in query_lower:
                score += 0.4
            
            # æ£€æŸ¥è¯­ä¹‰åŒ¹é…
            for type_name, keywords in type_keywords.items():
                if type_name in entity_type:
                    for keyword in keywords:
                        if keyword in query_lower:
                            score += 0.2
                            break
        
        return min(score, 1.0)
    
    def get_database_stats(self) -> Dict:
        """è·å–æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯"""
        if not self.collection:
            return {"status": "Collection not initialized"}
        
        return {
            "collection_name": self.collection.name,
            "total_documents": self.collection.count(),
            "status": "ready",
            "enhancement": "multi-stage retrieval with reranking"
        }

# æµ‹è¯•å‡½æ•°
def test_enhanced_system():
    """æµ‹è¯•å¢å¼ºç³»ç»Ÿ"""
    print("ğŸ§ª æµ‹è¯•å¢å¼ºåµŒå…¥ç³»ç»Ÿ")
    print("=" * 50)
    
    # åˆå§‹åŒ–å¢å¼ºç³»ç»Ÿ
    enhanced_db = EnhancedVectorDatabaseManager()
    enhanced_db.initialize_collection(reset=True)
    
    # å¡«å……æ•°æ®åº“
    print("ğŸ“Š å¡«å……å¢å¼ºæ•°æ®åº“...")
    enhanced_db.populate_enhanced_database()
    
    # æµ‹è¯•æŸ¥è¯¢
    test_queries = [
        "Who is the leader of Belgium?",
        "Where is Amsterdam Airport Schiphol located?",
        "What type of entity is Belgium?"
    ]
    
    for query in test_queries:
        print(f"\nâ“ æŸ¥è¯¢: {query}")
        
        # å¤šé˜¶æ®µæ£€ç´¢
        results = enhanced_db.multi_stage_retrieval(query, n_results=5)
        
        print("ğŸ” å¤šé˜¶æ®µæ£€ç´¢ç»“æœ:")
        for i, result in enumerate(results, 1):
            triple = result['triple']
            rerank_score = result['rerank_score']
            detailed_scores = result['detailed_scores']
            
            print(f"   {i}. {triple}")
            print(f"      é‡æ’åˆ†æ•°: {rerank_score:.4f}")
            print(f"      è¯¦ç»†åˆ†æ•°: {detailed_scores}")

if __name__ == '__main__':
    test_enhanced_system()