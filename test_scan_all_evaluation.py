# test_scan_all_evaluation.py - æµ‹è¯•å…¨é‡æ‰«æè¯„ä¼°åŠŸèƒ½

import sys
from pathlib import Path
sys.path.append(str(Path(__file__).parent))

from retrieval_evaluation_system import RetrievalEvaluator

def test_scan_all_functionality():
    """æµ‹è¯•å…¨é‡æ‰«æåŠŸèƒ½"""
    print("ğŸ§ª æµ‹è¯•å…¨é‡æ‰«æè¯„ä¼°åŠŸèƒ½")
    print("=" * 50)
    
    evaluator = RetrievalEvaluator()
    
    # æµ‹è¯•åŠ è½½å…¨éƒ¨æ•°æ®
    print("ğŸ“Š æµ‹è¯•1: åŠ è½½å…¨éƒ¨QAæ•°æ®")
    all_questions = evaluator.load_qa_dataset(scan_all=True)
    
    if all_questions:
        print(f"âœ… æˆåŠŸåŠ è½½ {len(all_questions)} ä¸ªé—®é¢˜")
        
        # ç»Ÿè®¡ä¿¡æ¯
        question_types = {}
        source_files = {}
        
        for q in all_questions:
            q_type = q.get('question_type', 'unknown')
            source_file = q.get('source_file', 'unknown')
            
            question_types[q_type] = question_types.get(q_type, 0) + 1
            source_files[source_file] = source_files.get(source_file, 0) + 1
        
        print(f"\nğŸ“‹ é—®é¢˜ç±»å‹åˆ†å¸ƒ:")
        for q_type, count in sorted(question_types.items()):
            percentage = (count / len(all_questions)) * 100
            print(f"   - {q_type}: {count} ({percentage:.1f}%)")
        
        print(f"\nğŸ“ æ•°æ®æ–‡ä»¶åˆ†å¸ƒ:")
        for source_file, count in sorted(source_files.items()):
            percentage = (count / len(all_questions)) * 100
            print(f"   - {source_file}: {count} ({percentage:.1f}%)")
        
        # æ˜¾ç¤ºå‰å‡ ä¸ªé—®é¢˜ç¤ºä¾‹
        print(f"\nğŸ“ é—®é¢˜ç¤ºä¾‹ (å‰5ä¸ª):")
        for i, q in enumerate(all_questions[:5], 1):
            print(f"{i}. ç±»å‹: {q.get('question_type', 'unknown')}")
            print(f"   é—®é¢˜: {q['question'][:80]}...")
            print(f"   ç­”æ¡ˆ: {q['expected_answer']}")
            print(f"   æ¥æº: {q.get('source_file', 'unknown')}")
            print()
    else:
        print("âŒ æœªæ‰¾åˆ°QAæ•°æ®")
    
    # æµ‹è¯•å¯¹æ¯”é™åˆ¶æ¨¡å¼
    print("\nğŸ“Š æµ‹è¯•2: å¯¹æ¯”é™åˆ¶æ¨¡å¼")
    limited_questions = evaluator.load_qa_dataset(limit=10, scan_all=False)
    print(f"é™åˆ¶æ¨¡å¼: {len(limited_questions)} ä¸ªé—®é¢˜")
    print(f"å…¨é‡æ¨¡å¼: {len(all_questions)} ä¸ªé—®é¢˜")
    
    print("\n" + "=" * 50)
    print("ğŸ¯ æµ‹è¯•å®Œæˆï¼")

def show_usage_examples():
    """æ˜¾ç¤ºä½¿ç”¨ç¤ºä¾‹"""
    print("\nğŸ’¡ å…¨é‡æ‰«æè¯„ä¼°ä½¿ç”¨ç¤ºä¾‹:")
    print("=" * 40)
    print("1. æ‰«æå…¨éƒ¨QAæ•°æ®:")
    print("   python retrieval_evaluation_system.py --mode scan-all")
    print()
    print("2. æŒ‡å®šæ•°æ®é›†è·¯å¾„:")
    print("   python retrieval_evaluation_system.py --mode scan-all --qa-path custom_qa_datasets")
    print()
    print("3. è‡ªå®šä¹‰Kå€¼:")
    print("   python retrieval_evaluation_system.py --mode scan-all --k-values 1 3 5 10 20")
    print()
    print("4. æŒ‡å®šè¾“å‡ºç›®å½•:")
    print("   python retrieval_evaluation_system.py --mode scan-all --output-dir custom_evaluation")
    print("=" * 40)

if __name__ == "__main__":
    # æ˜¾ç¤ºä½¿ç”¨ç¤ºä¾‹
    show_usage_examples()
    
    # è¿è¡Œæµ‹è¯•
    test_scan_all_functionality()