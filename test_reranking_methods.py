#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
æµ‹è¯•æ–°æ—§é‡æ’æ–¹æ³•æ€§èƒ½å¯¹æ¯”
ä»QAæ•°æ®é›†ä¸­éšæœºé€‰æ‹©100ä¸ªé—®é¢˜ï¼Œæµ‹è¯•recall@kå’ŒnDCG@kæ€§èƒ½
"""

import json
import random
import numpy as np
from pathlib import Path
from typing import List, Dict, Tuple
from tqdm import tqdm
import time

from enhanced_embedding_system import EnhancedVectorDatabaseManager

class RerankingMethodComparator:
    """é‡æ’æ–¹æ³•å¯¹æ¯”å™¨"""
    
    def __init__(self):
        self.db_manager = EnhancedVectorDatabaseManager()
        self.db_manager.initialize_collection()
        
    def load_qa_questions(self, max_questions: int = 100) -> List[Dict]:
        """ä»QAæ•°æ®é›†ä¸­åŠ è½½é—®é¢˜"""
        qa_files = list(Path("qa_datasets").glob("*.json"))
        
        if not qa_files:
            print("âŒ æœªæ‰¾åˆ°QAæ•°æ®é›†æ–‡ä»¶")
            return []
        
        all_questions = []
        
        for qa_file in qa_files:
            try:
                with open(qa_file, 'r', encoding='utf-8') as f:
                    qa_data = json.load(f)
                
                for item in qa_data:
                    if isinstance(item, dict) and 'question' in item and 'answer' in item:
                        all_questions.append({
                            'question': item['question'],
                            'answer': item['answer'],
                            'source_file': qa_file.name
                        })
                        
            except Exception as e:
                print(f"âš ï¸ è¯»å–æ–‡ä»¶ {qa_file} å¤±è´¥: {e}")
                continue
        
        if not all_questions:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„QAé—®é¢˜")
            return []
        
        # éšæœºé€‰æ‹©æŒ‡å®šæ•°é‡çš„é—®é¢˜
        if len(all_questions) > max_questions:
            selected_questions = random.sample(all_questions, max_questions)
        else:
            selected_questions = all_questions
        
        print(f"âœ… åŠ è½½äº† {len(selected_questions)} ä¸ªæµ‹è¯•é—®é¢˜")
        return selected_questions
    
    def calculate_precision_at_k(self, retrieved_items: List[Dict], expected_answer: str, k_values: List[int]) -> Dict[str, float]:
        """è®¡ç®—Precision@KæŒ‡æ ‡"""
        precision_scores = {}
        
        for k in k_values:
            relevant_count = 0
            top_k_items = retrieved_items[:k]
            
            for item in top_k_items:
                # æ£€æŸ¥æ£€ç´¢é¡¹æ˜¯å¦ä¸æœŸæœ›ç­”æ¡ˆç›¸å…³
                if self._is_relevant(item, expected_answer):
                    relevant_count += 1
            
            # Precision@K = ç›¸å…³æ–‡æ¡£æ•° / K
            precision_scores[f'precision@{k}'] = relevant_count / k if k > 0 else 0.0
        
        return precision_scores
    
    def calculate_recall_at_k(self, retrieved_items: List[Dict], expected_answer: str, k_values: List[int]) -> Dict[str, float]:
        """è®¡ç®—Recall@KæŒ‡æ ‡"""
        recall_scores = {}
        
        for k in k_values:
            relevant_count = 0
            top_k_items = retrieved_items[:k]
            
            for item in top_k_items:
                # æ£€æŸ¥æ£€ç´¢é¡¹æ˜¯å¦ä¸æœŸæœ›ç­”æ¡ˆç›¸å…³
                if self._is_relevant(item, expected_answer):
                    relevant_count += 1
            
            # Recall@K = ç›¸å…³æ–‡æ¡£æ•° / æ€»ç›¸å…³æ–‡æ¡£æ•° (è¿™é‡Œå‡è®¾æ€»ç›¸å…³æ–‡æ¡£æ•°ä¸º1)
            recall_scores[f'recall@{k}'] = min(relevant_count, 1)  # æœ€å¤šä¸º1
        
        return recall_scores
    
    def calculate_ndcg_at_k(self, retrieved_items: List[Dict], expected_answer: str, k_values: List[int]) -> Dict[str, float]:
        """è®¡ç®—nDCG@KæŒ‡æ ‡"""
        ndcg_scores = {}
        
        for k in k_values:
            top_k_items = retrieved_items[:k]
            
            # è®¡ç®—DCG@K
            dcg = 0.0
            for i, item in enumerate(top_k_items):
                relevance = 1.0 if self._is_relevant(item, expected_answer) else 0.0
                dcg += relevance / np.log2(i + 2)  # i+2 å› ä¸ºlog2(1)=0
            
            # è®¡ç®—IDCG@K (ç†æƒ³æƒ…å†µä¸‹çš„DCG)
            idcg = 1.0 / np.log2(2)  # å‡è®¾åªæœ‰1ä¸ªç›¸å…³æ–‡æ¡£ï¼Œä¸”åœ¨ç¬¬1ä½
            
            # nDCG@K = DCG@K / IDCG@K
            ndcg_scores[f'ndcg@{k}'] = dcg / idcg if idcg > 0 else 0.0
        
        return ndcg_scores
    
    def _is_relevant(self, item: Dict, expected_answer: str) -> bool:
        """åˆ¤æ–­æ£€ç´¢é¡¹æ˜¯å¦ä¸æœŸæœ›ç­”æ¡ˆç›¸å…³"""
        # ç®€å•çš„ç›¸å…³æ€§åˆ¤æ–­ï¼šæ£€æŸ¥ä¸‰å…ƒç»„ä¸­æ˜¯å¦åŒ…å«æœŸæœ›ç­”æ¡ˆçš„å…³é”®è¯
        triple = item.get('triple', [])
        text = item.get('text', '')
        document = item.get('document', '')
        
        expected_lower = expected_answer.lower()
        
        # æ£€æŸ¥ä¸‰å…ƒç»„çš„å„ä¸ªéƒ¨åˆ†
        for part in triple:
            if isinstance(part, str):
                part_clean = part.replace('_', ' ').lower()
                if expected_lower in part_clean or part_clean in expected_lower:
                    return True
        
        # æ£€æŸ¥æ–‡æœ¬å†…å®¹
        if expected_lower in text.lower() or expected_lower in document.lower():
            return True
        
        return False
    
    def test_single_question(self, question: str, expected_answer: str) -> Dict:
        """æµ‹è¯•å•ä¸ªé—®é¢˜çš„ä¸¤ç§é‡æ’æ–¹æ³•"""
        k_values = [1, 3, 5, 10]
        
        # æµ‹è¯•åŸæœ‰é‡æ’æ–¹æ³•
        start_time = time.time()
        original_results = self.db_manager.multi_stage_retrieval(
            query=question,
            n_results=10,
            rerank_top_k=40,
            rerank_method='original'
        )
        original_time = time.time() - start_time
        
        # æµ‹è¯•Cross-Encoderé‡æ’æ–¹æ³•
        start_time = time.time()
        cross_encoder_results = self.db_manager.multi_stage_retrieval(
            query=question,
            n_results=10,
            rerank_top_k=40,
            rerank_method='cross_encoder'
        )
        cross_encoder_time = time.time() - start_time
        
        # è®¡ç®—æŒ‡æ ‡
        original_precision = self.calculate_precision_at_k(original_results, expected_answer, k_values)
        original_recall = self.calculate_recall_at_k(original_results, expected_answer, k_values)
        original_ndcg = self.calculate_ndcg_at_k(original_results, expected_answer, k_values)
        
        cross_encoder_precision = self.calculate_precision_at_k(cross_encoder_results, expected_answer, k_values)
        cross_encoder_recall = self.calculate_recall_at_k(cross_encoder_results, expected_answer, k_values)
        cross_encoder_ndcg = self.calculate_ndcg_at_k(cross_encoder_results, expected_answer, k_values)
        
        return {
            'question': question,
            'expected_answer': expected_answer,
            'original_method': {
                'precision': original_precision,
                'recall': original_recall,
                'ndcg': original_ndcg,
                'time': original_time,
                'results_count': len(original_results)
            },
            'cross_encoder_method': {
                'precision': cross_encoder_precision,
                'recall': cross_encoder_recall,
                'ndcg': cross_encoder_ndcg,
                'time': cross_encoder_time,
                'results_count': len(cross_encoder_results)
            }
        }
    
    def run_comparison(self, test_questions: List[Dict]) -> Dict:
        """è¿è¡Œå®Œæ•´çš„å¯¹æ¯”æµ‹è¯•"""
        print(f"ğŸš€ å¼€å§‹é‡æ’æ–¹æ³•å¯¹æ¯”æµ‹è¯•")
        print(f"ğŸ“Š æµ‹è¯•é—®é¢˜æ•°é‡: {len(test_questions)}")
        print("=" * 60)
        
        all_results = []
        
        for i, qa_item in enumerate(tqdm(test_questions, desc="æµ‹è¯•è¿›åº¦")):
            try:
                result = self.test_single_question(qa_item['question'], qa_item['answer'])
                result['test_id'] = i + 1
                result['source_file'] = qa_item['source_file']
                all_results.append(result)
                
            except Exception as e:
                print(f"âš ï¸ æµ‹è¯•é—®é¢˜ {i+1} å¤±è´¥: {e}")
                continue
        
        # è®¡ç®—å¹³å‡æŒ‡æ ‡
        summary = self._calculate_summary_metrics(all_results)
        
        return {
            'summary': summary,
            'detailed_results': all_results,
            'test_info': {
                'total_questions': len(test_questions),
                'successful_tests': len(all_results),
                'failed_tests': len(test_questions) - len(all_results)
            }
        }
    
    def _calculate_summary_metrics(self, results: List[Dict]) -> Dict:
        """è®¡ç®—æ±‡æ€»æŒ‡æ ‡"""
        if not results:
            return {}
        
        k_values = [1, 3, 5, 10]
        
        # åˆå§‹åŒ–ç´¯è®¡å€¼
        original_precision_sum = {f'precision@{k}': 0.0 for k in k_values}
        original_recall_sum = {f'recall@{k}': 0.0 for k in k_values}
        original_ndcg_sum = {f'ndcg@{k}': 0.0 for k in k_values}
        cross_encoder_precision_sum = {f'precision@{k}': 0.0 for k in k_values}
        cross_encoder_recall_sum = {f'recall@{k}': 0.0 for k in k_values}
        cross_encoder_ndcg_sum = {f'ndcg@{k}': 0.0 for k in k_values}
        
        original_time_sum = 0.0
        cross_encoder_time_sum = 0.0
        
        # ç´¯è®¡æ‰€æœ‰ç»“æœ
        for result in results:
            for k in k_values:
                original_precision_sum[f'precision@{k}'] += result['original_method']['precision'][f'precision@{k}']
                original_recall_sum[f'recall@{k}'] += result['original_method']['recall'][f'recall@{k}']
                original_ndcg_sum[f'ndcg@{k}'] += result['original_method']['ndcg'][f'ndcg@{k}']
                cross_encoder_precision_sum[f'precision@{k}'] += result['cross_encoder_method']['precision'][f'precision@{k}']
                cross_encoder_recall_sum[f'recall@{k}'] += result['cross_encoder_method']['recall'][f'recall@{k}']
                cross_encoder_ndcg_sum[f'ndcg@{k}'] += result['cross_encoder_method']['ndcg'][f'ndcg@{k}']
            
            original_time_sum += result['original_method']['time']
            cross_encoder_time_sum += result['cross_encoder_method']['time']
        
        # è®¡ç®—å¹³å‡å€¼
        n = len(results)
        
        return {
            'original_method': {
                'avg_precision': {k: v/n for k, v in original_precision_sum.items()},
                'avg_recall': {k: v/n for k, v in original_recall_sum.items()},
                'avg_ndcg': {k: v/n for k, v in original_ndcg_sum.items()},
                'avg_time': original_time_sum / n
            },
            'cross_encoder_method': {
                'avg_precision': {k: v/n for k, v in cross_encoder_precision_sum.items()},
                'avg_recall': {k: v/n for k, v in cross_encoder_recall_sum.items()},
                'avg_ndcg': {k: v/n for k, v in cross_encoder_ndcg_sum.items()},
                'avg_time': cross_encoder_time_sum / n
            }
        }
    
    def print_comparison_results(self, comparison_results: Dict):
        """æ‰“å°å¯¹æ¯”ç»“æœ"""
        summary = comparison_results['summary']
        test_info = comparison_results['test_info']
        
        print("\n" + "="*60)
        print("ğŸ“Š é‡æ’æ–¹æ³•å¯¹æ¯”ç»“æœ")
        print("="*60)
        
        print(f"ğŸ“ˆ æµ‹è¯•ç»Ÿè®¡:")
        print(f"   - æ€»é—®é¢˜æ•°: {test_info['total_questions']}")
        print(f"   - æˆåŠŸæµ‹è¯•: {test_info['successful_tests']}")
        print(f"   - å¤±è´¥æµ‹è¯•: {test_info['failed_tests']}")
        
        print(f"\nğŸ“‹ Precision@K å¯¹æ¯”:")
        print(f"{'æŒ‡æ ‡':<12} {'åŸæœ‰æ–¹æ³•':<12} {'Cross-Encoder':<15} {'æå‡':<10}")
        print("-" * 50)
        
        for k in [1, 3, 5, 10]:
            metric = f'precision@{k}'
            original_score = summary['original_method']['avg_precision'][metric]
            cross_encoder_score = summary['cross_encoder_method']['avg_precision'][metric]
            improvement = cross_encoder_score - original_score
            
            print(f"{metric:<12} {original_score:<12.4f} {cross_encoder_score:<15.4f} {improvement:+.4f}")
        
        print(f"\nğŸ“‹ Recall@K å¯¹æ¯”:")
        print(f"{'æŒ‡æ ‡':<12} {'åŸæœ‰æ–¹æ³•':<12} {'Cross-Encoder':<15} {'æå‡':<10}")
        print("-" * 50)
        
        for k in [1, 3, 5, 10]:
            metric = f'recall@{k}'
            original_score = summary['original_method']['avg_recall'][metric]
            cross_encoder_score = summary['cross_encoder_method']['avg_recall'][metric]
            improvement = cross_encoder_score - original_score
            
            print(f"{metric:<12} {original_score:<12.4f} {cross_encoder_score:<15.4f} {improvement:+.4f}")
        
        print(f"\nğŸ“‹ nDCG@K å¯¹æ¯”:")
        print(f"{'æŒ‡æ ‡':<12} {'åŸæœ‰æ–¹æ³•':<12} {'Cross-Encoder':<15} {'æå‡':<10}")
        print("-" * 50)
        
        for k in [1, 3, 5, 10]:
            metric = f'ndcg@{k}'
            original_score = summary['original_method']['avg_ndcg'][metric]
            cross_encoder_score = summary['cross_encoder_method']['avg_ndcg'][metric]
            improvement = cross_encoder_score - original_score
            
            print(f"{metric:<12} {original_score:<12.4f} {cross_encoder_score:<15.4f} {improvement:+.4f}")
        
        print(f"\nâ±ï¸ æ€§èƒ½å¯¹æ¯”:")
        original_time = summary['original_method']['avg_time']
        cross_encoder_time = summary['cross_encoder_method']['avg_time']
        time_ratio = cross_encoder_time / original_time if original_time > 0 else 0
        
        print(f"   - åŸæœ‰æ–¹æ³•å¹³å‡æ—¶é—´: {original_time:.4f}ç§’")
        print(f"   - Cross-Encoderå¹³å‡æ—¶é—´: {cross_encoder_time:.4f}ç§’")
        print(f"   - æ—¶é—´æ¯”ç‡: {time_ratio:.2f}x")
        
        # ä¿å­˜è¯¦ç»†ç»“æœ
        output_file = "reranking_comparison_results.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(comparison_results, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ’¾ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {output_file}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ”„ é‡æ’æ–¹æ³•æ€§èƒ½å¯¹æ¯”æµ‹è¯•")
    print("ğŸ¯ å¯¹æ¯”åŸæœ‰å¤šç­–ç•¥é‡æ’ vs Cross-Encoderé‡æ’")
    print("=" * 60)
    
    try:
        # åˆå§‹åŒ–å¯¹æ¯”å™¨
        comparator = RerankingMethodComparator()
        
        # åŠ è½½æµ‹è¯•é—®é¢˜
        test_questions = comparator.load_qa_questions(max_questions=1000)
        
        if not test_questions:
            print("âŒ æ²¡æœ‰å¯ç”¨çš„æµ‹è¯•é—®é¢˜")
            return
        
        # è¿è¡Œå¯¹æ¯”æµ‹è¯•
        comparison_results = comparator.run_comparison(test_questions)
        
        # æ‰“å°ç»“æœ
        comparator.print_comparison_results(comparison_results)
        
        print("\nâœ… é‡æ’æ–¹æ³•å¯¹æ¯”æµ‹è¯•å®Œæˆ!")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()