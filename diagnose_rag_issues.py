#!/usr/bin/env python3
# diagnose_rag_issues.py - è¯Šæ–­RAGç³»ç»Ÿé—®é¢˜

import json
from pathlib import Path
from collections import defaultdict, Counter
from enhanced_retrieval_engine import EnhancedRetrievalEngine
from enhanced_embedding_system import EnhancedVectorDatabaseManager

class RAGDiagnostics:
    """RAGç³»ç»Ÿè¯Šæ–­å·¥å…·"""
    
    def __init__(self):
        self.engine = EnhancedRetrievalEngine()
        self.db_manager = EnhancedVectorDatabaseManager()
        self.db_manager.initialize_collection()
    
    def analyze_validation_results(self, validation_file: str):
        """åˆ†æéªŒè¯ç»“æœï¼Œæ‰¾å‡ºé—®é¢˜æ¨¡å¼"""
        print("ğŸ” åˆ†æRAGç³»ç»Ÿé—®é¢˜")
        print("=" * 50)
        
        # åŠ è½½éªŒè¯ç»“æœ
        with open(validation_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        detailed_results = data.get('detailed_results', [])
        
        print(f"ğŸ“Š åŸºæœ¬ç»Ÿè®¡:")
        print(f"   æ€»é—®é¢˜æ•°: {data['total_questions']}")
        print(f"   RAGæ­£ç¡®ç‡: {data['rag_accuracy']:.2%}")
        print(f"   LLMæ­£ç¡®ç‡: {data['llm_accuracy']:.2%}")
        
        # åˆ†æé”™è¯¯ç±»å‹
        self._analyze_error_patterns(detailed_results)
        
        # åˆ†æé—®é¢˜ç±»å‹è¡¨ç°
        self._analyze_question_type_performance(detailed_results)
        
        # åˆ†æç­”æ¡ˆé•¿åº¦å½±å“
        self._analyze_answer_length_impact(detailed_results)
        
        # æ£€æŸ¥æ£€ç´¢è´¨é‡
        self._check_retrieval_quality(detailed_results)
        
        return detailed_results
    
    def _analyze_error_patterns(self, results):
        """åˆ†æé”™è¯¯æ¨¡å¼"""
        print(f"\nğŸ” é”™è¯¯æ¨¡å¼åˆ†æ:")
        print("-" * 30)
        
        error_types = {
            'empty_answer': 0,      # ç©ºç­”æ¡ˆ
            'wrong_entity': 0,      # é”™è¯¯å®ä½“
            'partial_correct': 0,   # éƒ¨åˆ†æ­£ç¡®
            'completely_wrong': 0   # å®Œå…¨é”™è¯¯
        }
        
        rag_errors = [r for r in results if not r['rag_correct']]
        
        for result in rag_errors:
            expected = result['expected'].lower().strip()
            rag_answer = result['rag_answer'].lower().strip()
            
            if not rag_answer or rag_answer in ['no answer', 'unknown', 'error']:
                error_types['empty_answer'] += 1
            elif any(word in rag_answer for word in expected.split()) or any(word in expected for word in rag_answer.split()):
                error_types['partial_correct'] += 1
            else:
                error_types['completely_wrong'] += 1
        
        total_errors = len(rag_errors)
        if total_errors > 0:
            for error_type, count in error_types.items():
                percentage = count / total_errors * 100
                print(f"   {error_type}: {count} ({percentage:.1f}%)")
        
        # æ˜¾ç¤ºå…¸å‹é”™è¯¯æ¡ˆä¾‹
        print(f"\nâŒ å…¸å‹é”™è¯¯æ¡ˆä¾‹:")
        for i, result in enumerate(rag_errors[:5], 1):
            print(f"\n   æ¡ˆä¾‹ {i}:")
            print(f"     é—®é¢˜: {result['question']}")
            print(f"     æœŸæœ›: {result['expected']}")
            print(f"     RAG: {result['rag_answer']}")
    
    def _analyze_question_type_performance(self, results):
        """åˆ†æä¸åŒé—®é¢˜ç±»å‹çš„è¡¨ç°"""
        print(f"\nğŸ“‹ é—®é¢˜ç±»å‹è¡¨ç°åˆ†æ:")
        print("-" * 30)
        
        # ä»é—®é¢˜ä¸­æ¨æ–­ç±»å‹
        type_performance = defaultdict(lambda: {'total': 0, 'correct': 0})
        
        for result in results:
            question = result['question'].lower()
            
            # ç®€å•çš„é—®é¢˜ç±»å‹åˆ†ç±»
            if question.startswith(('who', 'which person')):
                q_type = 'person'
            elif question.startswith(('what', 'which')):
                q_type = 'entity'
            elif question.startswith(('where', 'in which')):
                q_type = 'location'
            elif question.startswith(('when')):
                q_type = 'time'
            elif question.startswith(('how')):
                q_type = 'method'
            else:
                q_type = 'other'
            
            type_performance[q_type]['total'] += 1
            if result['rag_correct']:
                type_performance[q_type]['correct'] += 1
        
        for q_type, stats in type_performance.items():
            if stats['total'] > 0:
                accuracy = stats['correct'] / stats['total']
                print(f"   {q_type}: {stats['correct']}/{stats['total']} ({accuracy:.2%})")
    
    def _analyze_answer_length_impact(self, results):
        """åˆ†æç­”æ¡ˆé•¿åº¦å¯¹æ­£ç¡®ç‡çš„å½±å“"""
        print(f"\nğŸ“ ç­”æ¡ˆé•¿åº¦å½±å“åˆ†æ:")
        print("-" * 30)
        
        length_buckets = {
            'short': {'range': (0, 20), 'total': 0, 'correct': 0},
            'medium': {'range': (21, 50), 'total': 0, 'correct': 0},
            'long': {'range': (51, 999), 'total': 0, 'correct': 0}
        }
        
        for result in results:
            expected_len = len(result['expected'])
            
            for bucket_name, bucket in length_buckets.items():
                if bucket['range'][0] <= expected_len <= bucket['range'][1]:
                    bucket['total'] += 1
                    if result['rag_correct']:
                        bucket['correct'] += 1
                    break
        
        for bucket_name, bucket in length_buckets.items():
            if bucket['total'] > 0:
                accuracy = bucket['correct'] / bucket['total']
                print(f"   {bucket_name} ({bucket['range'][0]}-{bucket['range'][1]} chars): {bucket['correct']}/{bucket['total']} ({accuracy:.2%})")
    
    def _check_retrieval_quality(self, results):
        """æ£€æŸ¥æ£€ç´¢è´¨é‡"""
        print(f"\nğŸ¯ æ£€ç´¢è´¨é‡æ£€æŸ¥:")
        print("-" * 30)
        
        # éšæœºé€‰æ‹©å‡ ä¸ªé”™è¯¯æ¡ˆä¾‹è¿›è¡Œæ·±åº¦æ£€ç´¢åˆ†æ
        rag_errors = [r for r in results if not r['rag_correct']][:5]
        
        for i, result in enumerate(rag_errors, 1):
            print(f"\n   æ¡ˆä¾‹ {i}: {result['question']}")
            
            try:
                # æ‰§è¡Œæ£€ç´¢
                rag_result = self.engine.retrieve_and_rewrite(result['question'])
                retrieved_items = rag_result.get('retrieved_items', [])
                
                print(f"     æ£€ç´¢åˆ° {len(retrieved_items)} ä¸ªé¡¹ç›®")
                
                if retrieved_items:
                    # æ˜¾ç¤ºæœ€ç›¸å…³çš„æ£€ç´¢ç»“æœ
                    top_item = retrieved_items[0]
                    print(f"     æœ€ä½³åŒ¹é…: {top_item.get('triple', 'N/A')}")
                    print(f"     è·ç¦»: {top_item.get('distance', 'N/A'):.4f}")
                    print(f"     æ–‡æ¡£: {top_item.get('document', 'N/A')[:100]}...")
                    
                    # æ£€æŸ¥æ˜¯å¦æ£€ç´¢åˆ°ç›¸å…³ä¿¡æ¯
                    expected_words = set(result['expected'].lower().split())
                    retrieved_text = ' '.join([item.get('document', '') for item in retrieved_items[:3]]).lower()
                    
                    overlap = len(expected_words.intersection(set(retrieved_text.split())))
                    print(f"     è¯æ±‡é‡å : {overlap}/{len(expected_words)} ä¸ªè¯")
                else:
                    print(f"     âŒ æœªæ£€ç´¢åˆ°ä»»ä½•ç›¸å…³é¡¹ç›®")
                    
            except Exception as e:
                print(f"     âŒ æ£€ç´¢å¤±è´¥: {e}")
    
    def check_database_coverage(self):
        """æ£€æŸ¥æ•°æ®åº“è¦†ç›–åº¦"""
        print(f"\nğŸ“Š æ•°æ®åº“è¦†ç›–åº¦æ£€æŸ¥:")
        print("-" * 30)
        
        try:
            # è·å–æ•°æ®åº“ç»Ÿè®¡
            stats = self.db_manager.get_database_stats()
            print(f"   æ€»æ–‡æ¡£æ•°: {stats['total_documents']}")
            print(f"   é›†åˆçŠ¶æ€: {stats['status']}")
            
            # æ£€æŸ¥å…³ç³»ç±»å‹åˆ†å¸ƒ
            all_data = self.db_manager.collection.get()
            if all_data and all_data['metadatas']:
                relations = Counter()
                entities = Counter()
                
                for metadata in all_data['metadatas']:
                    rel = metadata.get('rel', 'unknown')
                    sub = metadata.get('sub', 'unknown')
                    obj = metadata.get('obj', 'unknown')
                    
                    relations[rel] += 1
                    entities[sub] += 1
                    entities[obj] += 1
                
                print(f"\n   å…³ç³»ç±»å‹åˆ†å¸ƒ (Top 10):")
                for rel, count in relations.most_common(10):
                    print(f"     {rel}: {count}")
                
                print(f"\n   å®ä½“é¢‘ç‡ (Top 10):")
                for entity, count in entities.most_common(10):
                    print(f"     {entity}: {count}")
            
        except Exception as e:
            print(f"   âŒ æ•°æ®åº“æ£€æŸ¥å¤±è´¥: {e}")
    
    def generate_improvement_suggestions(self, results):
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        print(f"\nğŸ’¡ æ”¹è¿›å»ºè®®:")
        print("=" * 30)
        
        rag_accuracy = sum(1 for r in results if r['rag_correct']) / len(results)
        
        suggestions = []
        
        if rag_accuracy < 0.5:
            suggestions.extend([
                "1. ğŸ” æ£€æŸ¥æ•°æ®è´¨é‡: ç¡®ä¿å‘é‡æ•°æ®åº“åŒ…å«è¶³å¤Ÿçš„ç›¸å…³ä¿¡æ¯",
                "2. ğŸ¯ ä¼˜åŒ–æ£€ç´¢ç­–ç•¥: è°ƒæ•´æ£€ç´¢å‚æ•°ï¼Œå¢åŠ æ£€ç´¢æ•°é‡",
                "3. ğŸ“ æ”¹è¿›ç­”æ¡ˆæå–: ä¼˜åŒ–CoTKRé‡å†™é€»è¾‘",
                "4. ğŸ”§ è°ƒæ•´åµŒå…¥æ¨¡å‹: è€ƒè™‘ä½¿ç”¨æ›´é€‚åˆçš„åµŒå…¥æ¨¡å‹"
            ])
        
        # æ£€æŸ¥ç©ºç­”æ¡ˆæ¯”ä¾‹
        empty_answers = sum(1 for r in results if not r['rag_answer'].strip())
        if empty_answers > len(results) * 0.1:
            suggestions.append("5. âŒ å‡å°‘ç©ºç­”æ¡ˆ: æ£€æŸ¥ç­”æ¡ˆæå–é€»è¾‘ï¼Œé¿å…è¿”å›ç©ºç»“æœ")
        
        # æ£€æŸ¥æ£€ç´¢è¦†ç›–åº¦
        suggestions.extend([
            "6. ğŸ“Š å¢åŠ æ•°æ®è¦†ç›–: è¡¥å……ç¼ºå¤±çš„çŸ¥è¯†é¢†åŸŸæ•°æ®",
            "7. ğŸ”„ ä¼˜åŒ–é‡æ’åº: æ”¹è¿›å¤šé˜¶æ®µæ£€ç´¢çš„é‡æ’åºç®—æ³•",
            "8. ğŸ¨ è°ƒæ•´prompt: ä¼˜åŒ–CoTKRçš„promptæ¨¡æ¿"
        ])
        
        for suggestion in suggestions:
            print(f"   {suggestion}")
        
        return suggestions

def main():
    """ä¸»å‡½æ•°"""
    # æŒ‡å®šéªŒè¯ç»“æœæ–‡ä»¶
    validation_file = r"D:\PythonFile\newSystem\evaluation\evaluation_result\validation_results_20250831_010000.json"
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not Path(validation_file).exists():
        print(f"âŒ éªŒè¯ç»“æœæ–‡ä»¶ä¸å­˜åœ¨: {validation_file}")
        print("ğŸ’¡ è¯·å…ˆè¿è¡Œ quick_validate.py ç”ŸæˆéªŒè¯ç»“æœ")
        return
    
    # åˆ›å»ºè¯Šæ–­å™¨
    diagnostics = RAGDiagnostics()
    
    # åˆ†æéªŒè¯ç»“æœ
    results = diagnostics.analyze_validation_results(validation_file)
    
    # æ£€æŸ¥æ•°æ®åº“è¦†ç›–åº¦
    diagnostics.check_database_coverage()
    
    # ç”Ÿæˆæ”¹è¿›å»ºè®®
    diagnostics.generate_improvement_suggestions(results)
    
    print(f"\nâœ… è¯Šæ–­å®Œæˆï¼")
    print(f"\nğŸ¯ å…³é”®æ”¹è¿›æ–¹å‘:")
    print(f"   1. æ£€æŸ¥å¹¶è¡¥å……æ•°æ®åº“å†…å®¹")
    print(f"   2. ä¼˜åŒ–æ£€ç´¢å’Œç­”æ¡ˆæå–é€»è¾‘")
    print(f"   3. è°ƒæ•´ç³»ç»Ÿå‚æ•°")
    print(f"   4. æ”¹è¿›promptæ¨¡æ¿")

if __name__ == '__main__':
    main()